A NEW COMPANION TO DIGITAL HUMANITIES EDITED BY SUSAN SCHREIBMAN, RAY SIEMENS, AND JOHN UNSWORTH This edition first published 2016 © 2016 John Wiley & Sons, Ltd. Registered Office John Wiley & Sons, Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK Editorial Offices 350 Main Street, Malden, MA 02148.5020, USA 9600 Garsington road, Oxford, OX4 2DQ, UK The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK For details of our global editorial offices, for customer services, and for information about how to apply for permission to reuse the copyright material in this book please see our website at www.wiley.com/wiley.blackwell. The right of Susan Schreibman, ray Siemens, and John Unsworth to be identified as the authors of the editorial material in this work has been asserted in accordance with the UK Copyright, Designs and Patents Act 1988. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher. Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and product names used in this book are trade names, service marks, trademarks or registered trademarks of their respective owners. The publisher is not associated with any product or vendor mentioned in this book. Limit of Liability/Disclaimer of Warranty: While the publisher and authors have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. It is sold on the understanding that the publisher is not engaged in rendering professional services and neither the publisher nor the author shall be liable for damages arising herefrom. If professional advice or other expert assistance is required, the services of a competent professional should be sought. Library of Congress Cataloging.in.Publication data applied for Hardback 9781118680599 Paperback 9781118680643 A catalog record for this book is available from the British Library. Cover image: Zden.k S.kora, Lines No. 56 (Humberto), 1988, oil on canvas, 200 . 200 cm. Collection of the Museum of Modern Art Olomouc, The Czech republic. Photo Zden.k Sodoma. © Zden.k S.kora - heir, Lenka S.korová, 2015 Set in 11/12.5pt Garamond3 by SPi Global, Pondicherry, India 1 2016 29 Crowdsourcing in the Digital Humanities Melissa Terras As Web 2.0 technologies changed the World Wide Web from a read.only to a co.creative digital experience, a range of commercial and noncommercial platforms emerged to allow online users to contribute to discussions and use their knowledge, experience, and time to build online content. Alongside the widespread success of collaboratively produced resources such as Wikipedia came a movement in the cultural and heritage sectors to trial crowdsourcing – the harnessing of online activities and behavior to aid in large.scale ventures such as tagging, commenting, rating, reviewing, text correcting, and the creation and uploading of content in a methodical, task.based fashion (Holley, 2010) – to improve the quality of, and widen access to, online collections. Building on this, within digital humanities there have been attempts to crowdsource more complex tasks traditionally assumed to be carried out by academic scholars, such as the accurate transcription of manuscript material. This chapter aims to survey the growth and uptake of crowdsourcing for culture and heritage, and more specifically within digital humanities. It raises issues of public engage.ment and asks how the use of technology to involve and engage a wider audience with tasks that have been the traditional purview of academics can broaden the scope and appreciation of humanistic inquiry. Finally, it asks what this increasingly common public.facing activity means for digital humanities itself, as the success of these projects demon.strates the effectiveness of building projects for, and involving, a wide online audience. Crowdsourcing: an Introduction Crowdsourcing – the practice of using contributions from a large online community to undertake a specific task, create content, or gather ideas – is a product of a critical cultural shift in Internet technologies. The first generation of the World Wide Web had been dominated by static websites, facilitated by search engines which only allowed information.seeking behavior. However, the development of online platforms which allowed and encouraged a two.way dialog rather than a broadcast mentality fostered public participation, the co.creation of knowledge, and community.building, in a phase which is commonly referred to as “Web 2.0” (O’Reilly, 2005; Flew, 2008). In 2005, an article in Wired magazine discussed how businesses were beginning to use these new platforms to outsource work to individuals, coining the term “crowdsourcing” as a portmanteau of “outsourcing” and “crowd”: Technological advances in everything from product design software to digital video cam.eras are breaking down the cost barriers that once separated amateurs from professionals. Hobbyists, part.timers, and dabblers suddenly have a market for their efforts, as smart companies in industries as disparate as pharmaceuticals and television discover ways to tap the latent talent of the crowd. The labor isn’t always free, but it costs a lot less than paying traditional employees. It’s not outsourcing; it’s crowdsourcing. (Howe, 2006a) The term was quickly adopted online to refer to the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. This can take the form of peer.production (when the job is performed collaboratively), but is also often undertaken by sole individuals. The crucial prerequisite is the use of the open call format and the large network of potential laborers. (Howe, 2006b) Within a week of the term being coined, 182,000 other websites were using it (Howe, 2006c) and it rapidly became the word used to describe a wide range of online activities from contributing to online encyclopedias such as Wikipedia, to tagging images on image.sharing websites such as Flickr, to writing comments on blogs, to proofreading out.of.copyright texts on Project Gutenberg, or contributing to open.source software (an analagous term to crowdsourcing, citizen science, has also been used where the small.scale tasks carried out online contribute to scientific projects; Silvertown, 2009). It is important to note here that the use of distributed (generally volunteer) labor to undertake small portions of much larger tasks, gather information, contribute to a larger project, or solve problems, is not new. There is a long history of scientific prizes, architectural competitions, genealogical research, scientific observation and recording, and linguistic study (to name but a few applications) that have relied on the contribu.tion of large numbers of individuals to undertake a centrally managed task, or solve a complex problem (see Finnegan, 2005, for an overview). For example, the Mass.Observation Project was a social research organization in the United Kingdom between 1937 and the 1960s, which relied on a network of 500 volunteer correspondents to record everyday life in Britain, including conversation, culture, and behavior (Hubble, 2006). The difference between these projects and the modern phenomenon of crowd.sourcing identified by Howe is, of course, the use of the Internet, the World Wide Web, and interactive web platforms as the mechanism for distributing information, collecting responses, building solutions, and communicating around a specified task or topic. There was an intermediary phase, however, between offline volunteer labor and the post.2006 “crowdsourcing” swell, where volunteer labor was used in conjunction with computers and online mechanisms to collect data. Brumfield (2013a) identifies at least seven genealogy projects, such as Free Births, Marriages and Deaths (FreeBMD, http://freebmd.org.uk), Free Registers (FreeREG, http://www.freereg.org. uk), and Free Census (FreeCEN, http://www.freecen.org.uk), that emerged in the 1990s: out of an (at least) one hundred year old tradition of creating print indexes to manuscript sources which were then published. Once the web came online, the idea of publishing these on the web [instead] became obvious. But the tools that were used to create these were spreadsheets that people would use on their home computers. Then they would put CD ROMs or floppy disks in the post and send them off to be published online. (Brumfield, 2013a) The recent phenomenon of crowdsourcing, or citizen science, can thus be seen as a continuation of the use of available platforms and communications networks to distribute tasks amongst large numbers of interested individuals, working towards a common goal. What types of web.related activities are now described as “crowdsourcing”? Daren Brabham (2013:45) proposes a useful typology, looking at the mostly commercial projects which exist in the crowdsourcing space, suggesting that there are two types of problems which can be best solved using this approach: information management issues and ideation problems. Information management issues occur where information needs to be located, created, assembled, sorted, or analyzed. Brabham suggests that knowledge discovery and management techniques can be used for crowdsourced information management, as they are ideal for gathering sources or reporting problems: an example of this would be SeeClickFix (http://en.seeclickfix.com), which encourages people to “report neighborhood issues and see them get fixed” (SeeClickFix, 2013). An alternative crowdsourcing approach to information management is what Brahbam calls “distributed human intelligence tasking”: when “a corpus of data is known and the problem is not to produce designs, find information, or develop solutions, but to process data” (Brabham, 2013:50). In the least creative and intellectually demanding of the crowdsourcing techniques, users can be encouraged to undertake repetitive “micro.tasks,” often for monetary compensation, if the task is for a commercial entity. An example of this would be Amazon’s Mechanical Turk (https://www.mturk.com), which “gives businesses and developers access to an on.demand, scalable workforce. Workers select from thousands of tasks and work whenever it’s convenient” (Amazon Mechanical Turk, 2014) – although Amazon Turk has been criticized for its “unethical” business model, with a large proportion of its workers living in developing countries, working on tasks for very little payment (Cushing, 2013). The second type of task that Brabham identified as suited to crowdsourcing consists of ideation problems: where creative solutions need to be proposed, that are either empirically true, or a matter of taste or market support (Brabham, 2013:48–51). Brabham suggests that crowdsourcing is commonly used as a form of “broadcast search” to locate individuals who can provide the answer to specific problems, or provide the solution to a challenge, sometimes with pecuniary rewards. An example of an online platform using this approach is InnoCentive.com, which is predominantly geared towards the scientific community to generate ideas or reach solutions, for research and development, sometimes with very large financial prizes: at time of writing, there were three awards worth $100,000 on offer. Brabham suggests that an alternative crowd.sourcing solution to ideation problems is “peer.vetted creative production” (Brabham, 2013:49), where a creative phase is opened up to an online audience, who submit a large number of submissions, and voting mechanisms are then put in place to help sort through the proposals, hoping to identify superior suggestions. An example of this approach would be Threadless.com, a creative community that designs, sorts, creates, and provides a mechanism to purchase various fashion items (the website started with t.shirts, but has since expanded to offer other products). Since its introduction in 2006, the term “crowdsourcing” is now used to cover a wide variety of activities across a large number of sectors: Businesses, non.profit organizations, and government agencies regularly integrate the creative energies of online communities into day.to.day operations, and many organiza.tions have been built entirely from these arrangements. (Brabham, 2013:xv) Brabham’s overall typology is a useful tool, as it provides a framework in which to think about both the type of problem that is being addressed by the online platform, and the specific crowdsourcing mechanism that is being used to propose a solution. Given the prevalence of the use of crowdsourcing in online communities for a range of both commercial and not.for.profit tasks, it is hardly surprising that various implementations of crowdsourcing activities have emerged in the cultural and heritage sector at large, and the digital humanities in particular. The Growth of Crowdsourcing in Cultural and Heritage Applications There are many aspects of crowdsourcing that are useful to those working in history, cultural and heritage, particularly within galleries, libraries, archives, and museums (GLAMs), which have a long history of participating with members of the public and generally have institutional aims to promote their collections and engage with as wide an audience as possible. However, “Crowdsourcing is a concept that was invented and defined in the business world and it is important that we recast it and think through what changes when we bring it into cultural heritage” (Owens, 2012a). The most obvious difference is that payment to those who undertake tasks is generally not an option for host institutions, but also that “a clearly ethical approach to inviting the public to help in the collection, description, presentation, and use of the cultural record” needs to be identified and pursued. Owens (2012a) sketches out a range of dif.ferences between the mass crowdsourcing model harnessed by the commercial sector and the use of online volunteer labor in cultural and heritage organizations, stressing that “many of the projects that end up falling under the heading of crowdsourcing in libraries, archives and museums have not involved large and massive crowds and they have very little to do with outsourcing labor.” Heritage crowdsourcing projects are not about anonymous masses of people, they are about inviting participation from those who are interested and engaged, and generally involve a small cohort of enthusiasts to use digital tools to contribute (in the same way as they may have volunteered offline to organize and add value to collections in the past). The work is not “labor” but a meaningful way in which individuals can interact with, explore, and understand the historical record. It is often highly motivated and skilled individuals that offer to help, rather than those who can be described with the derogatory term “amateurs.” Owens (2012a) suggests that crowdsourcing within this sector is then a complex interplay between understanding the potentials for human computation, adopting tools and software as scaffolding to aid this process, and understanding human motivation. No chronological history of the growth of crowdsourcing in culture and heritage exists, but the earliest large.scale project which adopted this model of interaction with users was the Australian Newspaper Digitisation Program (http://www.nla.gov.au/ content/newspaper.digitisation.program), which in August 2008 asked the general public to correct the OCR (optical character recognition) text of 8.4 million articles generated from their digitized historic Australian newspapers. This has been a phenomenally successful project, and by July 2015 over 166 million individual lines of newspaper articles had been proofread and corrected by volunteer labour. The resulting transcriptions can aid others not only in reading, but also in finding, text in the digitized archive. After the success of this project, and the rise of commercial crowdsourcing, other projects began to adopt crowdsourcing techniques to help digitize, sort, and correct heritage materials. In 2009 one of the earliest citizen science projects that is based on historical data, the North American Bird Phenology Program (www.pwrc.usgs.gov/bpp) was launched to transcribe 6 million migration.card obser.vations collected by a network of volunteers “who recorded information of first arrival dates, maximum abundance, and departure dates of migratory birds across North America” between 1880 and 1970 (North American Bird Phenology Program, undated). At time of writing, over a million cards have been transcribed by volunteers, allowing a range of scientific research to be carried out on the resulting data. Crowdsourcing in the heritage sector began to gather speed around 2010 with a range of projects being launched that asked the general public for various types of help via an online interface. One of the most successful of these is another combination of historical crowdsourcing and citizen science, called Old Weather (www.oldweather.org), which invites the general public to transcribe weather observations that were noted in ships’ logbooks dating from the mid.nineteenth century to the present day in order to “contribute to climate model projections and … improve our knowledge of past environmental conditions” (Old Weather, 2013a). Old Weather launched in October 2010 as part of the Zooniverse (www.zooniverse.org) portal of 15 different citizen science projects (which had started with the popular gallery classification tool, Galaxy Zoo (www.galaxyzoo.org), in 2009). The Old Weather project is a collaboration of a diverse range of archival and scientific institutions and museums and universities in both the UK and the USA (Old Weather, 2013b), showing how a common digital platform can bring together physically dispersed information for analysis by users. At time of writing, over 34,000 logs and seven voyages have been transcribed (three times, by different users, to ensure quality control, meaning that over 1,000,000 individual pages have been transcribed by users; Brohan, 2012), and the resulting data are now being used by both scientists and historians to understand both climate patterns and naval history (with their blog regularly updated with findings: http://blog.oldweather.org). A range of other notable crowdsourcing projects launched in the 2010–2011 period, showing the breadth and scope of the application of online effort to cultural heritage. These include (but are not limited to): Transcribe Bentham, which is discussed in more detail below; the Victoria and Albert Museum’s tool to get users to improve the cropping of their photos in the collection (http://collections.vam.ac.uk/crowdsourcing); the United States Holocaust Museum’s “Remember Me” project, which aims to identify children in photographs taken by relief workers during the immediate aftermath of the Second World War, to facilitate connections amongst survivors (http://rememberme. ushmm.org); New York Public Library’s What’s on the Menu? project (http://menus. nypl.org), in which users can transcribe their collection of historical restaurant menus; and the National Library of Finland’s DigitalKoot project (www.digitalkoot.fi/index_ en.html), which allowed users to play games that helped improve the metadata of their Historical Newspaper Library. The range and spread of websites that come under the crowdsourcing umbrella in the cultural and heritage sector continues to increase, and it is now a relatively established, if evolving, method used for galleries, libraries, archives, and museums. A list of nonprofit crowdsourcing projects in GLAM institutions is maintained at www.digitalglam.org/crowdsourcing/projects. Considering this activity in light of Brabham’s typology, above, it is clear that most projects fall into the “information management” category (Brabham, 2013), where an organization (or col.laborative project between a range of organizations) tasks the crowd with helping to gather, organize, and collect information into a common source or format. What is the relationship of these projects to those working in digital humanities? Obviously, many crowdsourcing projects depend on having information – or things – to comment on, transcribe, analyze, or sort, and therefore GLAM institutions, who are custodians of such historical material, often partner with university researchers who have an interest in using digital techniques to answer their humanities or heritage.based research question. There is often much sharing of expertise and technical infra.structure between different projects and institutions: for example, the Galaxy Zoo platform which underpins Old Weather also is used by Ancient Lives (http://ancientlives. org) to help crowdsource transcription of papyri, and Operation War Diary (http://www. operationwardiary.org) to help transcribe First World War unit diaries. Furthermore, those working in digital humanities can often advise and assist colleagues in partner institutions and scholarly departments: Transcribe Bentham is a collaboration between University College London’s Library Services (including UCL’s Special Collections), the Bentham Project (based in the Faculty of Laws), UCL Centre for Digital Humanities, the British Library, and the University of London Computing Centre, with the role of the Digital Humanities Centre being to provide guidance and advice with online activities, best practice, and public engagement. Another example of collaboration can be seen in events such as the CITSCribe Hackathon in December 2013, which “brought together over 30 programmers and researchers from the areas of biodiversity research and digital humanities for a week to further enable public participation in the transcription of biodiversity specimen labels” (iDigBio, 2013). Crowdsourcing in the digital humanities can also be used to sort and improve incomplete datasets, such as a corpus of 493 non.Shakespearean plays written between 1576 and 1642 in which 32,000 partially transcribed words were corrected by students over the course of an eight.week period using an online tool (http://annolex.at. northwestern.edu; see Mueller, 2014), indicating how we can use crowdsourcing to involve humanities students in the gathering and curating of corpora relevant to the wider humanities community. Scholars in the digital humanities are well placed to research, scope, and theorize crowdsourcing activities across a wider sector: for example, the Modeling Crowdsourcing for Cultural Heritage project (http://cdh.uva.nl/projects.2013.2014/m.o.c.c.a.html) based at the Centre for Digital Humanities and Creative Research Industries Amsterdam, both at the University of Amsterdam, is aiming to determine a comprehensive model for “determining which types and methods of crowdsourcing are relevant for which specific purposes” (Amsterdam Centre for Digital Humanities, 2013). As we shall see, below, digital humanities scholars and centers are investigating and building new platforms for crowdsourcing activities – particularly in the transcription of historical texts. In addition, digital humanities academics can help with suggestions on what we can do with crowdsourced information once collected; we are now moving into a next phase of crowdsourcing, where understanding data mining and visualization techniques to query the volume of data collected by volunteer labor is necessary. Finally, there is the beginnings of a body of literature on the wider area of crowdsourcing, both across the digital humanities and in the GLAM sector, and taken together these can inform those who are contemplating undertaking a crowdsourcing project for a related area. It should be stressed that it is often hard to make a distinction between what should be labeled a “GLAM sector” project and what should be labeled “digital humanities” in the area of crowdsourcing, as many projects are using crowdsourcing not only to sort or label or format historical information, but to provide the raw materials and methodologies for creating and understanding novel information about our past, our cultural inheritance, or our society. Following on from the success of the Australian Newspapers Digitisation Program which she managed, Holley (2010) brought issues of “Crowdsourcing: how and why should libraries do it” to light, in a seminal discussion that much subsequent research and project implementation has benefited from. Holley proposes that there are several potential benefits in using crowdsourcing within a library context (which we can also extrapolate to cover those working across the GLAM sector, and in digital human.ities). The benefits of crowdsourcing noted are that it can help to: achieve goals the institution would not have the resources (temporal, financial, or staffing) to accomplish itself; achieve these goals more quickly than if working alone; build new user groups and communities; actively engage the community with the institution and its systems and collections; utilize external knowledge, expertise, and interest; improve the quality of data, which improves subsequent user search experiences; add value to data; improve and expand the ways in which data can be discovered; gain an insight into user opinions and desires by building up a relationship with the crowd; show the relevance and importance of the institution (and its collections) by the high level of public interest in the project; build trust and encourage loyalty to the institution; and encourage a sense of public ownership and responsibility towards cultural heritage collections (Holley, 2010). Holley also asks what the normal profile of a crowdsourcing volunteer in the cultural, heritage, and humanities sector is, stressing that from even early pilot projects the same makeup emerges: although there may be a large number of volunteers who originally sign up, the majority of the work is done by a small cohort of super.users, who achieve significantly larger amounts of work than anyone else. They tend to be committed to the project for the long term, appreciate that it is a learning experience, which gives them purpose and is personally rewarding, perhaps because they are inter.ested in it, or see it as a good cause. Volunteers often talk of becoming addicted to the activities, and the amount of work undertaken often exceeds the expectations of the project. Holley (2010) argues that “the factors that motivate digital volunteers are really no different to factors that motivate anyone to do anything,” saying that interest, passion, a worthy cause, giving back to the community, helping to achieve a group goal, and contributing to the discovery of new information in an important area are often reasons that volunteers contribute. Observations and surveys of volunteers by site managers noted various techniques that can improve user motivation, such as adding more content regularly, increasing challenges, creating a camaraderie, building relationships with the project, acknowledging the volunteer’s help, providing rewards, and making goals and progress transparent. The reward and acknowledgment process is often linked to progress reports, with volunteers being named, high achievers being ranked in publicly available tables, and promotional gifts. Holley provides various tips that have provided guidance for a variety of crowd.sourcing projects, and are worth following by those considering using this method. The project should have a clear goal that presents a big challenge, report regularly on progress, and showcase results. The system should be easy and fun, reliable and quick, intuitive, and provide options to users so they can choose what they work on (to a certain extent). The volunteers should be acknowledged, rewarded, supported by the project team, and trusted. The content should be interesting, novel, focused on history or science, and there should be lots of it (Holley, 2010). Holley’s paper was written just before many of the projects outlined above came on.stream, stressing the potential for institutions, and challenging institutional struc.tures to be brave enough to attempt to engage individuals in this manner. By 2012, with various projects in full swing, reports and papers began to appear about the nuances of crowdsourcing in this area, although “there is relatively little academic lit.erature dealing with its application and outcomes to allow any firm judgements to be made about its potential to produce academically credible knowledge” (Dunn and Hedges, 2012:4). Ridge (2012) explores the “frequently asked questions about crowdsourcing in cultural heritage,” noting various misconceptions and apprehensions surrounding the topic. Ridge agrees with Owens (2012b) that the industry definition of crowdsourcing is problematic, suggesting instead that it should be defined as an emerging form of engagement with cultural heritage that contributes towards a shared, significant goal or research area by asking the public to undertake tasks that cannot be done automatically, in an environment where the tasks, goals (or both) provide inherent rewards for participation. (Ridge, 2012) Ridge draws attention to the importance of the relationships built between individuals and organizations, and that projects should be mindful of the motivations for partici.pating. Institutional nervousness around crowdsourcing is caused by worries that malicious or deliberately bad information will be provided by difficult, obstructive users, although Ridge maintains this is seldom the case, and that a good crowdsourcing project should have inbuilt mechanisms to highlight problematic data or users, and validate the content created by its users. Ridge returns again to the ethics of using volunteer labor, allaying fears about the type of exploitation seen in the commercial sector exploitation by explaining that Museums, galleries, libraries, archives and academic projects are in the fortunate position of having interesting work that involves an element of social good, and they also have hugely varied work, from microtasks to co.curated research projects. Crowdsourcing is part of a long tradition of volunteering and altruistic participation. (Ridge, 2012) In a further 2013 post, Ridge also highlights the advantages of digital engagement via crowdsourcing, suggesting that digital platforms can allow smaller institutions to engage with users just as well as large institutions, can generate new relationships with different organizations in order to work together around a similar topic in a collaborative project, and can provide great potential for audience participation and engagement (Ridge, 2013). In fact, Owens (2012b) suggests that our thinking around crowdsourcing in culture and heritage is the wrong way round: rather than thinking of the end product and the better data that volunteers are helping us create, institutions should focus on the fact that crowdsourcing marks a fulfillment of the mission of putting digital collections online: What crowdsourcing does, that most digital collection platforms fail to do, is offers [sic] an opportunity for someone to do something more than consume information … Far from being an instrument which enables us to ultimately better deliver content to end users, crowdsourcing is the best way to actually engage our users in the fundamental reason that these digital collections exist in the first place … At its best, crowdsourcing is not about getting someone to do work for you, it is about offering your users the opportunity to participate in public memory. (Owens, 2012b) The lessons learned from these museum. and library.based projects are important starting points for those in the digital humanities who wish to undertake crowdsourcing themselves. Crowdsourcing and Digital Humanities In a 2012 scoping study of the use of crowdsourcing particularly applied to humanities research, 54 academic publications were identified that were of direct relevance to the field, and a further 51 individual projects, activities, or websites were found which documented or presented some aspect, application, or use of crowdsourcing within humanities scholarship (Dunn and Hedges, 2012). Many of these projects have cross.overs with libraries, archives, museums, and galleries, as partners who provide content or expertise, or who host projects themselves, and many of them are yet to produce a tangible academic outcome. As Dunn and Hedges point out, at a time when the web is simultaneously transforming the way in which people collaborate and communicate, and merging the spaces which the academic and non.academic com.munities inhabit, it has never been more important to consider the role which public communities – connected or otherwise – have come to play in academic humanities research. (Dunn and Hedges, 2012:3). Dunn and Hedges (2012:7) identify four factors that define crowdsourcing used within humanities research. These are: a clearly defined core research question and direction within the humanities; the potential for an online group to add to, transform, or inter.pret data that is important to the humanities; a definable task which is broken down into an achievable workflow; and the setting up of a scalable activity which can be undertaken with different levels of participation. Very similar to the work done in the GLAM sector, the theme and research question of the project are therefore the main distinguishing factors from other types of crowdsourcing, with digital humanities projects learning from other domains such as successful projects in citizen science or industry. An example of such a project fitting into this humanities crowdsourcing definition, given its purview, is Transcribe Bentham (http://blogs.ucl.ac.uk/transcribe.bentham), a manuscript transcription initiative that intends to engage students, researchers, and the general public with the thought and life of the philosopher and reformer, Jeremy Bentham (1748–1832), by making available digital images of his manuscripts for anyone, anywhere in the world, to transcribe. The fundamental research question driving this project is to understand the thought and writings of Bentham more completely – a topic of fundamental importance to those engaged in eighteenth. or nineteenth.century studies – given that 40,000 folios of his writings remain un.transcribed “and their contents largely unknown, rendering our understanding of Bentham’s thought – together with its historical significance and continuing philosophical importance – at best provisional, and at worst a caricature” (Causer and Terras, 2014a). The objectives of the project are clear, with the benefit to humanities (and law, and social science) research evident from the research objectives. Dunn and Hedges (2012:18–19) list the types of knowledge that may be usefully created in digital humanities crowdsourcing activities, resulting in new understanding of humanities research questions. These digital humanities crowdsourcing projects are involved in: making ephemera available that would otherwise not be; opening up information that would normally be accessible to distinct groups; giving a wider audi.ence to specific information held in little.known written documentation; circulation of personal histories and diaries; giving personal links to historical processes and events; identifying links between objects; summarizing and circulating datasets; synthesizing new data from existing sources; and recording ephemeral knowledge before it dissipates. Dunn and Hedges stress that an important point in these crowd.sourcing projects is that they enable the building up of knowledge of the process of how to conduct collaborative research in this area, while creating communities with a shared purpose, which often carry out research work that goes beyond the expectations of the project (19). However, they are keen to also point out that most humanities scholars who have used crowd.sourcing in its various forms now agree that it is not simply a form of cheap labour for the creation or digitization of content; indeed in a cost.benefit sense it does not always compare well with more conventional means of digitization and processing. In this sense, it has truly left its roots, as defined by Howe (2006) behind. The creativity, enthusiasm and alternative foci that commu.nities outside that academy can bring to academic projects is a resource which is now ripe for tapping in to. (Dunn and Hedges, 2012:40). As with Owens’ thoughts on crowdsourcing in the GLAM sector (2012), we can see that crowdsourcing in the humanities is about engagement, and encouraging a wide, and different, audience to engage in processes of humanistic inquiry, rather than merely being a cheap way to encourage people to get a necessary job done. Crowdsourcing and Document Transcription The most high.profile area of crowdsourcing carried out within the humanities is in the area of document transcription. Although commercial optical character recognition (OCR) technology has been available for over 50 years (Schantz, 1982), it still cannot generate high.quality transcripts of handwritten material. Work with texts and textual data is still the major topic of most digital humanities research: see the analysis by Scott Weingart of submissions to the Digital Humanities Conference 2014, which showed that of the 600 abstracts, 21.5% dealt with some form of text analysis, 19% were about literary studies, and 19% were about text mining (Weingart, 2013). It is therefore no surprise that most digital humanities crowdsourcing activities – or at least those emanating from digital humanities centers and/or associated in some sense with the digital humanities community – have been involved in the creation of tools which help transcribe important handwritten documents into machine.processable form. Ben Brumfield, in a talk presented in 2013, demonstrated that there were 30 collaborative transcription tools developed since 2005 (Brumfield, 2013a), situating the genealogical sites, and those such as Old Weather and Transcribe Bentham, in a trajectory which leads to the creation of tools and platforms which people can use to upload their own documents, and manage their own crowdsourcing projects (reviews of these different platforms are available on Brumfield’s blog at http://manus cripttranscription.blogspot.co.uk, and at time of writing there are now 37 collabora.tive tools for crowdsourcing document transcription, listed by Brumfield at http:// tinyurl.com/TranscriptionToolGDoc). The first of these customizable tools was Scripto (http://scripto.org), a freely available, open.source platform for community transcription developed in 2011 by the Center for History and New Media (CHNM) at George Mason University alongside their Papers of the United States War Department project (http://wardepartmentpapers.org). Another web.based tool, specifically designed for Transcription for Paleographical and Editorial Notation (T.PEN) (http://t.pen.org/TPEN), coordinated by the Center for Digital Theology at Saint.Louis University, provides a web.based interface for working with images of manu.scripts. Transcribe Bentham has also released a customizable, open.source version of its Mediawiki.based platform (https://github.com/onothimagen/cbp.transcription.desk), which has since been used by the Public Record Office of Victoria, Australia (http:// wiki.prov.vic.gov.au/index.php/Category:PROV_Transcription_Pilot_Project). The toolbar developed for Transcribe Bentham, which helps people encode various aspects of transcription such as dates, people, deletions, etc., has been integrated into the Letters of 1916 project at Trinity College Dublin (http://dh.tcd.ie/letters1916). The platform Letters of 1916 uses is the DIYHistory suite, built by the University of Iowa, which itself is based on CHNM’s Scripto tool. Links between crowdsourcing projects are common. There is now a range of transcription projects online, ranging from those created, hosted, and managed by scholarly or memory institutions, to those entirely organized by amateurs with no scholarly training or association. A prime example of the latter would be Soldier Studies (www.soldierstudies.org), a website dedicated to preserving the content of American Civil War correspondence bought and sold on eBay, to allow access to the contents of this ephemera before it resides in private collections, which, although laudable, uses no transcription conventions at all in cataloging or transcrib.ing the documents it finds (Brumfield, 2013a). The movement towards collaborative online document transcription by volunteers not only uncovers new, important historical primary source material, but it also “can open up activities that were traditionally viewed as academic endeavors to a wider audience interested in history” (Causer and Terras, 2014a). Brumfield points out that there are issues which come with this: There’s an institutional tension, in that editing of documents has historically been done by professionals, and amateur editions have very bad reputations. Well now we’re asking volunteers to transcribe. And there’s a big tension between, well how do volunteers deal with this [process], do we trust volunteers? Wouldn’t it be better just to give us more money to hire more professionals? So there’s a tension there. (Brumfield, 2013a) Brumfield further explores this in another blog post, where he asks: what is the qualitative difference between the activities we ask amateurs to do and the activities performed by scholars … we’re not asking “citizen scholars” to do real scholarly work, and then labeling their activity scholarship – a concern I share with regard to editing. If most crowdsourcing projects ask amateurs to do little more than wash test tubes, where are the projects that solicit scholarly interpretation? (Brumfield, 2013b) There is therefore a fear that without adequate guidance and moderation, the products of crowdsourced transcription will be what Shillingsburg referred to as “a dank cellar of electronic texts” where “the world is overwhelmed by texts of unknown provenance, with unknown corruptions, representing unidentified or misidentified versions” (2006:139). Brumfield (2013c) points out that Peter Robinson describes both the utopia and the dystopia of crowdsourcing transcription: utopia in which textual scholars train the world in how to read documents, and a dystopia in which hordes of “well.meaning but ill.informed enthusiasts will strew the web willy.nilly with error.filled transcripts and annotations, burying good scholarship in rubbish” (Robinson, quoted in Brumfield, 2013c). To avoid this, Brumfield suggests that partnerships and dialog between volunteers and professionals is essential, to make methodologies for approaching texts visible, and to allow volunteers to become advocates “not just for the material and the materials they are working on through crowdsourcing project, but for editing as a discipline” (Brumfield, 2013c). Care needs to be taken, then, when setting up a crowdsourcing transcription project, to ensure that the quality of the resulting transcription is suitable to be used as the basis for further scholarly humanistic inquiry, if the project is to be useful over a longer term and for a variety of research. The methods and approaches in assuring transcription quality of content need to be ascertained: whether the project uses double.keying (where two or more people enter the same text to ensure its veracity), or moderation (where an expert in the field signs off the text into a database, agreeing that its content meets benchmarked standards). However, in addition to this the format that the data is stored in needs to be structured to ensure that complex representational issues are preserved, and that any resulting data created can be easily reused and textual models can be understood, repurposed, or integrated with other collections. As Brumfield (2013a) points out, digital humanities already has a standard for documentary scholarly editing in the Text Encoding Initiative guidelines (2014), which have been available since 1990 and provide a flexible but robust framework within which to model, analyze, and present textual data. However, only seven of the crowdsourcing manuscript transcrip.tion tools (out of the 30 then available) attempted to integrate TEI compliant XML encoding into their workflow (Brumfield, 2013a). Projects which have used TEI markup as part of the manuscript transcription process, such as Transcribe Bentham, have demonstrated that users can easily learn the processes of encoding texts with XML if clear guidance and instruction is given to them, and it is explained why they should make the effort to do it (Brumfield, 2013a; Causer and Terras, 2014a, 2014b). Brumfield (2013a) stresses that is it the responsibility of those involved in academic scholarly edit.ing within the digital humanities to ensure that their work on establishing methods and guidelines for academic transcription is felt within the development of public.facing transcription tools, and if we are engaging users so that they can built their own skillsets, we need to use our digital platforms to train them according to pedagogical and scholarly standards: “Crowdsourcing is a school. Programs are the teachers. We have to get it right” (Brumfield, 2013d). Brumfield (2013c) also highlights that it is the responsibility of those working in document editing, and the digital humanities, to release guides to editing and transcribing that are accessible to those with no academic training in this area, such as computer programmers building transcriptions tools, if we wish for the resulting interfaces to allow community.led transcription to result in high.quality textual material. Future Issues in Digital Humanities Crowdsourcing We are now at a stage where crowdsourcing has joined the ranks of established digital methods for gathering and classifying data for use in answering the types of questions of interest to humanities scholars, although there is much research that still needs to be done about user response to crowdsourcing requests, and how best to build and deliver projects. There are also issues about data management, given that crowdsourcing is now reaching a mature phase where a variety of successful projects have amassed large amounts of data, often from different sources within individual projects: the million pages from Old Weather from different archives; over 3 million words transcribed by volunteer labor in the Transcribe Bentham project (Grint, 2013) from both UCL and the British Library; approximately 1500 letters transcribed in Soldier Studies (2014), which at a conservative estimate must give at least half a million words of correspondence from the American Civil War. Issues are therefore arising about sustainability: what will happen to all this data, particularly with regard to projects that do not have institutional resources or affiliation for long.term backup or storage? There are also future research avenues to investigate cross.project sharing and amalgamation of data: one can easily imagine either centrally managed or federated repositories of crowdsourced information that contain all the personal diaries that have been transcribed, searchable by date, place, person, etc.; or all letters and correspondence that have been sent over time, or all newspapers that were issued on a certain date worldwide. Both legal and technical issues will come into play with this, as questions of licensing (Who owns the volunteer.created data? Who does the copyright belong to?) and cross.repository searching will have to be negotiated, with related costs for delivering mechanisms and platforms covered. The question of the ethics of crowdsourcing is one that also underlies much of this effort in the humanities and the cultural and heritage sector, and projects have to be careful to work with volunteers, rather than exploit them, when building up these repositories and reusing and repurposing data in the future. Ethical issues come sharply into focus when projects start to pay (usually very little) for the labor involved, particularly when using online crowdsourcing labor brokers such as Amazon’s Mechanical Turk, which has been criticized as a digital sweatshop … critics have emerged from all corners of the labor, law, and tech communities. Labor activists have decried it as an unconscionable abuse of workers’ rights, lawyers have questioned its legal validity, and academics and other observers have probed its implications for the future of work and of technology. (Cushing, 2013) The relationship between commerce and volunteers, payment and cultural heritage, resources and outputs, online culture and the online workforce, is complex. A project such as Emoji.Dick (www.kickstarter.com/projects/fred/emoji.dick) – which translated Moby.Dick into Japanese Emoji icons using Amazon’s Mechanical Turk – is a prime example of what emerges when the lines of public engagement, culture, art, fun, low.paid crowdsourced labor, crowdfunding, and an internet meme, collide. Institutions and scholars planning on tapping into the potential labor force crowdsourcing offers have to be aware of the problems in outsourcing such labor, often very cheaply, to low.paid workers, often in developing countries (Cushing, 2013). Returning to Brabham’s typology on crowdsourcing projects, we can also see that although most projects that have used crowdsourcing in the humanities are information management tasks in that they ask volunteers to help enter, collate, sort, organize, and format information, there is also the possibility that crowdsourcing can be used within the humanities for ideation tasks: asking big questions, and proposing solutions. This area is undocumented within digital humanities, although the Association for Computers and the Humanities (ACH) and the 4Humanities.org initiative have both used an open.source platform, All Our Ideas (www.allourideas.org), to help scope out future initiatives (ACH, 2012; Rockwell, 2012). ACH also hosts and supports DH Questions and Answers (http://digitalhumanities.org/answers), a successful community.based questions and answers board for digital humanities issues, which falls within the ideation category of crowdsourcing. There is much scope within the humanities in general to explore this methodology and ideation mechanism further, and to engage the crowd in both proposing and solving questions about the humanities, rather than using it only to self.organize digital humanities initiatives. Crowdfunding is another relatively new area allied to crowdsourcing, which could be of great future benefit to digital humanities, and humanities projects in general. Only a few projects have been started to date within the GLAM sector, both for traditional collections acquisition and for digital projects. The British Library is attempting to crowdfund for the digitization of historical London maps (British Library, 2014); the Naturalis Biodiversity Centre in Leiden is raising funds via crowdfunding to purchase a Tyrannosaurus rex skeleton (http://tientjevoortrex. naturalis.nl); the Archiefbank or the Stadarcheif Amsterdam has raised €30,000 to digitize and catalog the Amsterdam death registers between 1892 and 1920 (Stadsarchief Amsterdam, 2012); and a campaign to crowdfund the £520,000 needed to buy the cottage on the Sussex coast where William Blake wrote “England’s green and pleasant land” was launched in 2014 (Flood, 2014). A project called Micropasts (http://micropasts.org), funded by the UK’s Arts and Humanities Research Council based at UCL and the British Museum, has developed a community platform for conducting, designing, and funding research into the human past, testing opportu.nities in crowdfunding: over the next few years this will be an area which has much potential for involving those outside the academy with core issues within humanities scholarship. Crowdsourcing also offers a relatively agile mechanism for those working in digital humanities to respond immediately to important contemporary events, preserving and collating evidence, ephemera, and archive material for future scholarship and comm.unity use. For example, the September 11th Digital Archive (http://911digitalarchive. org), which “uses electronic media to collect, preserve, and present the history of the September 11, 2001 attacks in New York, Virginia, and Pennsylvania and the public responses to them” (September 11 Digital Archive, 2011), began as a collaboration between the American Social History Project at the City University of New York Graduate Center, and the Center for History and New Media at George Mason University, immediately after the terrorist attacks. Likewise, the Our Marathon archive (http://marathon.neu.edu), led by Northeastern University, provides an archival and community space to crowdsource an archive of “pictures, videos, stories, and even social media related to the Boston Marathon; the bombing on April 15, 2013; the subsequent search, capture, and trial of the individuals who planted the bombs; and the city’s healing process” (Our Marathon, 2013). There is clearly a role here for those within the digital humanities with technical and archival expertise to respond to contemporary events by building digital platforms that will keep records for the future, while at the same time engaging with a community – and often a society – in need of sustained dialog to process the ramifications of such events. There is also potential for more sustained and careful use of crowdsourcing within both the university and the school classroom, to promote and integrate ongoing humanities research aims, but also to “meet essential learning outcomes of liberal education like gaining knowledge of culture, global engagement, and applied learning” (Frost Davis, 2012). There are opportunities for motivated students to become more involved and engaged with projects that digitize, preserve, study, and analyze resources, encouraging them to gain first.hand knowledge of humanities issues and methods, but also to understand the role that digital methods can play in public engagement: Essential learning outcomes aim at producing students with transferrable skills; in the globally networked world, being able to produce knowledge in and with the network is a vital skill for students. Students also benefit from exposure to how experts approach a project. While these tasks may seem basic, they lay the groundwork for developing deeper expertise with practice so that participation in crowdsourcing projects may be the beginning of a pipeline that leads students on to more sophisticated digital humanities research projects. Even if students don’t go on to become digital humanists, crowdsourced projects can help them develop a habit of engagement with the (digital) humanities, something that is just as important for the survival of the humanities. Indeed, a major motivation for humanities crowdsourcing is that involving the public in a project increases public support for that project. (Frost Davis, 2012) Crowdsourcing within the humanities will then continue to evolve, and offers much scope for using public interest in the past to bring together data and build projects which can benefit humanities research: Public involvement in the humanities can take many forms – transcribing handwritten text into digital form; tagging photographs to facilitate discovery and preservation; entering structured or semi.structured data; commenting on content or participating in discussions, or recording one’s own experiences and memories in the form of oral history – and the relationship between the public and the humanities is convoluted and poorly understood. (Dunn and Hedges, 2012:4) By systematically applying, building, evaluating, and understanding the uses of crowdsourcing within culture, heritage, and the humanities, by helping develop the standards and mechanisms to do so, and by ensuring that the data created will be usable for future scholarship, the digital humanities can aid in creating stronger links between the public and humanities research, which, in turn, means that crowdsourc.ing becomes a method of advocacy for the importance of humanities scholarship, involving and integrating non.academic sectors of society into areas of humanistic endeavor. Conclusion This chapter has surveyed the phenomenon of using digital crowdsourcing activities to further our understanding of culture, heritage, and history, rather than simply identi.fying the activities of digital humanities centers, or self.identified digital humanities scholars. This is an important distinction about the nature of digital humanities research, its home, and its purview. Much of the crowdsourcing activity identified in the GLAM sector comfortably fits under the digital humanities umbrella, even if those involved did not self.identify with that classification: there is a distinction to be made between projects which operate within the type of area which is of interest to digital humanities, and those run by digital humanities centers and scholars. With that in mind, this chapter has highlighted various ways in which those working in digital humanities can help advise, create, and build crowdsourcing pro.jects working in the area of culture and heritage, both to add to our understanding of crowdsourcing as a methodology for humanities research and to build up resulting datasets which will allow further humanities research questions to be answered. Given the current pace of development in the area of crowdsourcing within this sector, there is much that can be contributed from the digital humanities community to ensure that the resulting methods and datasets are useful, and reusable, particularly within the arena of document transcription and encoding. In addition, crowdsourcing affords vast opportunities for those working within the digital humanities to provide accessible demonstrators of the kind of digital tools and projects which are able to forward our understanding of culture and history, and also offers outreach and public engagement opportunities to show that humanities research, in its widest sense, is a relevant and important part of the scholarly canon to as wide an audience as possible. In many ways, crowdsourcing within the cultural and heritage sectors is digital humanities writ large: indicating an easily accessible way in which we can harness computational platforms and methods to engage a wide audience to contribute to our understanding of society and our cultural inheritance. References and Further Reading ACH. 2014. ACH agenda setting: next steps. Association for Computers and the Humanities blog. http://ach.org/2012/06/04/ach.agenda.setting.next.steps (accessed January 29, 2014). Amazon Mechanical Turk. 2014. Amazon Mechanical Turk, Welcome. https://www.mturk. com/mturk/welcome (accessed January 16, 2014). Amsterdam Centre for Digital Humanities. 2013. Modeling crowdsourcing for cultural heritage. http://cdh.uva.nl/projects.2013.2014/ m.o.c.c.a.html (accessed January 17, 2013). Brabham, D.C. 2013. Crowdsourcing. MIT Press Essential Knowledge Series. Cambridge, MA: MIT Press. British Library. 2014. Unlock London maps and views.http://support.bl.uk/Page/Unlock.London.Maps (accessed January 29, 2014). Brohan, P. 2012. New uses for old weather. Position paper, AHRC Crowdsourcing StudyWorkshop, May 2012. http://crowds. cerch.kcl.ac.uk/wp.content/uploads/2012/04/ Brohan.pdf (accessed January 29, 2014). Brumfield, B. 2013a. Itinera nova in the world(s) of crowdsourcing and TEI. Collaborative Manuscript Transcription blog. http:// manuscripttranscription.blogspot.co.uk/2013/04/ itinera.nova.in.worlds.of.crowdsourcing.html (accessed January 29, 2014). Brumfield, B. 2013b. A Gresham’s law for crowdsourcing and scholarship. Collaborative Manuscript Transcription blog. http://manuscript transcription.blogspot.co.uk/2013/10/a.greshams.law.for.crowdsouring.and.html (accessed January 29, 2014). Brumfield, B. 2013c. The collaborative future of amateur editions. Collaborative Manuscript Transcription blog. http://manuscripttranscription. blogspot.co.uk/2013/07/the.collaborative.future.of.amateur.html (accessed January 29, 2014). Brumfield, B. 2013d. In Text Theory, Digital Documents, and the Practice of Digital Editions, ed. J.J. van Zundert, C. van den Heuvel, B. Brumfield, et al. Panel session, Digital Humanities 2013, University of Nebraska, Lincoln. July 2013. Causer, T., and Terras, M. 2014a. Crowdsourcing Bentham: beyond the traditional boundaries of academic history. International Journal of Humanities and Arts Computing 8 (1), 46–64. Causer, T., and Terras, M. 2014b.”Many hands make light work. Many hands together make merry work”: Transcribe Bentham and crowd.sourcing manuscript collections. In Crowdsourcing our Cultural Heritage, ed, M. Ridge. London: Ashgate, 57–88. Cushing, E. 2013. Amazon Mechanical Turk: the digital sweatshop. UTNE. http://www.utne.com/ science.and.technology/amazon.mechanical.turk.zm0z13jfzlin.aspx#axzz3DNzILSHI (accessed Januaary 29, 2014). Dunn, S., and Hedges, M. 2012. Crowd.Sourcing Scoping Study: Engaging the Crowd with Humanities Research. Arts and Humanities Research Council. http://crowds.cerch.kcl.ac.uk/wp.content/ uploads/2012/12/Crowdsourcing.connected.communities.pdf (accessed January 16, 2014). Finnegan, R. 2005. Participating in the Knowledge Society: Research beyond University Walls. Basingstoke: Palgrave Macmillan. Flew, T. 2008. New Media: An Introduction, 3rd edition. Melbourne: Oxford University Press. Flood, A. 2014. Crowdfunding campaign hopes to save William Blake’s cottage for nation. The Guardian, September 11, http://www.the guardian.com/culture/2014/sep/11/crowd funding.campaign.william.blake.cottage (accessed June 20, 2015). Frost Davis, R. 2012. Crowdsourcing, undergradu.ates, and digital humanities projects. http:// rebeccafrostdavis.wordpress.com/2012/09/03/ crowdsourcing.undergraduates.and.digital.humanities.projects (accessed January 29, 2014). Grint, K. 2013. Progress update, 24 to 30 August 2013. Transcribe Bentham blog. http://blogs. ucl.ac.uk/transcribe.bentham/2013/08 (accessed January 29, 2014). Holley, R. 2010. Crowdsourcing: how and why should libraries do it? D.Lib Magazine 16 (3/4). http://www.dlib.org/dlib/march10/holley/ 03holley.html (accessed January 29, 2014). Howe, J. 2006a. The rise of crowdsourcing. Wired, June 2006. http://www.wired.com/wired/ archive/14.06/crowds.html (accessed January 17, 2014). Howe, J. 2006b. Crowdsourcing: a definition. Crowdsourcing blog. http://crowdsourcing. typepad.com/cs/2006/06/crowdsourcing_a. html (accessed January 17, 2014). Howe, J. 2006c. Birth of a meme. Crowdsourcing blog. http://www.crowdsourcing.com/cs/2006/05/ birth_of_a_meme.html (accessed January 17, 2014). Hubble, N. 2006. Mass.Observation and Everyday Life. Basingstoke: Palgrave Macmillan. iDigBio. 2013. CITScribe Hackathon. https:// www.idigbio.org/content/citscribe.hackathon (accessed January 30, 2014). Mueller, M. 2014. Shakespeare his contemporaries: collaborative curation and exploration of Early Modern drama in a digital environment. DHQ: Digital Humanities Quarterly 8 (3). http://www. digitalhumanities.org/dhq/vol/8/3/000183/ 000183.html (accessed January 30, 2014). North American Bird Phenology Program (undated). About BPP. http://www.pwrc.usgs. gov/bpp/AboutBPP2.cfm (accessed February 9, 2014). Old Weather. 2013a. Old Weather: our weather’s past, the climate’s future. http://www. oldweather.org (accessed January 17, 2014). Old Weather. 2013b. Old Weather, About. http:// www.oldweather.org/about (accessed January 17, 2014). O’Reilly, T. 2005. What is Web 2.0? http://www. oreilly.com/pub/a/oreilly/tim/news/2005/09/30/ what.is.web.20.html (accessed Januaary 16, 2014). Our Marathon. 2013. About the Our Marathon archive. http://marathon.neu.edu/about (accessed January 28, 2014). Owens, T. 2012a. The crowd and the library. http://www.trevorowens.org/2012/05/the.crowd.and.the.library (accessed January 16, 2014). Owens, T. 2012b. Crowdsourcing cultural heritage: the objectives are upside down. http://www. trevorowens.org/2012/03/crowdsourcing.cultural.heritage.the.objectives.are.upside.down (accessed January 17, 2014). Ridge, M. 2012. Frequently asked questions about crowdsourcing in cultural heritage. Open Objects blog. http://openobjects.blogspot.co. uk/2012/06/frequently.asked.questions.about. html (accessed January 18, 2014). Ridge, M. 2013. Digital participation, engagement, and crowdsourcing in museums. London Museums Group blog. http://www.london museumsgroup.org/2013/08/15/digital.participation.engagement.and.crowdsourcing.in.museums (accessed January 18, 2014). Rockwell, G. 2012. All our ideas: the value of the humanities. 4Humanities. http://4humanities. org/2012/10/all.our.ideas.the.value.of.the.humanities (accessed January 28, 2014). Schantz, H.F. 1982. The History of OCR, Optical Character Recognition. Manchester Center, VT: Recognition Technologies Users Association. SeeClickFix. 2013. Report non.emergency issues, receive alerts in your neighbourhood, http://en. seeclickfix.com (accessed January 16, 2014). Silvertown, J. 2009. A new dawn for citizen science. Trends in Ecology & Evolution 24 (9), 467–71. September 11 Digital Archive. 2011. About the September 11 Digital Archive. http:// 911digitalarchive.org/about (accessed January 29, 2014). Shillingsburg, P.L. 2006. From Gutenberg to Google: Electronic Representations of Literary Texts. Cambridge: Cambridge University Press. Soldier Studies. 2014. Civil War Voices. http://www. soldierstudies.org (accessed January 29, 2014). Stadsarchief Amsterdam. 2012. Actie Overgenomen Delen. https://stadsarchief. amsterdam.nl/archieven/archiefbank/actie_ overgenomen_delen (accessed June 20, 2015). Text Encoding Initiative. 2014. P5: Guidelines for Electronic Text Encoding and Interchange. http://www.tei.c.org/release/doc/tei.p5.doc/en/ html (accessed January 29, 2014). Weingart, S.B. 2013. Submissions to Digital Humanities 2014. The Scottbot irregular. http://www.scottbot.net/HIAL/?p=39588 (accessed January 28, 2014). 