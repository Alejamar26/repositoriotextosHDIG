CROWDSOURCING OUR CULTURAL HERITAGE Digital Research in the Arts and Humanities Series Editors Marilyn Deegan, Lorna Hughes, Andrew Prescott and Harold Short Digital technologies are becoming increasingly important to arts and humanities research, expanding the horizons of research methods in all aspects of data capture, investigation, analysis, modelling, presentation and dissemination. This important series will cover a wide range of disciplines with each volume focusing on a particular area, identifying the ways in which technology impacts on specific subjects. The aim is to provide an authoritative reflection of the ‘state of the art’ in the application of computing and technology. The series will be critical reading for experts in digital humanities and technology issues, and it will also be of wide interest to all scholars working in humanities and arts research. Other titles in the series Digital Archetypes Adaptations of Early Temple Architecture in South and Southeast Asia Sambit Datta and David Beynon ISBN 978 1 4094 7064 9 Paradata and Transparency in Virtual Heritage Edited by Anna Bentkowska-Kafel, Hugh Denard and Drew Baker ISBN 978 0 7546 7583 9 Art Practice in a Digital Culture Edited by Hazel Gardiner and Charlie Gere ISBN 978 0 7546 7623 2 Digital Research in the Study of Classical Antiquity Edited by Gabriel Bodard and Simon Mahony ISBN 978 0 7546 7773 4 Crowdsourcing our Cultural Heritage Edited by MIA RIDGE Open University, UK © Mia Ridge 2014 All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise without the prior permission of the publisher. Mia Ridge has asserted her right under the Copyright, Designs and Patents Act, 1988, to be identified as the editor of this work.  Published by  Ashgate Publishing Limited  Ashgate Publishing Company  Wey Court East  110 Cherry Street  Union Road    Suite 3-1  Farnham  Burlington, VT 05401-3818  Surrey, GU9 7PT  USA  England  www.ashgate.com  British Library Cataloguing in Publication Data A catalogue record for this book is available from the British Library The Library of Congress has cataloged the printed edition as follows: Ridge, Mia.  Crowdsourcing our cultural heritage / by Mia Ridge.       pages cm. – (Digital research in the arts and humanities) Includes bibliographical references and index. ISBN 978-1-4724-1022-1 (hardback) – ISBN 978-1-4724-1023-8 (ebook) – ISBN 978-1-4724-1024-5 (epub)  1. Cultural property–Management. 2. Cultural property–Philosophy. 3.  Human computation. 4. Digital media–Social aspects. 5. Museums–Collection management. 6. Collection management (Libraries) 7. Library materials–Digitization. 8.  Archival materials–Digitization.  I. Title.   CC135.R53 2014 363.6'90681–dc23                                                            2014011137 ISBN 9781472410221 (hbk) ISBN 9781472410238 (ebk – PDF) ISBN 9781472410245 (ebk – ePUB) Printed in the United Kingdom by Henry Ling Limited, at the Dorset Press, Dorchester, DT1 1HD Contents List of Figures List of Tables List of Abbreviations Notes on Contributors Series Preface  vii xi xiii xv xxi  Crowdsourcing Our Cultural Heritage: Introduction Mia Ridge  1  PART I: CASE STUDIES  1  Crowdsourcing in Brooklyn Shelley Bernstein  17  2  Old Weather: Approaching Collections from a Different Angle Lucinda Blaser  45  3  ‘Many Hands Make Light Work. Many Hands Together Make Merry Work’: Transcribe Bentham and Crowdsourcing Manuscript Collections Tim Causer and Melissa Terras  57  4  Build, Analyse and Generalise: Community Transcription of the Papers of the War Department and the Development of Scripto Sharon M. Leon  89  5  What’s on the Menu?: Crowdsourcing at the New York Public Library Michael Lascarides and Ben Vershbow  113  6  What’s Welsh for ‘Crowdsourcing’? Citizen Science and Community Engagement at the National Library of Wales Lyn Lewis Dafis, Lorna M. Hughes and Rhian James  139  7  Waisda?: Making Videos Findable through Crowdsourced Annotations Johan Oomen, Riste Gligorov and Michiel Hildebrand  161  8  Your Paintings Tagger: Crowdsourcing Descriptive Metadata for a National Virtual Collection Kathryn Eccles and Andrew Greg  185  PART II: CHALLENGES AND OPPORTUNITIES OF CULTURAL HERITAGE CROWDSOURCING  9  Crowding Out the Archivist? Locating Crowdsourcing within the Broader Landscape of Participatory Archives Alexandra Eveleigh  211  10  How the Crowd Can Surprise Us: Humanities Crowdsourcing and the Creation of Knowledge Stuart Dunn and Mark Hedges  231  11  The Role of Open Authority in a Collaborative Web Lori Byrd Phillips  247  12  Making Crowdsourcing Compatible with the Missions and Values of Cultural Heritage Organisations Trevor Owens  269  Index   281  List of Figures 1.1 The online evaluation tool used for Click! A Crowd-Curated Exhibition allowed participants to view images and rate them on a sliding scale. Screenshot: Brooklyn Museum Website 22 1.2 Click! A Crowd-Curated Exhibition was installed in the Brooklyn Museum from 27 June to 10 August 2008. Photograph: Brooklyn Museum 24 1.3 The online evaluation tool designed for Split Second: Indian Paintings displayed two objects side by side and asked participants to select as quickly as possible which painting they preferred from the pair. Screenshot: Brooklyn Museum Website 26 1.4 Split Second: Indian Paintings culminated in an exhibition at the Brooklyn Museum which ran from 13 July 2011 to 1 January 1 2012. Photograph: Brooklyn Museum 27 1.5 1,708 artists throughout Brooklyn registered to open their studios for GO. Map: Brooklyn Museum 32 1.6 GO neighbourhood coordinators worked throughout the project at the local level to connect artists, voters and volunteers, often through meetups held at small venues. Photograph: Brooklyn Museum 34 1.7 Every participating GO artist was assigned a unique number; voters would visit studios and use this number to log their visit by text message, through the GO app, or by writing it down and later entering it on the GO website. Photograph: Brooklyn Museum 35 1.8 During the open studio weekend, approximately 18,000 people logged 147,000 studio visits to artists throughout Brooklyn. Photograph: Brooklyn Museum 36 1.9 GO: a community-curated open studio project opened at the Brooklyn Museum on 1 December 2012 and ran through 24 February 2013. Photograph: Brooklyn Museum 39 3.1 The Transcribe Bentham ‘Transcription Desk’ platform 62 3.2 The Transcribe Bentham transcription interface, and transcription toolbar 64 3.3 Transcribe Bentham results, 8 September 2010 to 19 July 2013 66 3.4 Upgraded Transcription Desk in ‘maximised’ mode, showing rotated image, transcription toolbar and tabbed transcription interface 69 3.5 Upgraded Transcription Desk in ‘maximised’ mode, showing rotated image and preview of encoded transcript 70 3.6 Manuscript JB/050/135/001, courtesy UCL Library Special Collections. Image taken by UCL Creative Media Services 78 3.7 Time spent checking submitted transcripts, in seconds, 1 October 2012 to 19 July 2013 79 3.8 Changes made to text and mark-up of submitted transcripts, 1 October 2012 to 19 July 2013 82 4.1 Papers of the War Department, 1784–1800 website 90 4.2 PWD transcription interface 99 4.3 Total registered users and documents complete in comparison to active transcribers over 90 days 101 4.4 Number of edits from the most active users 102 4.5 Reasons for requesting a transcription account 103 4.6 Word frequency within stated transcriber interests 104 4.7 Scripto architecture schema 106 4.8 Scripto website 108 4.9 DIY History website 109 5.1 The user interface for the NYPL’s Map Warper (http://maps.nypl.org/warper) 116 5.2 Early prototype of the What’s on the Menu? transcription interface 120 5.3 Revised beta version of What’s on the Menu? transcription interface as it appeared at launch 121 5.4 What’s on the Menu? home page, two and a half weeks after launch 124 5.5 Graph of site visits over time. Source: NYPL 125 5.6 The redesigned What’s on the Menu? home page 127 5.7 The redesigned What’s on the Menu? dish detail page, showing visualisation tools 128 5.8 One of the only incidents of intentional vandalism we could find in the first six months; it is a fairly highbrow one, at that 133 6.1 The home page of Cymru1900Wales 147 6.2 Walter Sheppard on a camel in Cairo, c. 1917 150 6.3a Part of will of David ap John ap John, 1609, St Asaph Probate Records, SA1609–96. Source: NLW 154 6.3b Part of will of John Scurlock, Waungaled, Abergwili, Carmarthen, 1851, St David’s Probate Records, SD1851–276. Source: NLW 155 7.1 Digital content life cycle and crowdsourcing (adopted from the Make it Digital Guides, licensed under a Creative Commons Attribution 3.0 New Zealand Licence) 164 7.2 Home page 171 7.3 Game interface 172 7.4 Game recap 172 8.1 Comparison of motivations of Galaxy Zoo and Your Paintings Tagger volunteers 198 8.2 Comparison of motivations of Galaxy Zoo and Your Paintings Tagger volunteers (percentage of taggers and their motivations) 199 8.3 Responses to whether the ‘option for discussions with other taggers about the project’ would be likely to encourage more people to tag more paintings 201 8.4 Your Paintings Tagger achievement levels and productivity as of 17 June 2013 204 8.5 H. Quinton, The 4th Royal Irish Dragoon Guards on the way to the Crimea 1854, oil on canvas, 1869, The Military Museum of the Dragoon Guards 205 9.1 A user participation matrix 217 This page has been left blank intentionally List of Tables 3.1 Number of manuscripts worked on by volunteers, 8 September 2010 to 19 July 2013 73 3.2 Contributions of Transcribe Bentham’s Super Transcribers, 8 September 2010 to 19 July 2013 74 3.3 Time spent on quality control process, 8 September 2010 to 19 July 2013 76 3.4 Summary of quality control process, 1 October 2012 to 19 July 2013 77 3.5 Editorial intervention in manuscripts submitted between 1 October 2012 and 19 July 2013 81 3.6 Quality control of submitted transcripts, 1 October 2012 to 19 July 2013 83 6.1 Models of crowdsourcing projects in libraries, archives and museums, based on work by Oomen and Aroyo 145 7.1	 Classificationofcrowdsourcinginitiatives 163 7.2 Waisda? tag distribution over GTAA facets and Cornetto synset types 176 8.1 Summary of tagging tasks and properties 194 8.2 Compared motivations of Galaxy Zoo and Your Paintings Tagger volunteers 197 8.3 Your Paintings Tagger achievement levels and productivity as of 17 June 2013 203 10.1 Categories of humanities crowdsourcing processes 239 This page has been left blank intentionally List of Abbreviations AHRC UK Arts and Humanities Research Council APIs Application programming interfaces ARA Archives and Records Association (UK and Ireland) BBC British Broadcasting Corporation CDWA Categories for the Description of Works of Art DEP Data Enhancement Programme GLAMs Galleries, libraries, archives and museums GTAA Dutch acronym of the Common Thesaurus Audiovisual Archives HTR Handwritten Text Recognition JISC Formerly the Joint Information Systems Committee, now Jisc. KRO Katholieke Radio Omroep, Dutch public broadcasting organisation MBH Man Bijt Hond (in English, Man Bites Dog), Dutch television show NAGPRA Native American Graves Protection and Repatriation Act of 1990 NCRV Nederlandse Christelijke Radio Vereniging, Dutch broadcaster NEH-ODH US National Endowment for the Humanities Office of Digital Humanities NHPRC US National Archives and Records Administration’s National Historical Publications and Records Commission NIRP Scottish National Inventory Research Project NLW National Library of Wales NYPL New York Public Library OED Oxford English Dictionary OCR Optical Character Recognition PCF Public Catalogue Foundation PWD The Papers of the War Department, 1784–1800 RRCHNM Roy Rosenzweig Center for History and New Media RRN Reciprocal Research Network TEI Text Encoding Initiative UCL University College London UGC User-generated content ULAN Union List of Artists’ Names ULCC University of London Computer Centre V&A Victoria and Albert Museum XML Extensible Mark-up Language YPT Your Paintings Tagger This page has been left blank intentionally Notes on Contributors About the Editor: Mia Ridge is researching a PhD in the Department of History at the Open University, United Kingdom, investigating effective designs for participatory digital history and exploring historians’ use, evaluation of and contributions to scholarly crowdsourcing projects. She has published and presented widely on various topics including user experience research and design for cultural heritage. Mia has led workshops teaching design for crowdsourcing in cultural heritage and academia for groups such as the British Library’s Digital Scholarship programme and the Digital Humanities 2013 conference. Formerly Lead Web Developer at the Science Museum Group (UK), Mia has worked internationally as a business analyst, digital consultant and web programmer in the cultural heritage and commercial sectors, including roles at Museum Victoria (Australia) and the Museum of London. She is Chair of the Museums Computer Group (MCG), a member of the Executive Council of the Association for Computers and the Humanities (ACH) and serves on several museum and digital humanities conference programme committees and project steering groups. Mia has post-graduate qualifications in software development (RMIT University, 2001) and an MSc in Human-Centred Systems (City University, London, 2011). The museum crowdsourcing games Mia designed, built and evaluated for her MSc dissertation project were nominated as a case study of ‘outstanding digital practice in the heritage sector in the UK and internationally’ by the UK’s Heritage Lottery Fund (HLF). About the Contributors: Shelley Bernstein is the Vice Director of Digital Engagement & Technology at the Brooklyn Museum where she works to further the Museum’s community-oriented mission through projects including free public wireless access, web-enabled comment books, projects for mobile devices and putting the Brooklyn Museum collection online. She is the initiator and community manager of the Museum’s initiatives on the social web. She organised Click!, a crowd-curated exhibition, Split Second: Indian Paintings, and GO: a community-curated open studio project. In 2010, Shelley was named one of the 40 Under 40 in Crain’s New York Business and she has been featured in the New York Times. She can be found biking to work or driving her 1974 VW Super Beetle in Red Hook, Brooklyn with her dog Teddy. Lucinda Blaser is a Digital Project Manager at Royal Museums Greenwich. For the past five years she has led the Museum’s digitisation programme and worked on a variety of projects to enhance creatively the Museum’s collection. Lucinda has represented the Museum in its work in citizen science and crowdsourcing projects which range from participation in Old Weather to the transcription of 1915 Merchant Navy crew lists. Lori Byrd Phillips is the Digital Marketing Content Coordinator at The Children’s Museum of Indianapolis. Lori holds a Masters in Museum Studies from Indiana University and a BA in History from George Mason University. She is a leader in the GLAM-Wiki initiative, an international group of volunteer Wikipedians who help cultural institutions share resources through collaborations with Wikipedia. In 2012 she served as the US Cultural Partnerships Coordinator for the Wikimedia Foundation and established the GLAM-Wiki US Consortium, a network of institutions and individuals that support one another in the pursuit of Wikipedia projects in the cultural sector. Tim Causer is a Research Associate at the Bentham Project, Faculty of Laws at University College London (UCL). He joined the Bentham Project in October 2010, and is responsible for the coordination and day-to-day running of Transcribe Bentham, the award-winning collaborative transcription initiative. Tim is a historian of convict transportation, and carried out his PhD research on the infamous Norfolk Island penal settlement (1825–55) at the Menzies Centre for Australian Studies, King’s College London, which was supported by an Arts and Humanities Research Council Doctoral Award. He is currently writing up this research for publication in articles and, ultimately, a book. In 2010, Tim gave a keynote lecture on his work at the Professional Historians’ Association (NSW)’s 25th anniversary conference at Norfolk Island. He also holds an undergraduate MA and MLitt in history from the University of Aberdeen. He also acts as editor of the Journal of Bentham Studies, Associate Editor of Australian Studies (formerly the journal of the British Australian Studies Association) and was a member of the advisory board for the ‘Digital Communities’ category of the 2012 and 2013 Prix Ars Electronica. Lyn Lewis Dafis is Head of digitisation, description and legacy acquisitions at the National Library of Wales. Previously a curator of photographs and metadata manager, at present he manages the Library’s varied special collections, their digitisation and description and is responsible for metadata standards at the institution. Stuart Dunn is a lecturer in Digital Humanities at King’s College London. He graduated from the University of Durham with a PhD in Aegean Bronze Age Archaeology in 2002, conducting fieldwork and research visits in Athens, Melos, Crete and Santorini, as well as excavating in Northumberland. Having developed research interests in geographic information systems, Stuart subsequently became a Research Assistant on the AHRC’s ICT in Arts and Humanities Research Programme. In 2006, he became a Research Associate at the Arts and Humanities e-Science Support Centre at King’s College London, and subsequently a Research Fellow in CeRch and lecturer. Stuart manages/ contributes to several projects in the area of visualisation, GIS and digital humanities. He has research interests in the use of digital methods in landscape history, spanning digital archaeology, visualisation in cultural heritage and GIS. He has published in all these areas; is a co-organiser of the London Digital Classicist group and chairs the Electronic Visualisation and the Arts London conference, under the auspices of the Computer Arts Society and the BCS. He has worked on the digital reconstruction of Iron Age round houses, on the construction of a digital gazetteer of historic English place names and on various web 2.0 digital community projects, especially those involving crowdsourcing. In 2012, he co-led a Crowdsourcing Scoping Study funded by the AHRC’s Connected Communities programme (www.stuartdunn.wordpress.com). Kathryn Eccles is a Research Fellow at the Oxford Internet Institute, University of Oxford. Kathryn’s research interests lie in the impact of new technologies on public engagement with cultural heritage, and on scholarly behaviour and research in the Humanities. She recently completed an AHRC Early Career Fellowship, which focused on the impact of crowdsourcing on the digital art collection Your Paintings, and continues to research in the area of public participation, and the interface between academics, cultural heritage and the public. Previous research has focused on the usage and impact of digitised scholarly resources, the impact of digital transformations on the Humanities and the role of e-infrastructures in the creation of global virtual research communities. She holds a DPhil in Modern History from the University of Oxford, and a BA and MPhil from the University of Birmingham. Alexandra Eveleigh is currently completing a PhD thesis on the impact of user participation on archival theory and practice. Her research has been funded by an Arts and Humanities Research Council collaborative doctoral award, jointly supervised by the Department of Information Studies at University College London (UCL) and The National Archives in the UK, and a UCL cross-disciplinary scholarship in conjunction with the UCL Interaction Centre (UCLIC). She previously worked as Collections Manager at West Yorkshire Archive Service, and prior to this as an archivist at the University of Southampton. She is particularly interested in digital technologies in an archival context, and is a Winston Churchill Fellow in connection to her work on local digital archives. Riste Gligorov is a PhD student in the Web & Media group at the Vrije Universiteit Amsterdam. His research interests include social tagging and games with a purpose for annotation and retrieval of video content. Gligorov has an MSc in computer science and engineering from the Technical University of Denmark. Andrew Greg is Director of the National Inventory Research Project (NIRP) in the College of Culture and Creative Arts, University of Glasgow. He studied History of Art at the University of Cambridge and spent a career in fine art curatorship and museum management in the Midlands and North-East of England. His curatorial interests included fine and decorative arts, contemporary craft and architecture. Since 2001 he has worked on museum collection research and digitisation projects, including NIRP, and, with the Public Catalogue Foundation, Your Paintings and the pioneering crowdsourcing project Your Paintings Tagger. He is currently interested both in promoting the public knowledge and understanding of art and museum collections, and in the creation, structuring and dissemination of cataloguing data. Mark Hedges is the Director of the Centre for e-Research at King’s College London, and a Senior Lecturer in the Department of Digital Humanities, teaching on a variety of modules in the MA in Digital Asset and Media Management. His original academic background was in mathematics and philosophy, and he gained a PhD in mathematics at University College London, before starting a 17-year career in the software and systems consultancy industry, working on large-scale development projects for industrial and commercial clients. After a brief career break – during which he studied Late Antique and Byzantine Studies – he began his career at King’s in the Arts and Humanities Data Service. His research interests include digital libraries and archives, and the application of computational methods and ‘big data’ in the humanities, social sciences and culture. He was recently PI of a Crowdsourcing Scoping Study funded by the AHRC’s Connected Communities programme. Michiel Hildebrand is a Postdoctoral researcher at the VU University Amsterdam in the Web & Media group and also at CWI in the Information Access group. He researches interactive information systems for Linked Data and Media. He is work package manager in the European project LinkedTV and is part of the Dutch research project Data2Semantics. Lorna M. Hughes is the University of Wales Chair in Digital Collections, based in the National Library of Wales. She is a Senior Research Fellow at the Centre for Advanced Welsh and Celtic Studies. Lorna leads a research programme based around the digital collections of the National Library of Wales, with a particular focus on understanding the use, value and impact of digital resources on research, teaching and public engagement. She is particularly interested in the use of ICT tools and methods for the analysis of large-scale digital crowdsourcing of our cultural heritage collections, and in research collaborations between humanities and scientific disciplines. Prior to taking up her appointment in January 2011, she worked at King’s College London, most recently as the Deputy Director of the Centre for e-Research, and the co-Director of the Arts and Humanities e-Science Support Centre (AHeSSC). From 2005 to 2008, she was Programme Director for the AHRC ICT Methods Network, a national initiative to promote and support the use of digital research across the arts and humanities disciplines. She has worked in digital humanities at New York University, Arizona State University, Oxford University and Glasgow University. She is the author of Digitizing Collections: Strategic Issues for the Information Manager (London: Facet, 2004), the editor of Evaluating & Measuring the Value, Use and Impact of Digital Collections (London: Facet, 2011), and the co-editor of The Virtual Representation of the Past (London: Ashgate, 2007). She is presently Chair of the European Science Foundation (ESF) Network for Digital Methods in the Arts and Humanities (www.nedimah.eu), and the PI on a JISC-funded mass digitisation initiative The Welsh Experience of the First World War (cymru1914.org). Rhian James is a full–time PhD candidate at the University of Wales Centre for Advanced Welsh and Celtic Studies, focusing on the potential of digital humanities approaches to large corpora of unstructured archival data, specifically in the context of the digitised wills and probate collection held at the National Library of Wales (NLW). Her research is funded by a University of Wales scholarship and is co-supervised through the NLW research programme in digital humanities. Prior to this, she completed an MScEcon in archive administration at Aberystwyth University and has worked for periods as an assistant archivist at NLW and at Powys County Archives Office. Rhian is currently exploring the feasibility of community generated manuscript transcription in a library and archive setting. Michael Lascarides is the Manager of the New Zealand National Library Online (part of the DigitalNZ team) and the author of the book Next-Gen Library Redesign (from ALA Press). Prior to moving to New Zealand in 2012, he was head of the New York Public Library’s web design and development team and a member of the MFA Computer Art faculty at the School of Visual Arts. Sharon M. Leon is the Director of Public Projects at the Roy Rosenzweig Center for History and New Media and Associate Professor of History at George Mason University. Leon received her bachelor of arts degree in American Studies from Georgetown University in 1997, and her doctorate in American Studies from the University of Minnesota in 2004. Her first book, An Image of God: The Catholic Struggle with Eugenics, was published by University of Chicago Press (May 2013). Her work has appeared in Church History, the Journal of the History of Medicine and Allied Sciences, the Public Historian and a number of edited collections. She is currently doing research on the Catholic Left in the United States after Vatican II. At RRCHNM, Leon oversees collaborations with library, museum and archive partners from around the country. She directs the Center’s digital exhibit and archiving projects, as well as research and tool development for public history, including Omeka and Scripto. Finally, Leon writes and presents on using technology to improve the teaching and learning of historical thinking skills. Johan Oomen is Head of the R&D Department of the Netherlands Institute for Sound and Vision and researcher at the Web and Media group of the Vrije Universiteit Amsterdam. In 2012 he was elected as Network Officer for Europeana and board member of CLICK-NL, the Dutch Creative Industries knowledge and innovation network. His PhD research at the VU University focuses on how active user engagement can help to establish a more open, smart and connected cultural heritage. Oomen holds a BA in Information Science and an MA in Media Studies. He has worked for the British Universities Film and Video Council and RTL Nederland. Trevor Owens is a digital archivist at the National Digital Information Infrastructure and Preservation Program (NDIIPP) in the Office of Strategic Initiatives at the Library of Congress and a doctoral student at GMU. He is interested in online communities, digital history, and video games. He blogs at http://www.trevorowens.org/, and at playthepast.org. Melissa Terras is Director of UCL Centre for Digital Humanities and Professor of Digital Humanities in UCL’s Department of Information Studies. With a background in Classical Art History, English Literature and Computing Science, her doctorate (University of Oxford) examined how to use advanced information engineering technologies to interpret and read Roman texts. Publications include Image to Interpretation: Intelligent Systems to Aid Historians in the Reading of the Vindolanda Texts (Oxford University Press, 2006) and Digital Images for the Information Professional (Ashgate, 2008). She is the secretary of the European Association of Digital Humanities, on the board of the Alliance of Digital Humanities Organisations, and the General Editor of Digital Humanities Quarterly. Her research focuses on the use of computational techniques to enable research in the arts and humanities that would otherwise be impossible, and she is one of the project co-investigators for Transcribe Bentham.You can generally find her on twitter @melissaterras. Ben Vershbow is founder and manager of NYPL Labs, an in-house technology startup at The New York Public Library which has won awards for its inventive handling of archives and special collections online. Investigating what a public memory organisation can be in the age of the network, Labs projects invite deep interaction with library materials, collaborating directly with users on the creation of new digital resources, data sets and tools. Before joining NYPL, Ben worked for four years with Bob Stein at the Institute for the Future of the Book, a Brooklyn-based think tank exploring the future of reading, writing and publishing. Ben studied theatre at Yale and is active as a writer/director/performer around New York, creating original work with his company Group Theory. Series Preface This series explores the various ways by which engagement with digital technologies is transforming research in the arts and humanities. Digital tools and resources enable humanities scholars to explore research themes and questions which cannot be addressed using conventional methods, while digital artists are reshaping such concepts as audience, form and genre. Digital humanities is a convenient umbrella term for these activities, and this series exemplifies and presents the most exciting and challenging research in the digital humanities. Digital humanities encompass the full spectrum of arts and humanities work, and scholars working in the digital humanities are strongly committed to interdisciplinary and collaborative methods. Consequently the digital humanities are inextricably bound to a changing view of the importance of the arts and humanities in society and provide a space for restating and debating the place of arts and humanities disciplines within the academy and society more widely. As digital technologies fundamentally reshape the sociology of knowledge, they challenge humanities scholars and artists to address afresh the fundamental cognitive problem of how we know what we know. Computing is the modelling of method, and this series reflects the belief that digital humanities proceeds by examining from many different perspectives the methods used in the arts and humanities, in some cases modifying and extending them, and in others drawing on relevant fields to develop new ones. The volumes in this series describe the application of formal computationally based methods in discrete but often interlinked areas of arts and humanities research. The distinctive issues posed by modelling and exploring the archives, books, manuscripts, material artefacts and other primary materials used by humanities scholars, together with the critical and theoretical perspectives brought to bear on digital methods by the arts and humanities, form the intellectual core of the digital humanities, and these fundamental intellectual concerns link the volumes of this series. Although generally concerned with particular subject domains, tools or methods, each title in this series is accessible to the arts and humanities community as a whole. Individual volumes not only stand alone as guides but collectively provide a survey of ‘the state of the art’ in research on the digital arts and humanities. Each publication is an authoritative statement of current research at the time of publication and illustrate the ways in which engagement with digital technologies are changing the methods, subjects and audiences of digital arts and humanities. While reflecting the historic emphasis of the digital humanities on methods, the series also reflects the increasing consensus that digital humanities should have a strong theoretical grounding and offers wider critical perspectives in the humanities. The claim that digital humanities is an academic discipline is frequently controversial, but the range and originality of the scholarship described in these volumes is in our view compelling testimony that digital humanities should be recognised as a major field of intellectual and scholarly endeavour. These publications originally derived from the work of the AHRC ICT Methods Network, a multi-disciplnary partnership which ran from 1 April 2005 to 31 March 2008 providing a national forum for the exchange and dissemination of expertise, with funding from the UK’s Arts and Humanities Research Council. The success of this network in generating strong synergies across a wide community of researchers encouraged the continuation of this series, which bears witness to the way in which digital methods, tools and approaches are increasingly featuring in every aspect of academic work in the arts and humanities. Crowdsourcing Our Cultural Heritage: Introduction Mia Ridge This book brings together for the first time the collected wisdom of international leaders in the theory and practice of the emerging field of cultural heritage crowdsourcing. It features eight accessible case studies of groundbreaking projects from leading cultural heritage and academic institutions, and four thought-provoking essays that reflect on the wider implications of this engagement for participants and on the institutions themselves. Crowdsourcing, originally described as the act of taking work once performed within an organisation and outsourcing it to the general public through an open call for participants,1 is becoming increasingly common in museums, libraries, archives and the humanities as a tool for digitising or computing vast amounts of data, whether the private correspondence of eighteenth-century English philosophers (Chapter 3) or modern Dutch popular television (Chapter 7). Asking members of the public to help with tasks can be hugely productive – for example, participants in the Old Weather project (Chapter 2) transcribed over a million pages from thousands of Royal Navy logs in less than two years,2 the entire 1940 US Census was indexed by 160,000 volunteers in just four months,3 the National Library of Australia’s Trove project has over 130 million transcription corrections and more than 2.8 million tags4 and participants in the British Library’s Georeferencer project have added spatial coordinates to thousands of historic maps.5 And cultural heritage crowdsourcing is not limited to transforming existing content into digital formats – Museum Victoria’s Describe Me is crowdsourcing descriptions of their objects for people who are blind,6 Snapshot Serengeti asks people to identify animals recorded by remote cameras7 and Galaxy Zoo’s Quench project asks ‘citizen scientists’to help analyse results and collaborate with scientists to write 1.Howe, ‘The Rise of Crowdsourcing’. 2.Brohan, ‘One Million, Six Hundred Thousand New Observations’. 3.1940 US Census Community Project. 4.As of June 2014. Current figures are listed at http://trove.nla.gov.au/system/ stats?env=prod. 5.http://www.bl.uk/maps/. 6.http://describeme.museumvictoria.com.au/. 7.Kosmala, ‘Some Results from Season 4’. an article on their findings.8 But crowdsourcing in cultural heritage is more than a framework for creating content: as a form of engagement with the collections and research of memory institutions, it benefits both audiences and institutions. Cultural heritage crowdsourcing projects ask the public to undertake tasks that cannot be done automatically, in an environment where the activities, goals (or both) provide inherent rewards for participation, and where their participation contributes to a shared, significant goal or research interest. Crowdsourcing can be immensely effective for engaging audiences with the work and collections of galleries, libraries, archives and museums (GLAMs), and there is growing evidence that typical GLAM crowdsourcing activities encourage skills development and deeper engagement with cultural heritage and related disciplines.9 For organisations whose missions encompass engaging people with cultural heritage, there is sometimes a sense that, as Trevor Owens says in Chapter 12, the transcriptions produced are a ‘wonderful by-product’ of creating meaningful activities for public participation. This book will help practitioners who wish to create their own crowdsourcing projects understand how other institutions found the right combination of source material and tasks for their ‘crowd’–typically, a combination of casual participants and dedicated ‘super contributors’ working online – to achieve the desired results. Building a successful crowdsourcing project requires an understanding of the motivations for initial and on-going participation, the characteristics of tasks suited to crowdsourcing and the application of best practices in design for participation, content validation, marketing and community building. For readers interested in the workings of museums, libraries, archives and academia, this volume is an opportunity to hear from people behind the projects about their goals, their experiences building and launching crowdsourcing sites, what worked and what did not, how their designs improved over successive iterations and how these projects changed the host organisation. Sharon Leon’s report (Chapter 4) that almost 10 per cent of peopleregistering to use the Scripto tool were motivated by curiosity about the transcription tool and process suggests the need for this collection of in-depth reports. The case studies in Part I of this book discuss a range of approaches taken to various materials, audiences and goals by a selection of internationally significant projects in museums, libraries, archives and universities. Part II features theoretical reflections on the impact of crowdsourcing on GLAM professionals; institutional relationships with audiences; public engagement and organisational mission; and the implications of new models of authority. Together, the chapters collected here will help organisations understand both the potential of crowdsourcing, and the practical and philosophical implications of inviting the public to work with them on our shared cultural heritage. 8.Trouille, ‘Galaxy Zoo Quench’. 9.Ridge, ‘From Tagging to Theorizing’; Dunn and Hedges, ‘Crowd-Sourcing Study’. Background and Context As the pioneering projects described here inspire others, it is an apt moment to reflect on the lessons to be learnt from them. The projects discussed range from crowd-curated photography and art exhibitions to collecting objects at in-person ‘roadshow’ events. The number of projects in the emerging field of cultural heritage crowdsourcing increases constantly and the subsequent lessons learnt by museums, libraries, archives and academia are gradually being absorbed back into those institutions and in turn inspire new ideas. A range of disciplines and roles have informed the perspectives collected here. They range from historians interested in scholarly editions of archival documents, to technologist- and collections-led public engagement and data enhancement projects in museums, to archivists considering the challenges of participatory archives. Further differences are apparent in the approaches museums, libraries and archives have developed for managing physical collections and the knowledge around them, and in their preferred forms of public access and engagement. However, as designs for online collections tend to follow similar principles, the disciplinary differences between the providers of those collections appear to be converging (at least from the audiences’ perspective).10 Defining ‘Crowdsourcing’ and Related Concepts Since its coining by Jeff Howe and Mark Robinson in 2006, the term ‘crowdsourcing’ has been used as a label for a variety of new and pre-existing concepts. It is worth returning to Jeff Howe’s ‘White Paper Version’of their definition: ‘Crowdsourcing is the act of taking a job traditionally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call.’11 Interestingly, Howe’s ‘soundbyte’ definition of crowdsourcing – the ‘application of Open Source principles to fields outside of software’–does not retain the problematic relationship with ‘outsourcing’, instead claiming an affinity with the highly skilled activities and mutually beneficial ethos of open source software development. Crowdsourcing in cultural heritage benefits from its ability to draw upon the notion of the ‘greater good’ in invitations to participate, and this may explain why projects generally follow collaborative and cooperative, rather than competitive, models. Concepts often grouped under the same ‘umbrella’ in the commercial crowdsourcing sector include ‘crowd contests’, or ‘asking a crowd for work and only providing compensation to the chosen entries’12 and the ‘wisdom of crowds’ (collective decision-making or 10.For further discussion of this, see Duff et al., ‘From Coexistence to Convergence’. 11.Undated quote in the sidebar of Howe, ‘Crowdsourcing: A Definition’. 12.Bratvold, ‘Defining Crowdsourcing’s Taxonomy’. For an account of the dangers of crowd contests for GLAMs, see Sweetapple, ‘How the Sydney Design Festival Poster Competition Went Horribly Wrong’. problem-solving), which is referred to in several chapters (particularly Chapter 1, but also Chapters 6, 7, 10 and 12). Crowdfunding, or crowdsourced fundraising, makes only a brief appearance (see Chapter 10) but is obviously an issue in which many institutions are interested. At first, GLAM crowdsourcing projects may look similar to Web 2.0-style user-generated content (UGC) projects which invite audiences to ‘have your say’. However, crowdsourcing projects are designed to achieve a specific goal through audience participation, even if that goal is as broadly defined as ‘gather information from the public about our collections’. Citizen science, in which ‘volunteers from the general public assist scientists in conducting research’13 has been an influential model for humanities and ‘citizen history’14 crowdsourcing projects. ‘Crowdsourcing’, whether in the commercial, heritage or academic sectors, is suffering the fate of many buzzwords as its boundaries are pushed by those with something to sell or careers to make. Alexandra Eveleigh points out in Chapter 9 that the term is appliedbroadly, and even retrospectively, to ‘almost any initiative in the field which seeks to engage users to contribute to archives or to comment upon archival practice’(p. 211) online.15 Various definitions of cultural heritage crowdsourcing reveal unresolved tensions about the role of expertise and the disruption of professional status, or lines of resistance to the dissolving of professional boundaries. Ultimately, however, definitions that seek to draw a line around crowdsourcing so that some projects can be ‘in’while others are ‘out’are less useful than thinking of crowdsourcing in cultural heritage as a coalescence around a set of principles, particularly the value placed on meaningful participation and contributions by the public. Defining ‘the Crowd’ in Cultural Heritage Crowdsourcing While ‘crowdsourcing’is a useful shorthand, many projects and writers have used other terms for ‘crowd’participants, such as ‘community-sourcing’(Chapters 4, 11), ‘targeted crowdsourcing’ (Chapter 6), or ‘micro-volunteering’ (Chapter 5), acknowledging that often the crowd is neither large nor truly anonymous, but perhaps also reflecting discomfort with the broadness, anonymity or vagueness of ‘the crowd’. These terms additionally reflect the fact that while some cultural heritage crowdsourcing projects are inspired by a desire for greater public engagement, the more specialised the skills, knowledge or equipment required, the more strongly a ‘crowd-sifting’ effect operates as individuals unable to acquire the necessary attributes fall out from the pool of potential participants (as discussed in Chapter 3). 13.Raddick et al., ‘Galaxy Zoo’. 14.Frankle, ‘More Crowdsourced Scholarship’. 15.See also Estelles-Arolas and Gonzalez-Ladron-de-Guevara, ‘Towards an Integrated Crowdsourcing Definition’ and Ridge, ‘Frequently Asked Questions about Crowdsourcing’. Models for Crowdsourcing in Cultural Heritage The issues facing contemporary crowdsourcing projects are not new. Accepting contributions from members of the public for inclusion in collections documentation and other informatics systems has always raised issues about how to validate those contributions. Nineteenth-century natural historians corresponding with amateur observers about the distribution of botanical specimens had to try to determine the veracity and credibility of their contributions,16 just as modern manuscript transcription projects such as Transcribe Bentham (Chapter 3) initially questioned the editorial quality of volunteer-produced transcripts. The Smithsonian Institution has a long history17 with ‘proto-crowdsourcing’, as does the Oxford English Dictionary (OED), whose editor launched in 1879 an ‘Appeal to the English-speaking and English-reading public’to help provide evidence for the history and usage of words to complete the dictionary.18 Many chapters relate crowdsourcing to long traditions of volunteer augmentations of GLAM collections (see for example Chapter 6). Technology has enabled crowdsourcing as we know it, but models for public participation in collection, research and observation pre-date it. The ability of digital technologies to provide almost instantaneous data gathering and feedback, computationally validate contributions and the ability to reach both broad and niche groups through loose networks have all been particularly important in the modern era. As some chapters explicate, the ability to track data provenance computationally and verify remediated primary sources is particularly important for scholarly projects. Digitisation has also helped manage the limitations of physical space, conservation, location and opening hours that previously affected access to collections.19 UNESCO’s definition of ‘cultural heritage’as ‘the legacy of physical artefacts and intangible attributes […] inherited from past generations’ provides a broad outline for this book.20 Cultural heritage crowdsourcing projects have followed a variety of models, including ‘commons-based peer-production’and participatory archives (see Chapters 4 and 9). The National Library of Australia’s Trove21 Optical Character Recognition (OCR) correction project (and Rose Holley’s excellent articles on its genesis, process and results)22 has been hugely influential. 16.Secord, ‘Corresponding Interests’. 17.For examples, see Millikan, ‘Joseph Henry’ and Bruno, ‘Smithsonian Crowdsourcing since 1849!’. 18.Gilliver, ‘“Your Dictionary Needs You”’. The original text of the 1879 appeal is available at http://public.oed.com/history-of-the-oed/archived-documents/april-1879.appeal/april-1879-appeal/. 19.Ridge, ‘From Tagging to Theorizing’. 20.UNESCO Office in Cairo, ‘Tangible Cultural Heritage’. 21.http://trove.nla.gov.au/. 22.See for example Holley, Many Hands Make Light Work and Holley, ‘Crowdsourcing’. The Zooniverse23 suite of citizen science projects, which began with Galaxy Zoo, has been particularly important, and some cultural heritage organisations have used the Zooniverse software platform for their own projects. Lori Byrd Phillips examines the evolution of the open source model as a form of ‘barn raising’ by online communities in Chapter 11, and several other authors cite the open source software movement as a model for their own projects or have released the code for their crowdsourcing tools under open source licences. Some crowdsourcing projects were inspired by organisational missions –in Chapter 1, Shelley Bernstein relates Brooklyn Museum’s innovative digital projects to their ‘community-driven mission’. Others realise the potential importance of crowdsourcing to their mission through developing projects – Michael Lascarides and Ben Vershbow (Chapter 5) report that the New York Public Library came to regard crowdsourcing ‘not only as a way to accomplish work that might not otherwise be possible, but as an extension of our core mission’(p. 115). In Chapter 6, Lyn Lewis Dafis, Lorna M. Hughes and Rhian James’ translation of ‘crowdsourcing’ into Welsh (‘cyfrannu torfol’) highlights the ‘collective contributions’ and community engagement so important to the National Library of Wales. Common Tasks in Cultural Heritage Crowdsourcing Generally, the tasks performed by participants in cultural heritage crowdsourcing involve transforming content from one format to another (for example, transcribing text or musical notation), describing artefacts (through tags, classifications, structured annotations or free text), synthesising new knowledge, or producing creative artefacts (such as photography or design). Additional semantic context is required for structured text search –for example, searches for specific entities like people, places or events within large datasets – and can be supported through ‘structured transcription’, in which metadata that describe the entity through emergent or externally defined concepts are recorded alongside the transcribed text. Two common approaches to structured transcription are discussed in various chapters. The Transcribe Bentham project (Chapter 3) uses full text transcription wrapped in descriptive ‘inline’ tags where additional information is desired, while user interfaces for Old Weather (Chapter 2) and What’s on the Menu? (Chapter 5) are designed to transcribe relevant sections of text into pre-defined database fields. The inherent variability of materials in cultural heritage collections means that the same class of task – whether transcribing handwriting, tagging a painting or georeferencing a map – could be quick and uncomplicated or could require tricky subjective judgement to accomplish, depending on the legibility of the source material and the cognitive overhead required to (for example) add structured mark-up or choose between hierarchical subject terms. While many chapters focus on digitising documents as varied as wills and menus, other tasks 23.http://www.zooniverse.org/. include crowd curation and creativity with artworks and photography, creating descriptive tags for paintings and time-based annotations for audio-visual archives, and georeferencing maps. Some participants prefer apparently ‘simple’ tasks like correcting errors in OCR-generated transcriptions or classifying images (though the sophisticated visual processing and pattern recognition required is a form of ‘human computation’ that computers cannot easily manage), while others prefer more complex tasks that require subjective judgement or specific skills or knowledge. Key Trends and Issues To paraphrase a military adage, it seems ‘no plan survives contact with the crowd’, and many initiatives change significantly after their initial launch. Several successful case studies report on iterative improvements to interfaces, in part because a high quality ‘user experience’ (particularly task design) is vital for creating interfaces that are both productive and engaging. Chapter 4 discusses improvements to the Scripto interface designed to help transcribers work with documents more effectively, Chapter 5 describes tweaks to the What’s on the Menu? interface and Chapter 3 reports on newly launched (at the time of writing) improvements to the Transcribe Bentham interface. Contact with participant communities also seems to change a project in more fundamental ways, including the development of new research questions. As Lucinda Blaser reports in Chapter 2, Old Weather was initially promoted ‘as a climate science project as this was the scientific goal of the project, but the audience saw it as a historicalresearch project’ (p. 54). If crowdsourcing projects are almost inevitably changed (and changed for the better) by contact with the crowd, they necessarily create a challenge for any organisations and funders used to regarding the website launch as the end of their active involvement with a project. The resources and workflows required for community management (for example, content moderation, communication and updates on progress) and maintaining the supply of content are relatively new for many organisations, even when some tasks can themselves be crowdsourced. When Howe stated that a ‘crucial prerequisite’in crowdsourcing is a ‘perfect meritocracy’based not on external qualifications but on ‘the quality of the work itself’,24 he created a challenge for traditional models of authority and credibility. This challenge underlies many reflections in this volume, particularly those of Lori Byrd Phillips in Chapter 11. Amodel for public participation in science research devised by Bonney et al.25 is useful for categorising non-commercial crowdsourcing projects according to the amount of control participants have over the design of the project itself – or to look at it another way, how much authority the organisation has ceded to the crowd. Their model contains three categories: ‘contributory’, where the public contributes data to a project designed by the organisation; 24.Howe, ‘The Rise of Crowdsourcing’; Howe, Crowdsourcing. 25.Bonney et al., Public Participation in Scientific Research. ‘collaborative’, where the public can help refine project design and analyse data in a project led by the organisation; and ‘co-creative’, where the public can take part in all or nearly all processes, and all parties design the project together. It may be that by providing opportunities to help define questions for study or analyse data (rather than merely contribute it), collaborative project structures are a factor in successfully encouraging deeper engagement with related disciplines. Several chapters (including Chapters 2, 8 and 10) discuss the ways in which the crowd may also be changed by their contact with cultural heritage organisations, interests and collections. Astrength of this volume is the accumulation of insights about participant demographics and motivations and the ways in which participants have developed their skills and experience through crowdsourcing projects. The importance of ‘super contributors’who often do most of the work on a project is also a common theme. Institutional drivers behind the popularity of crowdsourcing include the sheer quantity of archival material and a desire to make better use of collections in the face of reduced funding for digitisation and other collections work. However, it appears that crowdsourcing projects also change the institution and related professions (see, for example, Chapter 9). While the potential savings in staff resources and enhancements to collections are the most obvious benefits of cultural heritage crowdsourcing, deepening relationships with new and pre-existing communities has been important to many organisations. Ultimately, the key trend in cultural heritage crowdsourcing is the extent and pace of constant change. Looking to the Future of Crowdsourcing in Cultural Heritage Currently, crowdsourcing in cultural heritage is mostly focused on using the capacity of interested publics to transform existing content from one format to another, and exploring the ‘wisdom of crowds’ through crowd-curation. However, projects like Old Weather (Chapter 2, see also Chapter 10) demonstrate opportunities for generating new knowledge and research questions, and there is great potential in archive-based participatory digitisation projects embedded in the work researchers are already performing, such as the Papers of the War Department. The discussion of Transcribe Bentham hints at future challenges ahead: improvements in machine learning and computational ability to deal with tasks that were previously better (and enjoyably) performed by people – such as transcribing handwriting, OCR correction, describing images and discerning patterns – might render these activities less meaningful as crowdsourced tasks. Kittur et al. offer a vision of ‘hybrid human-computer systems’ that ‘tap into the best of both human and machine intelligence’,26 but the impact on cultural heritage crowdsourcing remains to be seen. However, crowdsourcing projects continue to evolve to meet these challenges and other changes in the digital and social landscape. For example, the genealogy site FamilySearch released a mobile 26.Kittur, ‘The Future of Crowd Work’. application that allows people to transcribe small ‘snippets’of text on their phone or tablet; a response to technological changes that also encourages participants to help even while ‘waiting to be seated at a restaurant’.27 The Structure and Content of This Book The case studies in Part I offer insights into the genesis of various projects, the motivations of participants and practical lessons for interface design. Some focus on single projects while others present an overview of relevant activities across the whole organisation. In Chapter 1, ‘Crowdsourcing in Brooklyn’, Brooklyn Museum’s Shelley Bernstein looks closely at three large-scale projects grounded in their collections, locale and audiences: Click!, a crowdsourced exhibition; Split Second, an experiment in responsive interpretation; and GO: a community-curated open studio project. She explores their roots in specific research questions and in the Museum’s mission to engage the community. She explains how they were designed for very specific types of participation, and the cumulative impact of these initiatives on the organisation and its goals. In Chapter 2, ‘Old Weather: Approaching Collections from a Different Angle’, Lucinda Blaser explores the potential for citizen science projects to enhance historic collections while also producing genuine scientific results, explaining that in the Old Weather project, ‘many users came for the climate science but stayed for the history’(p. 46). She discusses how crowd-curation and data enhancement projects relate to Royal Museum Greenwich’s mission, and how cultural heritage crowdsourcing and citizen science can unite the riches within collections with passionate and dedicated supporters. In Chapter 3, ‘“Many Hands Make Light Work. Many Hands Together Make Merry Work”: Transcribe Bentham and Crowdsourcing Manuscript Collections’, Tim Causer and Melissa Terras explain the considerable volume and variety of the archive on which University College London’s Transcribe Bentham project is based. They review its value as an experiment with complex, challenging tasks – the opposite of the micro-tasks discussed elsewhere – and the validation required for scholarly editions, and re-evaluate their earlier assessment of the return on investment in crowdsourcing transcription. They also consider the impact of publicity, the importance of super-contributors and introduce their newly redesigned interface. In Chapter 4, ‘Build, Analyse and Generalise: Community Transcription of the Papers of the War Department and the Development of Scripto’, Sharon M. Leon describes the lessons learnt from developing the Scripto application for community transcription of the distributed collections of the Papers of the War Department. She explains how it tapped into the existing user community, 27.Probst, ‘New FamilySearch Indexing App Now Available’. the process of generalising the tool for use as a transcription platform by other projects and its place in the Roy Rosenzweig Center for History and New Media’s philosophy of public history. In Chapter 5, ‘What’s on the Menu?: Crowdsourcing at the New York Public Library’, Michael Lascarides and Ben Vershbow present the New York Public Library’s What’s on the Menu? project, which aimed to turn historical menus into a searchable database, but was so successful at engaging the public that the library had to reorganise workflows to maintain the supply of menus. They discuss the factors that make a crowdsourcing project successful, the goals of various iterations in the interface design and the importance of their public mission to the project. Lyn Lewis Dafis, Lorna M. Hughes and Rhian James discuss the National Library of Wales’ crowdsourcing projects in Chapter 6, ‘What’s Welsh for “Crowdsourcing”? Citizen Science and Community Engagement at the National Library of Wales’, including the Cymru1900Wales place name gathering project, the community content generation exercise around First World War material and their experiments around community transcription of wills for Welsh Wills Online. They relate these projects and crowdsourcing generally to the overall work of the library. In Chapter 7, ‘Waisda?: Making Videos Findable with Crowdsourced Annotations’, Johan Oomen, Riste Gligorov and Michiel Hildebrand present the design decisions behind the social tagging game Waisda? and consider the impact of participatory culture on institutions. They elaborate on the results of extensive evaluations carried out in this long-term research project from the Netherlands Institute for Sound and Vision, one of Europe’s largest audiovisual archives, and VU University Amsterdam, including two large-scale pilots involving thousands of users. Kathryn Eccles and Andrew Greg discuss the Your Paintings Tagger project in Chapter 8, ‘Your Paintings Tagger: Crowdsourcing Descriptive Metadata for a National Virtual Collection’, including the project background and goals. They examine the impact of working with multiple stakeholders (including academics, the BBC and the Public Catalogue Foundation) and understandings of expertise, and the impact this had on design decisions and metadata standards. The results of user research, including a profile of taggers, their motivations for participation and the potential for providing a platform for community are discussed. Part II of this book explores the challenges and opportunities of cultural heritage crowdsourcing, including the potential for better relationships with the public and new ways of thinking about informal education. These chapters also consider the implications of participatory projects for heritage organisations and professionals and current notions of authority. In Chapter 9, ‘Crowding Out the Archivist? Locating Crowdsourcing within the Broader Landscape of Participatory Archives’, Alexandra Eveleigh contrasts the hype around ‘crowdsourcing’with the reality, reflects on the impact crowdsourcing has had on the archival profession and makes a significant contribution in her matrix for conceptually mapping the ‘participatory landscape’ in relation to archives. In Chapter 10, ‘How the Crowd Can Surprise Us: Humanities Crowdsourcing and the Creation of Knowledge’, Stuart Dunn and Mark Hedges examine crowdsourcing from an academic humanities perspective, looking beyond ‘mechanical tasks’ to ‘the creation of complex content and the circulation of knowledge’, and propose a valuable framework for thinking about humanities crowdsourcing in terms of assets, processes, tasks and outputs. Lori Byrd Phillips reflects on the potential for a model of ‘open authority’ to meet the challenge organisations face in balancing institutional expertise with the potential of collaborative online communities. She draws on models from technology, education and museum theory to present solutions for addressing issues of democratisation and voice in a fast-paced digital world in Chapter 11, ‘The Role of Open Authority in a Collaborative Web’. In Chapter 12, ‘Making Crowdsourcing Compatible with the Missions and Values of Cultural Heritage Organisations’, Trevor Owens considers the compatibility of crowdsourcing with the ‘values and missions’of cultural heritage organisations, and concludes that the value of crowdsourcing lies not only in the productivity of the crowd but in ‘providing meaningful ways for the public to enhance collections while more deeply engaging with and exploring them’(p. 279). Taken together, these chapters not only provide an overview of current projects and practices – they also provide a glimpse of the ways in which audiences and institutions can together discover the future of crowdsourcing our cultural heritage. References 1940 US Census Community Project, ‘We Did It! The 1940 US Census Community Project’, 2012. http://us2.campaign-archive2.com/?u=b0de542dc933cfcb848d 187ea&id=c6e095aa92. Bonney, R., H. Ballard, R. Jordan, E. McCallie, T. Phillips, J. Shirk and C.C. Wilderman. Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education. A CAISE Inquiry Group Report. Washington, DC: Center for Advancement of Informal Science Education (CAISE), 2009. http://caise.insci.org/uploads/docs/PPSR%20 report%20FINAL.pdf. Bratvold, D. ‘Defining Crowdsourcing’s Taxonomy – a Necessary Evil’. Daily Crowdsource, 2011. http://dailycrowdsource.com/2011/09/07/crowd-leaders/ crowd-leader-david-bratvold-defining-crowdsourcings-taxonomy-a-necessary-evil/. Brohan, P. ‘One Million, Six Hundred Thousand New Observations’. Old Weather Blog, 2012. http://blog.oldweather.org/2012/07/23/one-million-six-hundred.thousand-new-observations/. Bruno,E.‘Smithsonian Crowdsourcing since 1849!’. Smithsonian Institution Archives, April 14, 2011. http://siarchives.si.edu/blog/smithsonian-crowdsourcing-1849. Duff, W.M., J. Carter, J.M. Cherry, H. Macneil and L.C. Howarth. ‘From Coexistence to Convergence: Studying Partnerships and Collaboration among Libraries, Archives and Museums’. Info 18, no. 3 (2013). http://informationr.net/ir/18–3/ paper585.html. Dunn, S. and M. Hedges. ‘Crowd-Sourcing Study: Engaging the Crowd with Humanities Research’. AHRC Connected Communities Programme, 2012. http://crowds.cerch.kcl.ac.uk. Estelles-Arolas, E. and F. Gonzalez-Ladron-de-Guevara. ‘Towards an Integrated Crowdsourcing Definition’. Journal of Information Science 38, no. 2 (2012): 189–200. Frankle, E. ‘More Crowdsourced Scholarship: Citizen History’. Center for the Future of Museums, 2011. http://futureofmuseums.blogspot.co.uk/2011/07/ more-crowdsourced-scholarship-citizen.html. Gilliver, P. ‘“Your Dictionary Needs You”: ABrief History of the OED’s Appeals to the Public’. Oxford English Dictionary, October 4, 2012. http://public.oed. com/the-oed-appeals/history-of-the-appeals/. Holley, R. Many Hands Make Light Work: Public Collaborative OCR Text Correction in Australian Historic Newspapers. Canberra: National Library of Australia, March 2009. Holley, R. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3/4 (2010). http://www.dlib.org/dlib/march10/holley/03holley.html. Howe, J. ‘The Rise of Crowdsourcing’. Wired, June 2006. http://www.wired.com/ wired/archive/14.06/crowds_pr.html. Howe, J. ‘Crowdsourcing: A Definition’. June 2, 2006. http://crowdsourcing. typepad.com/cs/2006/06/crowdsourcing_a.html. Howe, J. Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business. 1st Edn. New York: Crown Business, 2008. Kittur, A., J.V. Nickerson, M. Bernstein, E. Gerber, A. Shaw, J. Zimmerman, M. Lease and J. Horton. ‘The Future of Crowd Work’. In Proceedings of the 2013 Conference on Computer Supported Cooperative Work, 2013, 1301–18. http:// dl.acm.org/citation.cfm?id=2441923. Kosmala, M. ‘Some Results from Season 4’. Snapshot Serengeti Blog, 2013. http://blog.snapshotserengeti.org/2013/01/30/some-results-from-season-4/. Millikan, F.R. ‘Joseph Henry: Father of Weather Service’. The Joseph Henry Papers Project, Smithsonian Institution Archives, 2012. http://siarchives. si.edu/history/jhp/joseph03.htm. Probst, D. ‘New FamilySearch Indexing App Now Available’. LDSTech, 2012. http://tech.lds.org/index.php/component/content/article/1-miscellaneous/455.new-familysearch-indexing-app-now-available. Raddick, M.J., G. Bracey, P.L. Gay, C.J. Lintott, P. Murray, K. Schawinski, A.S. Szalay and J. Vandenberg. ‘Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9, no. 1 (2010). http:// portico.org/stable?au=pgg3ztfdp8z. Ridge, M. ‘Frequently Asked Questions about Crowdsourcing in Cultural Heritage’. Open Objects, 2012. http://openobjects.blogspot.co.uk/2012/06/ frequently-asked-questions-about.html. Ridge, M. ‘From Tagging to Theorizing: Deepening Engagement with Cultural Heritage through Crowdsourcing’. Curator: The Museum Journal 56, no. 4 (2013): 435–50. Secord, A. ‘Corresponding Interests: Artisans and Gentlemen in Nineteenth-Century Natural History’. British Journal for the History of Science 27, no. 4 (1994): 383–408. Sweetapple, K. ‘How the Sydney Design Festival Poster Competition Went Horribly Wrong’. The Conversation, May 24, 2013. http://theconversation. com/how-the-sydney-design-festival-poster-competition-went-horribly.wrong-14199. Trouille, L. ‘Galaxy Zoo Quench – Experience the Full Scientific Process’. Galaxy Zoo, 2013. http://blog.galaxyzoo.org/2013/07/10/galaxy-zoo-quench.experience-the-full-scientific-process/. UNESCO Office in Cairo. ‘Tangible Cultural Heritage’, undated document. http:// www.unesco.org/new/en/cairo/culture/tangible-cultural-heritage/. This page has been left blank intentionally PART I Case Studies This page has been left blank intentionally Chapter 1 Crowdsourcing in Brooklyn Shelley Bernstein Over the past decade, the Brooklyn Museum’s leadership has developed a thoughtful and comprehensive strategy to rethink the Museum experience and strengthen its offerings in order to inspire visitors. Starting with its core mission, the Museum’s priority is to build on a long-established commitment to serve its communities: to act as a bridge between the rich artistic heritage of world cultures, as embodied in its collections, and the unique experience of each visitor. Dedicated to the primacy of the visitor experience; committed to excellence in every aspect of its collections and programs; and drawing on both new and traditional tools of communication, interpretation, and presentation; the Museum aims to serve its diverse public as a dynamic, innovative, and welcoming center for learning through the visual arts.1 In the digital efforts at the Brooklyn Museum, we strive to bring to life this community-driven mission and all that it can mean both in the visitor’s experience within the building and online. In this chapter, we will look closely at selected large-scale projects to show the differences between them, discuss how the institution’s goals have shifted over time, and demonstrate how each project – from digital comment books; Click! A Crowd-Curated Exhibition; Split Second: Indian Paintings, an experiment in responsive interpretation to GO: a community-curated open studio project – was designed for a very specific kind of participation. Early Projects When first starting our digital efforts in 2007, we began asking ourselves what community meant on the web and quickly found inspiration in the image hosting site Flickr which hosted a strong community of participants deeply engaged in photography. The community at Flickr had been fostered through a series of design choices that allowed for strong associations and recognition among participants. When looking at a photograph on the site, you could see the life behind it by getting to know the photographer through how they saw the world, and also through other photographers who were commenting or tagging images in ways that allowed us to .http://www.brooklynmuseum.org/about/mission.php. get to know them, too. The most successful thing about the design of the site was what its founder, Caterina Fake, said were her own goals in creating platforms for the web, ‘You should be able to feel the presence of other people on the Internet.’2 You could feel the presence of people pervasively throughout the Flickr platform; it had been designed to foster community from the very early days of the site’s creation. Our challenge was to take these same ideas and apply them to a museum setting. How could we could we highlight the visitor’s voice in a meaningful way and utilise technology and the web to foster this exchange? Two early efforts included major projects both in the gallery and online – the publication of our collection on the website and the creation of electronic comment books throughout the galleries. There is nothing more important to a museum than the objects which form its basis and the visitors coming through its doors; both projects were designed to help to form a backbone of trust between visitors and the Museum, allowing them a voice in our facility and holdings. When publishing our collection online3 we wanted object records to come to life, infused with the visitor activity around them. Each collection record would allow people to tag, comment and mark it as a ‘favourite’, but they would also go beyond simple information gathering by asking people to join our ‘Posse’, a community of participants on the Museum’s collection website who would help us augment object records while being attributed for their efforts. When a visitor to our website looks at an object in our collection, they can quickly see a whole universe of activity around it with people commenting and tagging objects; it is easy to see an individual’s activity and to gain an understanding of who they are and the contributions they have made because this activity is displayed right along with the Museum’s object in question. In addition to allowing this type of activity on each object’s page, online and in-gallery games were created which allow participants to tag or clean up records in a more competitive setting. Through this project, the institution has gained valuable information which has helped fix problems in our collection online4 and has made our collection more accessible through tagging contributions. During the same year the Museum began a project to replace existing paper visitor comment books with electronic versions which would run on small computer kiosks and, later, iPads. These comment books, available in every major gallery, ask our visitors to tell us about their experiences. The feedback we receive is displayed both in the gallery and online, while curatorial and visitor services staff are emailed weekly digests of the activity. Using this system, a visitor can leave us a comment about their experience and another potential visitor can read those thoughts online before deciding to come and see a show because both positive and 2.Leonard, ‘What You Want’. 3.http://www.brooklynmuseum.org/opencollection/collections/. 4.See, for example, the comment pointing out an upside-down image on http://www. brooklynmuseum.org/opencollection/objects/164389/%7CUntitled%7C_Horses_Eye/set/ right_tab/talk/. negative comments are displayed on an exhibition’s web presence. As staff receive weekly digests about the visitor experience, they can quickly discover what worked and did not within any given show. Staff have been able to adapt and change the visitor experience on the fly based on some of the feedback received, but with larger issues, staff consider how to adjust future shows to improve those experiences. The comment books present the institution with a full cycle of participation and learning, allowing our visitors to participate in feedback and honouring that participation by showing it to other visitors and our staff to gain greater understanding of what each person experiences when he or she comes to the Museum. These early initiatives in crowdsourcing have allowed our audience to participate while the institution gained considerable insight into its holdings and audience, but both examples fell short of a truly meaningful exchange. Visitor interactions with objects on our website were never connected to their in-person visitation and, though they could participate, there was no way to have a meaningful dialogue. Community members could establish profiles and, as a result, there was a life to their presence online, but it was fairly limited in scope. Staff could respond to comments left in the electronic comment books, but visitors were not notified if their questions had been answered or, more importantly, that their feedback had made a difference. As we continue to move forward with these two initiatives we are developing ways to bridge the online and in-person visitor gap. In the meantime, we have also looked to create more specific projects which would allow visitor interaction with the institution to become all the more meaningful and create a deeper sense of engagement. Crowdsourcing as Exhibition In 2008, the Museum embarked on Click! A Crowd-Curated Exhibition, a photography installation that invited the general public to participate in the exhibition design process. The project took its inspiration from the critically acclaimed book The Wisdom of Crowds, in which New Yorker business and financial columnist James Surowiecki asserted that a diverse crowd is often wiser at making decisions than expert individuals. Click! explored whether Surowiecki’s premise could be applied to the visual arts. In The Wisdom of Crowds, James Surowiecki asserts that maintaining diversity and independence are two key factors for a crowd to be wise. Both issues are discussed at length in Chapters 2 and 3 of the book: Diversity helps because it actually adds perspectives that would otherwise be absent and because it takes away, or at least weakens, some of the destructive characteristics of group decision making. Independence is important to intelligent decision making for two reasons. First, it keeps the mistakes that people make from becoming correlated. […] Second, independent individuals are more likely to have new information rather than the same old data everyone is already familiar with. The smartest groups, then, are made up of people with diverse perspectives who are able to stay independent of each other.5 While we had been designing for community in early projects, the subject matter of Click! required us to design for crowds. In addition to trying to discover if a diverse crowd was just as ‘wise’ at evaluating art as the trained experts, the Brooklyn-focused content of Click! was intended to foster a local audience and to put the community’s choices on the walls of the institution, which was normally seen as sacred space for curators. Click! began with an open call asking photographers to submit a work of photography electronically that would respond to the exhibition’s theme, ‘Changing Faces of Brooklyn’. While not specifically requiring photographers to be Brooklyn-based, the theme was defined to appeal to those who understood the borough and to, eventually, foster a local audience of visitors coming to view the exhibition. As Surowiecki had noted, diversity was a key factor in facilitating wise decision-making among crowds, so the theme was selected with an eye towards the variety of interpretations it could inspire. In total, 389 photographers submitted images, with subjects ranging from Brooklyn’s ongoing gentrification, depictions of social issues facing the borough and specific scenes in neighbourhoods which illustrated the changes taking place in familiar areas. In order to minimise influences from outside the project, the open call was a blind process, each participant could only submit one photograph for consideration, and during the call photographers could not see the photographs submitted by other photographers. Despite these design choices to foster crowd-like participation, communities quickly formed around the process. Many of the photographers were creating small groups who would go out and shoot every weekend, discuss the resulting work, and post their progress online throughout the four week submission period prior to selecting the single work they would eventually submit. After the conclusion of the open call, the general public was asked to evaluate photographs online using a specifically designed interface which would minimise influence, another factor important to Surowiecki’s theory. While many of the features seen on successful websites are designed to foster community, they also create a great deal of influence – the number of views, comments, favourites, most emailed and leader boards of the modern website are built to influence others. When thinking about the creation of a tool where submitted photographs would be evaluated, we wanted to minimise influence as much as possible and rethink the ‘social’ design features now commonplace. During the six week evaluation period, anyone on the web could evaluate the pool of 389 submitted photographs. As part of the evaluation, each participant self-selected his/her knowledge level (from ‘none’ to ‘expert’) and geographic Surowiecki, The Wisdom of Crowds, 29. location. Participants were asked to assess the photographs that were submitted, using a sliding scale to label photographs from ‘most’ to ‘least’ effective, taking into consideration aesthetics, the photographic techniques used and the relevance of the work to the exhibition’s theme. The online evaluation tool was designed to promote fairness. Works were presented at random, and an algorithm ensured all photographs were seen an equal number of times. To minimise influence, works were displayed without the artist attribution; evaluators were unable to skip past images or to forward links to individual works. Participants could leave comments during the rating process, but they were not visible to other participants until after the exhibition had opened. The constraints created to minimise influence and get our community thinking as a crowd of independents participants were frustrating to many participants and responses were varied, from blog comments like: Regarding the evaluation process I’ll describe my own experience. I didn’t evaluate any images. Not that I didn’t want to but I found myself to be unsuited to the process. Ideally I would have preferred to have been able to view icons or small sized versions of the images and select the ones that appealed to me for closer, full sized, evaluation. I don’t remember the exact constraints of the process at this point, but that overview which I would have preferred was not available. Reading the commentary I can now understand the wisdom of that, in that it would have allowed some people to flood the evaluations with good or bad judgments. I think that still could have been possible if someone or group was very much inclined to do so. But at least it would have required some patience and effort. So the aspect of random image choice makes sense.6 to: This is a brilliant idea. At first I had the urge to check and see how others were rating the works that I had rated, but after reading your post here I am impressed by the design. It does indeed seem to promote unbiased evaluation. I will be curious to see the results!7 and: It’s shocking to hear that the evaluation tool was so disliked. I found it simple, intuitive and very satisfying to use. When judging images, particularly such a large number, the subtle details matter. The slider interface gracefully captured that. The evaluation would have been frustrating if we had been limited to 6.http://www.brooklynmuseum.org/community/blogosphere/2008/06/04/gaming.click/comment-page-1/#comment-373. 7.http://www.brooklynmuseum.org/community/blogosphere/2008/03/31/ minimizing-influence/comment-page-1/#comment-208. Figure 1.1 The online evaluation tool used for Click! A Crowd-Curated Exhibition allowed participants to view images and rate them on a sliding scale. Screenshot: Brooklyn Museum Website something like a 5 point scale. The before/after thumbnails were the right size: large enough to provide a sense of context but small enough to prevent me from processing the image. I rated every image and there’s no way that would’ve happened without this interface. I’ve pointed several people to it as a great example of elegant design. If others were mostly frustrated about the methodology, I sympathize. Nonetheless, the rigor makes this more than just some marginal online poll or social networking experiment. Evaluating stacks of images can be exhausting. It’s worth it because you never know what great shot awaits.8 In the end, however, participants took on the task given to them and accepted the challenges presented by the evaluation interface, and 3,344 evaluators cast 410,089 evaluations. On average, an evaluator looked at 135 works and viewed an image for 22 seconds before casting an evaluation. Even though commenters could not see the words of others to help spark thoughts and ideas, 3,098 comments were given during the evaluation period. Interestingly, no matter how much we specifically engineered the project for a broad crowd, it was the local community who put the most time and effort into Click! Even though evaluation took place on the web and participants in more than 40 countries took part in rating photographs, 64.5 per cent of participants were local to the extended ‘tri-state area’ around New York. A deeper look at the statistics reveals that the bulk of the participation was coming from a local audience: 74.1 per cent of the evaluations were cast by those in the tri-state area with 45.7 per cent of evaluations being cast by those within Brooklyn. When the exhibition opened on 27 June 2008 the top 20 per cent of the 389 images curated by the crowd were installed in the physical gallery. Within the installation, images were displayed at various sizes according to their relative ranking, so upon entering the gallery a visitor could see almost instantly which images the crowd had most responded to. Data about each image were published on the website9 along with the comments that were left during the evaluation stage. The website also presented the results according to the self-rated knowledge (from ‘none’ to ‘expert’) provided by participants: there was a remarkable similarity in each group’s top choices.10 Whether the final choices were, indeed, ‘wise’ is a matter of opinion given the subjectivity of art in general, but the resulting data suggested there was a lot of agreement about which images resonated and surfaced to the top. More important than the data gathered was the community’s response to the exhibition. A total of 20,000 people came to see the show during the six week 8.http://www.brooklynmuseum.org/community/blogosphere/2008/05/27/thank-you/ comment-page-1/#comment-270. 9.http://www.brooklynmuseum.org/exhibitions/click/intro.php. 10.http://www.brooklynmuseum.org/exhibitions/click/comparison.php. installation and museum guards anecdotally reported it seemed as if there was always a flurry of activity in the small gallery on the second floor where Click! was installed. Surrounded by photographs in the small space, visitors would lounge on large platform-like seating installed in the middle of the gallery and use laptops to access the resulting data on the website. Photographers would proudly come into the space and pose with their work. Responsive Interpretation in a Split Second Our next large-scale visitor-driven project came in 2011 with the launch of Split Second: Indian Paintings.11 Split Second was an opportunity to facilitate collaboration between our curators and our online community using in-gallery technology and the web to learn more about the visitor experience. The online experiment and resulting installation explored how someone’s initial reaction to a work of art is affected by what they already know, are asked or are told about the object in question. Unlike Click!, the work in Split Second came directly from our permanent collection and the project was intended as a way to use interactive technology to foster a dialogue about works in our collection and how we install them. The main source of inspiration for this project was Malcolm Gladwell’s Blink: The Power of Thinking without Thinking, a book which explores the power and pitfalls of initial reactions and split-second thinking. The Split Second project would explore the ideas around quick judgement and test how a person’s split-second reaction to a work of art would affect their museum-going experience. As visitors walk through our galleries, what kind of work are they drawn to? And if they stop, look, read or respond, how does their opinion of that work change? Split Second began with an eight week online evaluation; audiences participated in a three-part online activity which featured the Indian paintings of the Museum’s permanent collection. The first stage explored split-second reactions: in a timed trial, participants were shown two random objects side by side and asked to select which painting of the pair they preferred. Next, participants were asked to write in their own words about a painting before rating its appeal on a sliding scale. In the third phase, participants were asked to rate a work of art after being given unlimited time to view it alongside a typical interpretive text written by museum staff. Each part of the exercise aimed to examine how a different type of information – or a lack thereof – might affect a person’s reaction to a work of art. During the online evaluation, participants reported being ‘stressed out’ by having to select between two works of art very quickly, but analysis showed that the majority of participants completed all three phases of the online activity. In total 4,617 participants created 176,394 ratings and spent 7 minutes and 32 seconds on average in their session. Demographics of those participating in the activity were 11.http://www.brooklynmuseum.org/opencollection/labs/splitsecond/. analysed by age, gender, experience level, location and completion rates.12 Data from the online activity were analysed and published on the project’s website. Headline results from the data analysis were highlighted in the gallery installation along with objects that illustrated each data point. Findings included: • High/Low: certain paintings consistently inspired very positive and very negative responses, regardless of other factors. • Time: people respond differently with limited time. When participants had unlimited time to ponder the works, there was widespread change in their responses. Some paintings saw dramatic gains in their relative ranking, while others saw losses. • Context: adding context leads to more positive responses. Participants were asked to rate paintings with different amounts of contextual information. Some people rated paintings on screens that contained no information, while others were shown tags, a caption or a full interpretive label. On average, adding any sort of contextual information raised scores by about 10 points, in some instances as much as 16 points. • Complexity: when shown with no contextual information, complex images13 do better. Complex paintings – those determined to be rich in information by a computer program – reliably got higher scores, but only when shown without any accompanying text. • Engagement: participants who answered a question about a painting tended to give the painting similar ratings. When people were given unlimited time to rate an image, their ratings for any given painting tended to spread across the scale. However, when a group was instructed to answer a question about a work of art, their ratings tended to be closer to those of others who did the same task. The question that resulted in the most ratings agreement was: ‘About how many figures are depicted in this painting?’ Similar to Click!, this project saw a large amount of local participation even though the online evaluation was widely available to a web audience. Participants came from 59 countries with most traffic coming from the United States; there was an overwhelming majority coming from New York City and this local audience was the most dedicated. The average participant spent 7 minutes and 33 seconds in the online activity, but participants in the New York City area spent more than double that time with an average 15 minutes per session. Within the gallery exhibition, the same online activity was available at kiosks within the installation. Data gathered in this setting were compared to those gathered online and every major finding of the experiment was replicated, showing that the way 12.http://www.brooklynmuseum.org/opencollection/labs/splitsecond/stories. php?slug=split-second-thank-you. 13.Such as http://www.brooklynmuseum.org/opencollection/objects/3679/King_ Solomon_and_His_Court. our online audience viewed information did not differ so much from the physical visitor coming through the institution’s doors. This may not be surprising given the local makeup of the initial online participants. With Split Second, we were beginning to see a trend in local participation – those closer to the Museum were the most invested participants – and this aspect would heavily influence the creation of our next large-scale project. Moving from Crowd to Community If Click! was engineered for the crowd and Split Second looked inward to the Museum’s permanent collection, a more recent project was specifically designed to foster community and to think about the neighbours outside the walls of the institution. Recognising Brooklyn is home to more artists than anywhere else in the United States, GO: a community-curated open studio project14 was a borough-wide initiative designed to foster personal exchange between Brooklyn-based artists, their communities and the Museum. GO was conceptualised during 2011 to take place as a year long initiative that would run during 2012. Jointly organised by the Museum’s Managing Curator of Exhibitions, Sharon Matt Atkins, and then-Chief of Technology, Shelley Bernstein, GO asked Brooklyn-based artists to open their studios to the community over the course of one weekend. During the weekend, community members who had registered as voters could visit studios throughout Brooklyn and nominate artists for inclusion in a group exhibition which would open at the Museum on Target First Saturday, 1 December 2012. As part of the process, Brooklyn Museum curators would visit the studios of the top 10 artists nominated by the public and dynamically select artworks and build the exhibition from the public’s choices. In conceptualising GO, the institution was thinking about a long-standing commitment to foster and exhibit the work of Brooklyn-based artists. Previous exhibitions included Open House: Working in Brooklyn,15 a 2004 survey show which featured more than 300 works in all media by 200 Brooklyn artists. More recently, the Museum had started to show large-scale solo exhibitions by more established names (such as Fred Tomaselli, Mickalene Thomas), while also producing series of smaller installations by pre-emerging artists and accessioning key works by Brooklyn-based artists into the permanent collection. As a point of departure, the Raw/Cooked16 series, which had its inaugural season in 2011 and continued throughout 2013, exhibited Brooklyn artists, but also re-thought the model of how to exhibit local artists. In Raw/Cooked, an advisory panel of established Brooklyn-based artists (Ron Gorchov, Amy Sillman, Mickalene Thomas, Michael Joo, Paul Ramirez Jonas) each selected three ‘under 14.http://gobrooklynart.org/. 15.http://www.brooklynmuseum.org/exhibitions/open_house/. 16.http://www.rawcooked.com. the radar’ artists working in the borough who they felt were doing interesting work; the criteria for selection were that the proposed artists had never had a museum exhibition and did not yet have gallery representation. Eugenie Tsai, the Museum’s John and Barbara Vogelstein Curator of Contemporary Art, visited the studios of those suggested to determine which would be represented in the Raw/Cooked series. In this way, Raw/Cooked served as a way to discover and highlight new work while rethinking how the institution would find the artists. In the resulting installations, each artist selected by Tsai was given the opportunity to work with the Museum’s collection and to show in spaces of their choosing, however unconventional, in their first major museum exhibition. While the Museum was thinking about Brooklyn artists at all levels of their careers and exploring new models like the one Raw/Cooked presented, it was also considering the successful technology-driven community initiatives such as Click! and pondering next steps in that direction. GO was created to bring together these aspects by highlighting Brooklyn artists and creating a new model to discover and show their work. GO was inspired by two established programmes: ArtPrize,17 an annual publicly juried art competition in Grand Rapids, Michigan, and the long tradition of open studio weekends held each year in the Brooklyn neighbourhoods of Williamsburg, Greenpoint, DUMBO, Gowanus, Red Hook and Bushwick. During ArtPrize, any artist in the world can go to Grand Rapids and show a single work in a local venue. Work is displayed in myriad locations radiating three square miles from the centre of the city; venues include everything from bars, restaurants and cafes to the more obscure locations of hairdressing salons, animal shelters, offices of the Salvation Army and religious institutions. Voters must register in person using their state-issued identification and are then asked to view art installed throughout the city over a two week period and vote up or down the work they responded to. The top 10 works voted by the public receive a monetary prize. One of the most compelling things about the ArtPrize experience is the way the project takes over Grand Rapids for the two week duration with people engaging with art, the city and artists who are sometimes available to greet the public. ArtPrize allows for a communal experience among voters; they see each other on the street carrying maps, wearing ArtPrize merchandise and ducking in and out of venues to discover art throughout the city. As another point of departure, the GO organisers had been visiting the open studio weekends that take place throughout the year in various neighbourhoods of Brooklyn. As visitors were invited into an artist’s workspace to view their work, a high level of engagement was seen throughout the weekends. In meeting the artist where he/she worked, visitors were able to discover an entire body of an artist’s work by seeing projects completed and in progress. Visitors had direct access to the artist so they could ask questions and learn more about the process of a work’s creation. Visitors to studios were also discovering more about their neighbourhoods 17.http://www.artprize.org/. and goings-on in buildings they may have walked past every day, but never had the opportunity to go inside. In a typical open studio weekend, visitors were seeing a large body of work, having personal encounters with artists and discovering what was taking place in familiar locations throughout their neighbourhoods. In thinking about these two models, the organisers realised combining aspects of both would bring GO in line with the institution’s goals. GO would be designed as a hyper-local initiative to foster exchanges between artists working in Brooklyn and their neighbours. The project would invite the public into spaces where art was being created, give them direct access to those making it and encourage them to discover a new aspect of their neighbourhood. Rather than viewing one work in isolation, a voter would be ask to review a large body of work prior to making a decision about the art, and the project would focus on the artist, not a single work of art. Part of this process would include designing a system that would encourage voters to think more like curators and to get people away from quick social media-style ‘like’ button thinking to engage more deeply with the experience presented to them. The entire process would be designed to foster dialogue, so the organisers aimed to be fully transparent about decisions made and data collected so everyone participating in GO could see the thoughts behind the project, and to ask participants for their feedback at every step of the process. During a four week online registration process, 1,708 Brooklyn artists registered to open their studios for GO, including those in neighbourhoods where many were not aware artists had been working. The project served as a way to highlight these artists in neighbourhoods that were not part of existing established open studio programmes. To keep the project in line with the curatorial goals, it was important to create a system that would ensure voters were seeing the work in-person. Each artist was assigned a unique number printed on a poster designed to be hung inside their studio during the course of the open studio weekend; voters would visit each studio and use this unique number to ‘check-in’ at a location to log their physical visit. Just as a curator is expected to see many things and then make a choice from that broader selection, voters were required to ‘check-in’ to at least five studios in order to be eligible to nominate up to three for inclusion into the group exhibition. Another key design decision was to create a system which would allow for careful decision making, so the nomination process was unavailable during the open studio weekend itself to ensure a cooling off period. Eligible voters who had checked in to at least five studios were sent an email after the open studio weekend asking them to cast their nominations online and giving them a week long period to do so; voters could only nominate from the list of artists they had seen and studios they had checked in to during the open studio weekend. Another challenge included the sheer size of the physical location in which the project would be running. If it were an independent municipality, Brooklyn would be the fourth largest city in the United States with a population of 2,504,700.18 18.http://www.brooklyn.cuny.edu/pub/departments/csb/. Unlike the three-square miles of ArtPrize, GO would be taking place throughout an enormous location spanning 73 square miles within 67 individual neighbourhoods, each of which have their own distinct communities and local character. For the people who live in the borough, a ‘Brooklyn’ identity feels like something placed upon us by others, as local residents often most fondly identify with a particular neighbourhood. The key to GO would be taking these neighbourhood identities and finding a structure which would allow the project to run in a distributed fashion. During GO, 22 neighbourhood coordinators were hired to facilitate a hyper-local approach based on neighbourhood identity; coordinators lived in the neighbourhood they were working in and were the local experts who would connect artists, voters and volunteers with the project throughout its duration. Every step of the project would circle back to neighbourhoods at the most local level; coordinators would host meetups at local venues so artists and voters could get information about the project. Coordinators could consider the special needs of specific areas and adjust GO signage so that the visual design and language were responsive to local audiences. The neighbourhood coordinators were the project’s liaisons and the face of GO within the 44 neighbourhoods that would eventually participate in the project. The open studio weekend was held on 8–9 September 2012. During the course of the event approximately 18,000 people made roughly 147,000 studio visits, and 30 per cent of those visitors used the GO website, the GO iPhone app or SMS text messaging to register as voters and check-in to studios. Of the 4,929 voters who would eventually be eligible to nominate artists because they had visited at least five studios, 78 per cent of this group returned during the nomination period to cast 9,745 nominations which would determine the top 10 artists; this high return rate indicated a level of commitment on behalf of those who had participated in the project. In fact, while five visits were the threshold for eligibility to nominate artists, the average voter during GO visited eight studios and some of the most committed voters checked in to 70–150 studios over the course of their participation. When asked to share their stories, 361 participants wrote at length about their experiences, giving the project coordinators valuable feedback from the perspective of voters, volunteers and artists.19 On the whole, participants in GO showed a deep understanding of the project and its challenges, were willing to put in a good amount of time to register and visit studios, followed the entire process from start to finish and gave incredibly detailed feedback about their experiences. As designed, the open studio weekend worked well on a number of levels, but it also saw many challenges. Participants reported a greater understanding of their communities through the open studio weekend. Artists anecdotally reported approximately 70 per cent of the people coming into the studio were strangers or people they knew from their neighbourhood in passing. Voters recounted stories of discovering artists and their work, but, also, of gaining a better understanding of the artists working in their neighbourhoods. As one said: 19 http://gobrooklynart.org/about/shared_stories. I had no idea there were so many artists in my neighborhood, several living within a one block radius of my apartment in Crown Heights. Not only was this a great way to get to see local art from local artists, this also was a great way to meet my neighbors. On top of that, the abandoned warehouse several blocks north of me turns out to not be abandoned at all and is instead filled with artists.20 However, while the project facilitated discoveries, response to the competitive framework was complicated. Many artists took issue with the idea of competing for a ‘prize’ and expressed consternation at the idea of voting. The notion of fairness also came into play; many artists felt being in a low traffic area or out of the way location would impact their chances of being nominated for the exhibition: Admittedly, the registering and voting process seems to me to be quite unfair and weighted in favor of artists who work in high traffic neighborhoods and buildings, as well as in favor of artists who are internet savvy and have friends who would be willing to register and vote without ever stepping foot into an artist’s studio. It was important for me to let go of this aspect of the event in order to enjoy the positive aspects.21 The barriers put into place to keep GO from becoming a ‘like button’ popularity contest were also a point of contention for many artists and voters. Throughout the open studio weekend, the technical side of the project was designed for maximum accessibility, but many participants felt that the process to register and vote was too difficult and too reliant on access to technology. Voters could use an iPhone app, a mobile website or SMS text messaging to register and check-in to studios, but voters could also simply write down artists’ codes and enter them from home through the website. Interestingly, the simplest method of writing down codes and entering them on the website proved to be the most useful for participants with 43.7 per cent of voters doing so compared to 41.1 per cent who used the iPhone app, 9.2 per cent who utilised text messaging and only 6 per cent using the mobile website.22 Where GO succeeded the most was in the personal exchanges which took place over the weekend. Voters and artists reported the conversations in the studio were incredibly beneficial. I think it was very successful in getting the public INVOLVED … the viewers felt invested not passive. Almost all the visitors who had registered with the Museum were especially focused, asked interesting questions, made interesting comments and were polite about being in your private studio, and not just bumping along as sometimes happens too often on other big studio tours. So from an artist’s 20.http://gobrooklynart.org/about/shared_stories/1934. 21.http://gobrooklynart.org/about/shared_stories/2187. 22.http://gobrooklynart.org/about/statistics/33232643496. perspective it was also … despite the wiff of popularity contest thing … a more satisfying weekend with people really engaged in looking at the art.23 One measure of success was the low use of social media during the weekend. Participants were seen using social media to talk about their day generally, but were not wrapped up in the blow-by-blow reporting now commonplace at many events. Many indicated they were simply too busy interacting in the studios to engage on social media and cited the personal face-to-face conversations that made the weekend special. Based on the 10 community-nominated artists, museum curators began their studio visits to decide which artists and which works would be included in the exhibition. The two curators working on the project, Sharon Matt Atkins and Eugenie Tsai, visited studios independently and then discussed their reactions to the work. Curators discussed their methodology on the GO blog and selected five artists to be featured in the exhibition.24 The curatorial component of the exhibition was designed as a collaboration between the staff at the Museum and the general public. In past exhibitions like Click! the resulting exhibition was seen as the ‘community’ show where curators took a backseat to the community’s choices. This resulted in an exhibition beloved by the public, but seen as an anomaly within the institution: an exhibition hosted here, but not truly owned by the institution. By designing a process in which the curatorial staff at the Museum were in charge of reviewing the community’s choices and then building a show based on the community’s choices, it ensured the result was something the institution was engaged with at a core level. The project entailed key logistic concerns to consider given a finite space to house the exhibition within the building and an unpredictable set of results in the top 10 artists nominated – if every nominated artist had been creating large-scale works, fewer would fit into the physical space. In their choices, the curators were looking to show the diversity of the work taking place throughout the borough. The exhibition featured work by Adrian Coleman, Oliver Jeffers, Naomi Safran-Hon, Gabrielle Watson and Yeon Ji Yoo. Interestingly, four of the five artists were not born in the United States, a coincidence which illustrates Brooklyn’s multi-cultural population. In the final exhibition, work was displayed by artists at all levels of their careers including Gabrielle Watson, a self-taught painter who works as a lawyer by day, and Naomi Safran-Hon, a Yale educated painter who had attended the Skowhegan summer residency programme for emerging visual artists and is represented by a gallery. In deciding how many artists to include, curators decided they wanted to represent 23.http://gobrooklynart.org/about/shared_stories/2224. 24.http://gobrooklynart.tumblr.com/post/35783584955/our-go-featured-artists, http://gobrooklynart.tumblr.com/post/36672047058/creating-a-framework-to-collaborate.with-the-public, http://gobrooklynart.tumblr.com/post/36741548465/making-choices-to.create-an-exhibition. fully the type of work GO participants had seen in the studios of the nominated artists. This meant showing multiple examples of work from a select number of artists, not one work from each of the top 10 artists. The resulting exhibition was problematic for many of the participants; the reduction from 1,708 artists who opened their studios to 10 featured through public nominations and finally only five featured in the exhibition itself was inherently difficult. Over the course of the project, what started as an inclusive process (any artist with a studio in Brooklyn could register to participate in GO at no cost) ended with exclusion as the number of artists was narrowed to reach the exhibition stage. Many artists felt the resulting show did not represent the work happening in the borough and were frustrated with the end result. Others felt like the process of the open studio weekend was not well represented in the final product coming back to the Museum. Two comments illustrate this frustration: It is unfortunate that a pool (can one even call ten out of 1700, a pool? More like a sad dribble) a pool of only ten artist was made available, only to be then further decimated by questionable curatorial selections. After the overwhelming registration of 1700 or more artists and the data so painstakingly collected, it would have been so easy to open up this pool to 100 people or more and show a selection of 20 or more. It is a very awkward situation that has been created here. It appears that GO is now saying, that from the incredible creative out put in Brooklyn, a major creative metropolis, only five artist are worth to be shown?25 GO turned the whole city into a contemporary extension of the Brooklyn Museum. I’d been meaning to come to the Brooklyn Museum soon anyway, so now I have another trip planned in order to see the picked artists. Yet I can’t imagine that the ‘winners’ will be at all representative of the best that was out there – I feel there were just too many studios (not that I would want there to be less studios, just more time to visit a greater sample!) and not enough voters happening to come across the work of a couple [of] really great artists who I encountered just in the small batch I visited. I would eagerly participate in more events like this – and I hope the Brooklyn Museum will plan them so that we all get the chance!26 In looking at the show, voters expressed concern about how these particular artists came to be featured. It would have been impossible for every voter to see all 1,708 artists over the period of a weekend, so many of the people coming into the exhibition wanted to better understand how these artists rose to the top: 25.http://gobrooklynart.tumblr.com/post/35783584955/our-go-featured.artists#comment-712292782. 26.http://gobrooklynart.org/about/shared_stories/2164. Ultimately, I think the exhibition is great. The five artists represented here are showing some great work. I wish that there was a way, somehow, to show more of the work that was seen by the GO voters during the open studio weekend. I bet there were some phenomenal artists who may have been left out of the selection process due to low vote count.27 Some participants assumed the featured artists had strong social networks that must have propelled them to the top 10, but the resulting data tella different story, showing how voters moved through the project and the effect it had on the results. During the open studio weekend, voters had a tendency to stay local and many neighbourhoods saw a high rate of intra-neighbourhood traffic;28 data showed voters checking in to studios in the same zip code with which they had registered. This local traffic had a great effect on the end result because it became evident that neighbourhoods which had a strong community fabric helped propel their artists into the top of the standings. A small neighbourhood like Ditmas Park saw a high visitor to studio ratio; there were fewer artists opening their doors in this area compared to neighbourhoods like Red Hook, Gowanus and Bushwick, but there was a high rate of local traffic. In Ditmas, a strong community of neighbours turned out to discover the artists working in their local area and these factors ensured the artists working here saw a high rate of very concentrated traffic.29 In addition, statistics showed a sweet spot for artists who worked in buildings with 8–15 open studios. It appeared that for artists in big studio building with more than 15 studios, voters had too much choice; the artist in this example would see a lot of foot traffic (for example, some large studio buildings with hundreds of artists reported approximately 1,200 visitors during the two day weekend), but the voters’ nominations would become too dispersed for individual artists to rank highly. Many artists would receive nominations in these situations, but no one artist in the building would get enough to become a frontrunner. Conversely, artists in smaller buildings or off the beaten path found there was not enough foot traffic to gain a footing in the rankings. However, if you were working in a building of 8–15 studios there was just enough foot traffic combined with just the right amount of choice to boost artists higher in the standings. Of the top 10 artists, all 10 had either shown in buildings of 8–15 studios or were in small neighbourhoods, like Ditmas Park, with a small enough core of artists showing combined with an above average rate of intra-neighbourhood traffic that would change the results.30 Even with this sweet spot in effect, the visitation patterns within those buildings or neighbourhoods demonstrated this effect was not the only thing at play. In most cases, there was one artist in those situations who rose to the top and, often, the artist in question was not the easiest studio to get to within the building/area; all 27.http://www.gobrooklynart.org/about/exhibition_conversation. 28.http://www.gobrooklynart.org/about/neighborhood_data. 29.http://gobrooklynart.org/about/statistics/32264063137. 30.http://gobrooklynart.org/about/statistics/32399127427. artists in a ‘sweet spot’area benefited from the effect, but it did not matter if the studio was located on the first floor or the third; in every case, there would be one artist whose work resonated with the public. As just one example, Naomi Safran.Hon’s studio was on the third floor when there were many other studios open in the building which would have been seen prior to hers. When the exhibition opened at the Museum the visitor response varied. Critical reception from the art world, which focused on the end product of the show rather than the process, was overwhelmingly negative.31 Visitors to the show who participated as voters either found they discovered new work from the exhibition or expressed the view that the show did not represent the process of GO well enough and wished more artists could be included. Many of the visitors who were new to GO and finding the exhibition for the first time, found the concept and works installed compelling and expressed the desire that the institution do more, similar installations.32 As Brooklyn Museum staff move forward and think about future iterations of GO, we are specifically looking at the exhibition as the element of the project most in need of change. In a situation where the process is as much about the project as the end product, we wonder if an exhibition is a necessary component. As we consider how to iterate future designs based on the results of these projects, we reflect on the reports from voters during the open studio weekend that the decision-making process around nominating artists helped them engage with the project on a deeper level. Across the board, artists reported the visitors coming into their studios had focus, asked good questions and generally engaged in the studio in a much deeper way than other open studio events. GO put the Museum at the centre of an ongoing artistic movement in Brooklyn and opened up a dialogue which took place in the neighbourhoods of the borough and at the Museum. As a platform, it allowed for all types of conversation about the art being created in the borough all the way through to the process of running the open studio weekend, creating and viewing the exhibition. While it is clear one project cannot be everything to all participants, the level of engagement was incredibly deep with a committed audience invested in the outcome and taking part at every stage; this represents a successful shift in conceptualising a project which moved from crowd to community. The last decade at the Brooklyn Museum has seen a fairly consistent programme to foster visitors’ ownership using mission-driven technology projects at a community-minded institution. This idea of community, not crowd, is a direction the Museum will continue to move towards in all aspects of its technical programme and visitor experience initiatives. Each of the projects described here has a common thread that links each initiative together, but as we move forward the Museum is looking to position participatory initiatives as more of a backbone 31.http://www.nytimes.com/2013/01/03/arts/design/go-a-group-show-at-the-brooklyn.museum.html; http://artfcity.com/2013/01/24/review-go-at-the-brooklyn-museum/. 32.http://gobrooklynart.org/about/exhibition_conversation. so the language of visitor input can run throughout the institution and is placed front and centre in everything we do. References Gladwell, M. Blink: The Power of Thinking without Thinking. New York: Little, Brown, 2005. Leonard, D. ‘What You Want: Flickr Creator Spins Addictive New Web Service’. Wired Magazine, July 28, 2010. http://www.wired.com/magazine/2010/07/ ff_caterina_fake/all/. Surowiecki, J. The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. New York: Doubleday, 2004. This page has been left blank intentionally Chapter 2 Old Weather: Approaching Collections from a Different Angle Lucinda Blaser Cultural institutions like the United Kingdom’s National Maritime Museum hold a wealth of collections of objects, personal or official documents, books, maps, manuscripts and more. These collections have the potential to provide vast quantities of data for many research topics but are often inaccessible as their various items have not been indexed, described or transcribed. Cultural institutions also have often passionate and dedicated groups of supporters. With the current trend of museums and galleries wanting to increase the level of ‘conversation’ with its visitors, crowdsourcing is being seen as a way to unite and enhance these two resources. This chapter discusses crowdsourcing and citizen science in cultural institutions, particularly Old Weather, which began as a project to digitise the climate data in historic ships’ logs. ‘Crowdsourcing’ and ‘citizen science’ have been around as concepts for many years, and early forms of mass participation such as the University of Sussex’s Mass Observation Project,1 founded in 1937, have helped by providing useful demonstrations of the value of asking the public to help. Over time the definitions of crowdsourcing and citizen science have become more rigidly set, particularly as groups and institutions develop to manage and promote such projects. The practice of ‘citizen science’ involves members of the public (‘citizen scientists’) working with professional scientists to complete a research project,2 and forms a distinct group within the broader category of crowdsourcing projects. They are not however limited to being achieved through scientific methods, that is, through experimentation: as long as data are being collected and analysed with a scientific goal, a project can be identified as a citizen science project. As the names suggest both crowdsourcing and citizen science projects often require a large number of individuals to help the project-organising group(s) with tasks that often cannot be accomplished by computers or that need human intervention to complete them successfully. These projects use the personal contributions of individuals, particularly for tasks that require skills that are unique to people, which can be as diverse as understanding handwritten accounts 1.University of Sussex, Mass Observation. Available at: http://www.massobs.org. uk/a_brief_history.htm. 2.Raddick et al., ‘Galaxy Zoo’. accurately, extracting and submitting date, time and location specific data, or work that requires that bit more of an artistic flair and understanding. Cultural institutions, in particular museums, have engaged with crowdsourcing and citizen science projects. These concepts can help collections to be used in interesting and creative ways that will promote them to new audiences, ensuring that they remain a source of inspiration and research for years to come. These projects also allow users to discover new ideas and follow their interests down routes that they may otherwise have never discovered. In the Old Weather project (www.oldweather.org) many users came for the climate science but stayed for the history. Many users have commented in the Old Weather forum (forum.oldweather. org) that as a result of the project they were buying books and enrolling in courses to enhance their knowledge further. For a cultural institution to be able to say that through a project it has opened up a topic to people who are then continuing their research is a fantastic outcome. Collections are a focus for many cultural institutions, especially museums and libraries. People such as museum students and visitors sometimes question the purpose of museums and collections in the modern age. Therefore, cultural institutions may unsurprisingly be interested in how crowdsourcing and citizen science projects may aid their collection. The participation of people in a crowdsourcing or citizen science project seems inevitably to engage them with the collections and data they are handling and impart a sense of ownership and connection. In particular, reward systems (such as increasing your rank or score for increased level of participation) not only enhance public interaction with the project but also offer an additional incentive for large audience participation.3 The benefits of crowdsourcing and citizen science projects to the hosting group or institution can be manifold: relieving demand on resources for data and collection management, providing close interaction with the target audience with an active feedback-system; engaging and capturing the interest of larger target audiences; and customising the content of collections and exhibitions according to data gained from these projects. Cultural institutions have used both crowdsourcing and citizen science projects in different ways. Through reviewing a sample of institutions world-wide, there seem to be clear themes within the aims of these projects, particularly crowd-curation and data enhancement projects. Crowd-Curation Crowd-curation projects ask the audience to submit and/or select elements to be displayed in an exhibition, or to help prioritise work such as conservation. For museums this type of crowdsourcing project, though light-touch in terms of its goals and results, is becoming more common as it encourages interaction with the collection or themes of the institution, helps the audience to become more .Romeo and Blaser, ‘Bringing Citizen Scientists and Historians Together’. engaged and encourages conversations to take place. The institution may also be able actively to grow its collection if it asks for submissions of photography, art, oral histories and so on. The Brooklyn Museum undertook a crowd-curated exhibition called Click!4 that asked artists to submit images for display in an exhibition (see also Shelley Bernstein’s Chapter 1 in this volume). The general public were then asked to evaluate and short-list the submissions from which experts would then analyse and select the images for display. Following this first project, more power has been given to the general public in the public cultural space, with the public now being able to submit works themselves and have them displayed in a gallery alongside those of well-known artists. Royal Museums Greenwich has undertaken several projects, such as Beside the Seaside and Astronomy Photographer of the Year, where crowdsourced images and collection items share the same gallery space.5 The development and the increasing popularity of photo sharing sites such as Flickr6 have enabled these dialogues to take place. Royal Museums Greenwich continues to undertake such projects as this because it enables a wider audience to participate in and experience an exhibition even if they are not geographically able to attend. As a national museum it is important to be able to show images and achieve interaction beyond those who may be able to visit physically. These projects also allow the Museum to achieve its goal of being conversational with its audience and encouraging them to be intrigued by meeting them on a level that they are able to converse and interact on. However, these sites also present a challenge for the Museum when accessioning submitted images into its collection. Though many users may be willing and are often flattered to be asked to donate their material to a collection, others also view their images with a commercial eye or fear the restrictions of use when signing over ownership rights. As a result, negotiations to acquire these works can be time consuming. As museums develop more and more audience-driven digital spaces where audience created content is pulled from online websites to a physical space, crowdsourced displays will become more common. These displays allow the audience to feel that they are working together with the museum and their input is valued. The next step for museums in this format of crowd project may be the development of crowd-curated descriptions where the public develop the interpretative texts about the artwork or object. The question for cultural institutions then becomes: how does this meet their as well as the audience’s needs? Is an 4.Brooklyn Museum, 2008. Click! A Crowd-Curated Exhibition. http://www. brooklynmuseum.org/exhibitions/click/ 5 National Maritime Museum, Astronomy Photographer of the Year, http://www.rmg.co. uk/whats-on/exhibitions/astronomy-photographer-of-the-year/; Beside the Seaside, http://www. flickr.com/groups/besidetheseaside/ and http://www.flickr.com/groups/besidetheseaside/; Ansel Adams at the National Maritime Museum, http://www.flickr.com/groups/anseladamsatnmm/ 6 Flickr, http://www.flickr.com/. official space for the official expert voice only, or can interpretation include the audiences and even grow as audiences in the spaces change? The increasing popularity of Wikipedia edit-a-thons,7 which have been embraced by institutions such as the British Museum and the British Library, are making crowdsourced descriptions an increasingly realistic approach for museums, if not in the physical then at least in the digital world. As a result of these edit-a-thons over 2,000 Wikipedia records were updated in over 50 languages in 2010–11; targeted museum object records were also improved as a result.8 The data generated by these projects may also have been reviewed and fed back into institutions’ own systems. As the perceived value and quality of this work increases will museums be incorporating Wikipedia articles within their own online collections soon? Data Enhancement Projects Data enhancement has been the main focus for institutions undertaking crowdsourcing projects. As discussed in other chapters of this volume, many have asked the public to aid in the transcription,9 tagging10 and the correction of data.11 As museums often have vast collections, crowdsourcing the work on their data is seen as a solution for providing increased access to items that may otherwise remain undocumented by the institution and unseen by the public. Through opening up collections in this way, museums are reaching out and taking advantage of the passion of their audiences. These audiences will in turn promote the museum, and this could result in more fruitful and creative relationships. Incorporating crowdsourcing tasks, such as tagging, into their online collections catalogues shows that the value and capabilities of crowdsourcing the collection are becoming increasingly recognised by institutions.12 These tools not only enable the museum to identify previously less accessible or ‘hidden’ items that users were able to identify within the collections but also gain insights into 7.Wikipedia, Wikipedia:GLAM/Projects, http://en.wikipedia.org/wiki/Wikipedia:GLAM/ Projects. 8.GLAM, Museums Collaborating with Wikipedia: Sharing Curatorial Knowledge with the World, http://upload.wikimedia.org/wikipedia/commons/8/88/GLAM_One-Pager. pdf. 9 Transcribe Bentham, http://www.transcribe-bentham.da.ulcc.ac.uk/td/Transcribe_ Bentham and New York Public Library, What’s on the menu?, http://menus.nypl.org/. 10 Your Paintings Tagger, http://tagger.thepcf.org.uk/. 11.National Library of Australia’s Trove, http://trove.nla.gov.au/general/participating.in-digitised-newspapers-faq. 12.For example, National Maritime Museum, Collections, http://collections. rmg.co.uk/; Horniman Museum, Collections, http://www.horniman.ac.uk/collections and Powerhouse Museum, Powerhouse Museum Collection Search, http://www. powerhousemuseum.com/collection/database/menu.php. how its audience views its collections. This is an invaluable way of understanding how the collection is used and viewed, potentially giving the institution a new way of looking at itself and pulling out themes that it may have otherwise overlooked. As an example of a more visual approach, one of the Victoria and Albert (V&A) Museum’s crowdsourcing projects asked the public to help improve thumbnail images of their collection by cropping them to best show the object and to exclude photographer’s colour or scale cards.13 While not all crowdsourcing projects are as strongly focused on objects, artworks or photographs, it generally seems that an image as well as associated metadata is important for people when investigating and discovering collections. The correction and enhancement of data is an area of crowdsourcing that is also growing amongst institutions. Museums have started to become aware of the limitations of their knowledge: no employee can know everything there is to know about the institution’s collection or its subject history. Therefore, by opening up collections and data to the wider public, people are now able to share their knowledge with these institutions. In 2010, the Library of Congress released the Liljenquist family’s donated collection of photographs of American civil war soldiers on Flickr14 and asked for the public’s help in identifying the soldiers. As a result of the project many of these soldiers have now been identified. Since then museums have started to incorporate this ‘Do you know more?’ idea into their own online collections pages, making data enhancement a central part of their online conversation with their audience. For example, the National Maritime Museum’s Collections site pages have a ‘Help us’ section calling on visitors to ‘Share your knowledge’.15 Transcription projects are a particular favourite of cultural institutions undertaking crowdsourcing projects. This may be because manuscript material holds a wealth of information that often cannot be searched until the text is indexed, described or transcribed. Handwritten records in particular are currently difficult or impossible to digitise and process using computers, so human input may be necessary to transcribe or verify their content. These documents are becoming increasingly visible online through transcription with the help of the public using crowdsourcing projects. As seen by the Australian Newspapers Digitisation Program,16 transcription projects can become highly addictive with some contributors spending up to 45 hours per week on the project.17 If institutions have the ability to enable transcription of some documents by the public, perhaps they should take the step towards allowing all documents to be transcribed. Even if only a small proportion of an archive is 13.V&A thumbnail selector, Search the collections, http://collections.vam.ac.uk/ crowdsourcing/. 14.Library of Congress, Civil War Faces, http://www.flickr.com/photos/library_of_ congress/sets/72157625520211184/. 15.National Maritime Museum, Collections, http://collections.rmg.co.uk/. 16.Australian Newspapers Digitisation Program, http://www.nla.gov.au/content/ newspaper-digitisation-program. 17.Holley, ‘Many Hands Make Light Work’. transcribed this may encourage others to continue the work, as long as the interface available for doing so is simple and accessible. Citizen Science Projects in Cultural Institutions The previous examples give a general impression of crowdsourcing projects that cultural institutions are undertaking. It seems that crowdsourcing has become an accepted way of enhancing interpretation, knowledge and engagement of collections; but what about citizen science? When reviewing citizen science projects like the Global Tree Banding Project18 or Biodiversity snapshots19 that have been undertaken by cultural institutions it is clear that these projects are primarily partnerships with groups of recognised scientists. At present, cultural institutions primarily encourage audiences to collect new data through experimentation and/or observation – such as the Royal Museums Greenwich Royal Observatory’s Solar Stormwatch,20 or they are partnering in the analysis of data that is relevant to their key subject areas.21 These projects are fantastic in the results that they produce and the data that they contribute towards scientific research, but they do not directly benefit the museum’s collections because they cannot be fed back directly into museum object knowledge or interpretation. A key question, then, is whether museums can use citizen science to promote and enhance their collections while still producing a genuine scientific result? Old Weather Launched in October 2010, Old Weather (www.oldweather.org) is a website that asks the public to help improve reconstructions of past weather and climate across the world by finding and recording historical weather observations in ships’ logs. The initial Old Weather project, as established by the citizen science group Zooniverse, the UK’s national weather service the Met Office, the National Maritime Museum and Naval-History.net, asked the public to transcribe historic handwritten Royal Navy ships’logs that dated from 1914 to 1923 with the goal of creating structured weather data from the handwritten records. The original logs were held by the National Archives; since launch the scope has been expanded to 18 Global Tree Banding Project, https://treebanding.si.edu/. 19 Biodiversity snapshots, http://www.biodiversitysnapshots.net.au/BDRS/home.htm. 20 Solar Stormwatch, http://www.solarstormwatch.com/. See Romeo and Blaser, ‘Bringing Citizen Scientists and Historians Together’for further discussion of this project. 21 Old Weather, http://www.oldweather.org/. include logs held by other institutions for additional periods and navies.22 These data are being used by scientists to improve climate prediction models that aid our understanding of climate variability and change in the past and therefore the future. The National Maritime Museum participated in this project as it strongly linked with the Museum’s subject matter and the Museum would be able to benefit from the interesting stories and knowledge that would arise from the transcription of these documents. While the source documents were not in its collection, the information tied in closely with records that it does hold and the Museum could engage users further with links to historic photographs that would bring these vessels to life, making this project more than just a two dimensional transcription project. Institutions, though often driven by their collections, can link into projects that do not directly benefit them because their skills and expertise will enhance the overall experience of the user, which may in turn result in users connecting with other institutions. Do we have to be selfish and only think of ourselves in the results of crowdsourcing and citizen science projects, or is the ability to say that as an institution you have helped a large number of users engage with your subject matter in a meaningful way more than enough? Old Weather was probably one of the first citizen science projects that combined common crowdsourcing techniques, such as transcription, with a historical collection and a scientific goal. This project has shown that citizenscience projects have the potential to meet the needs of cultural institutions by helping interpret, interrogate and enhance their collections. The primary goal of working with these historic records was to generate scientific data and these documents were used because they are a well-known and reliable but previously untapped source. Naval vessels recorded weather information such as wind speed, direction and temperature alongside their location and movements at regular intervals no matter what they were doing or where they were. This means we can find out the weather in the middle of an ocean and other distant regions of the world, filling in gaps where traditional weather records do not exist for this time period. When developing the project it was initially supposed that the project would attract and retain people who were interested in the environment and the climate change debate, with the transcription of the ships’ activities taking second place. Users had the ability to transcribe more than the weather observations but were not required to do so, so the extent of transcription of the ships’ records was up to the individual. However, through observing interactions in the forum and user comments it became clear that it was the content of the documents – particularly daily events on board noted in the logs – that were keeping users coming back.23 Through transcribing the events and interactions with ‘other ships, battle debris, 22.See http://www.oldweather.org/about for a current list of partners and content providers. 23.Romeo and Blaser, ‘Bringing Citizen Scientists and Historians Together’. icebergs or sea ice, animals, aircraft [or] land’24 users became swept up in the stories of the vessels and the men on board, reliving the events with the crew. Post by: cyzaki on December 01, 2010: It’s so exciting when you choose a new ship! Do you go for one you think might be exciting, or one with beautiful handwriting, or one that seems sad and lonely with nobody caring for it? Or do you try for the jackpot of all three at once?! (Old Weather forum thread, ‘Riveting Log Entries’)25 Post by: christopherhenry on October 14, 2010: I have just started work on HMS Liverpool, my father-in law sailed on her from 1916–1919, the log started in March 1915 and I am now up to June 1915, will I be able to continue right through to when my father-in law sailed on her or will I miss out if another volunteer takes over and beats me to these pages? (Old Weather forum thread, ‘General Questions’)26 Through observing how the Old Weather volunteers were using the site via forum posts like these and discussions with the ‘super-users’, users were encouraged to transcribe the log accounts of life on board ship more fully. The historians saw it as a way of getting access to this historic information and the scientists considered that data as having the potential to enhance the valuable weather data. As the users transcribed more and more records the historians and scientists behind the project realised the potential outcomes for not only the institution that held these records but also for researchers of this period. Post by: thursdaynext on December 06, 2010: It’s good to know I’ll be able to follow all the stories through eventually. I’ll just have to be philosophical about it – the log has been there 95 years so I guess I can wait a bit longer. And after all, a couple of months ago I had never even heard of Old Weather and had no idea that I would ever care about what was happening on the Mantua in 1915! (Old Weather forum thread, ‘Has anyone else become Endeared …’)27 As a result of this dedication by Old Weather users the scientists and historians worked together to develop the Voyages28 feature that replayed all of the ship’s activity as an animation over a map of its journey using data from the transcriptions of the completed ship’s log. This feature really brought to life the stories held within the logs as well as the scientific data that were being 24.‘Old Weather – FAQ’, undated. Old Weather, http://www.oldweather.org/faq. 25.http://forum.oldweather.org/index.php?topic=209.0. 26.http://forum.oldweather.org/index.php?topic=9.0. 27.http://forum.oldweather.org/index.php?topic=71.0. 28 Old Weather Voyages, previously available at http://oldweather.org/voyages. extracted by displaying updates like geolocated tweets on a map showing a vessel’s movements. The team realised that to encourage participation in a citizen science project that did not involve beautiful source data, such as Galaxy Zoo’s astronomical photographs,29 building a community and drawing people into the stories behind the documents was essential, even if this work did not have a direct scientific outcome. The scientists saw that to promote the project and ensure its success it would be necessary to identify what the audience wanted from the material and not solely what the project wished to promote. As discussed above, the project discovered that transcribing the documents as fully as possible was important to the users. As a result future iterations of the project have tapped into this and any functionality that could enhance their ability to transcribe data better was promoted by the project. While transcribing the logs many of the users started developing their own data sets using the records based on their own interests. Examples of these are the number of victualing stops (in non-maritime terms this can be understood as landing to restock supplies), encounters with other ships and sickness records. This latter group particularly related to the spread of the 1918 ‘Spanish Flu’pandemic. These user-identified data sets clearly show the potential that collections, in particular historical documents, have for further interrogation in a citizen science project. If these records had not been made available online for analysis and interrogation by the public, these data would not have been identified within these documents. The Zooniverse Galaxy Zoo (www.galaxyzoo.org) project provides further examples of serendipitous discovery within data. The participant-driven campaign ‘Give peas a chance’ resulted in the discovery of a new class of astronomical object: small, compact galaxies forming stars at an incredible rate. Although not directly collections-related it shows the power of a vast number of targeted eyes and engaged users. The extraction of data from historic material should not be seen as the only benefit of a crowdsourcing or a citizen science project. It is important to note that these projects also have an impact upon the lives of the volunteers who take part. Old Weather encouraged users who were new to the subject of naval activities during the First World War to find out more about it. Users also became interested in studying handwriting techniques in order to improve their efficiency within the project. For a cultural institution to be able to record clear accounts of how working with their collections has impacted upon the lives of those interacting with it is a fantastic outcome by itself. As has been shown by the Old Weather project, cultural heritage collections hold potential value for citizen science projects. By looking at them with the eyes of a scientist to realise their potential, opportunities for cultural institutions and scientists to work together can be discovered. Once projects are underway, the potential data that these records may hold may be further highlighted by the questions that audiences and crowdsourcing participants ask of the material. 29 Galaxy Zoo, http://www.galaxyzoo.org/. By opening up resources to reach those who may not normally interact with collections, unseen patterns and possibilities can emerge that could unlock the potential of such projects. Collections that could be used for citizen science should not just be limited to archives and manuscripts: photographic images, prints and drawings and paintings also hold the potential to aid scientists and researchers. Diaries may hold information about the environment, health of a community or country. Official records may document weather and health conditions, and paintings and natural history collections may be able to show environmental changes to an area or the potential density of a species. Old Weather was initially promoted as a climate science project as this was the scientific goal of the project, but the audience saw it as a historical research project. This shows that just because a project may be a citizen science project it does not have to be promoted only as science. Old Weather demonstrated that it was the strength of the collection that pulled people in and kept them there. It appears that museums can use citizen science to promote and enhance their collections while still producing genuine scientific results, particularly when projects are able to respond to new areas of enquiry that emerge as participants interact with the collections. Of course crowdsourcing and citizen science cannot be seen as a solution to all problems that cultural institutions face with their collections. These projects present their own set of problems, such as the best way to incorporate the generated data back into the institution’s systems. These projects also develop user communities around them which require tending and encouragement in order to produce successful results. Institutions need to identify how they can maintain the passion that is often developed around a project and incorporate the established project communities into the considerations of the project in order to continue the successful relationship between audiences and the institution. If an institution recognises the power of citizen science and the power of their collections then adjusting the way it publicises or catalogues its material may present the opportunity for the public to discover and interpret material in creative and interesting ways. References Holley, R. ‘Many Hands Make Light Work: Public Collaborative OCR Text Correction in Australian Historic Newspapers’, 2009. http://www.nla.gov.au/ openpublish/index.php/nlasp/article/viewArticle/1406. Raddick, M.J., Georgia Bracey, Pamela L. Gay, Chris J. Lintott, Phil Murray, Kevin Schawinski, Alexander S. Szalay and Jan Vandenberg. ‘Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9, no. 1 (2010). http://portico.org/stable?au=pgg3ztfdp8z. Romeo, Fiona and Lucinda Blaser. ‘Bringing Citizen Scientists and Historians Together’. In Museums and the Web 2011: Proceedings, edited by Jennifer Trant and David Bearman. Toronto, Canada: Archives & Museum Informatics, 2011. http://www.museumsandtheweb.com/mw2011/papers/bringing_citizen_ scientists_and_historians_tog. This page has been left blank intentionally Chapter 3 ‘Many Hands Make Light Work. Many Hands Together Make Merry Work’1: Transcribe Bentham and Crowdsourcing Manuscript Collections Tim Causer and Melissa Terras The philosopher and reformer, Jeremy Bentham (1748–1832), was a firm supporter of innovation and inquiry. Amongst other things, Bentham proposed a scheme for preventing the forgery of bank notes,2 and in the designs of his proposed ‘panopticon’ prison, provided a detailed description of how the building would be heated, as well as a network of ‘conversation tubes’ which would allow the prison inspector to communicate instantly with individual prisoners in their cells.3 Bentham’s home, at Queen’s Square Place in Westminster, was itself centrally heated.4 Bentham believed that modern, scientific enquiry was the most accurate means by which to investigate and solve social ills. His principles and methods were adopted by social reformers of the 1820s and 1830s, who achieved the amelioration of the criminal code, the ending of convict transportation to New South Wales, the widening of the electoral franchise, and the crowning glory of the 1830s: the abolition of slavery across the British Empire in 1833.5 None of these reforms would have been possible, Bentham would have argued, without the widespread availability of knowledge and evidence. Following in his example, the Bentham Papers Transcription Initiative (Transcribe Bentham) has utilised modern technology to digitise the vast 1.This quotationis from a Benthammanuscript dated 21 December 1793, discovered by volunteer transcriber Peter Hollis. The fuller quotation reads: ‘Many hands make light work[.] Many hands together make merry work. Each to take the work of all the rest and critisize [sic] it’ (emphasis in original). See http://www.transcribe-bentham.da.ulcc.ac.uk/ td/JB/107/020/001, revision dated 15.37, March 18, 2013. 2.Pitkin, ‘Slippery Bentham’, 105. 3.Jeremy Bentham, ‘Panopticon Postscripts: Section XXIII’ and ‘Section VIII’, in Bowring, vol. 4, 110–18, 84–6. 4.O’Sullivan and Fuller, The Correspondence of Jeremy Bentham: Volume 12 (Collected Works of Jeremy Bentham, hereafter ‘CW’), 280–2n. 5.For example, see Quinn, Writings on the Poor Laws (CW), vols 1 and 2. collection of manuscripts written and composed by Bentham – held by University College London (UCL) Library’s Special Collections6 – and to make them available for scholars, students and the public at large to access and transcribe via a specially designed web platform. A collection of great historical and philosophical importance, previously only accessible on a research trip to London, is now progressively being made available to anyone in the world with an internet connection, and in a way which allows interested individuals to engage and contribute to our growing knowledge about this fascinating historical figure. Transcribe Bentham: Why? Bentham is perhaps best known for two things. First, for the aforementioned panopticon prison, based upon an idea conceived by Samuel Bentham, Jeremy’s younger brother. In the panopticon, the prisoners’cells were to be arranged in a circle around a central inspection tower, exposing the inmates to what they had to assume was constant surveillance by an unseen inspector, and thereby causing them to modify their behaviour to avoid punishment. By this ‘simple idea in Architecture’, as Bentham put it, the deviancy of criminals could be cured, and the ‘central inspection principle’ would be equally applicable to poor houses, factories, insane asylums and schools.7 Second, Bentham willed that his remains – in the hope that others would be encouraged by his example to donate their own bodies to medical science – were to be publicly dissected, and then ‘put together in such a manner as that the whole figure may be seated in a chair usually occupied by me when living in the attitude in which I am sitting when engaged in thought’.8 Bentham left his corpse to his friend, Dr Thomas Southwood Smith, who dissected the body, and then reassembled and dressed the skeleton. For the next 18 years, Bentham’s auto-icon (‘self-image’) sat in Smith’s house, until in 1850 it was brought to UCL.9 However, the panopticon and the auto-icon tend to obscure Bentham’s enduring importance in a wide range of fields. Bentham is one of the world’s great thinkers, whose thoughts and ideas have had a profound historical impact and are still of contemporary significance. He was the founder of the modern 6.UCL Special Collections, http://www.ucl.ac.uk/library/special-coll/ (last accessed February 21, 2013). 7.Bentham, ‘Panopticon, or, The Inspection-House’, in Bowring, vol. 4, 39. See pp. 39–66 for the panopticon letters, and pp. 67–172 for the lengthy, detailed ‘postscripts’. The key study of the panopticon is Semple, Bentham’s Prison; the panopticon was never built, and its failure was the great regret of Bentham’s life. 8.Bentham, Auto-Icon and Last Will and Testament. See also Marmoy, ‘The “Auto-Icon” of Jeremy Bentham’. 9 See http://www.ucl.ac.uk/Bentham-Project/who/autoicon and http://www.ucl.ac.uk/ Bentham-Project/who/autoicon/Virtual_Auto_Icon (both last accessed February 15, 2013). doctrine of utilitarianism: that the right and proper end of all action and legislation is to promote the greatest happiness. Bentham laid out a systematic theory of punishment which emphasised deterrence, proportionality of punishment and reformation of prisoners, his Nonsense upon Stilts is an influential critique of the doctrine of natural rights (the forerunner of human rights theory)10 and he was an important theorist of representative democracy. Bentham wrote on topics as varied as political economy, religion, jury reform and sexual morality (and this is only a summary). Researchers and students wishing to access Bentham’s thought, however, face a substantial obstacle: the edition of Bentham’s works published between 1838 and 1843 by his literary executor, John Bowring is sorely inadequate for the needs of modern scholarship. The ‘Bowring edition’ is incomplete, as it omits several works published in Bentham’s lifetime (particularly those concerning the sensitive topics of religion and sexual morality)11 and substantial unpublished works which survive in manuscript. The edition also includes edited translations into English of ‘simplified’French versions of some of Bentham’s works, produced by another of his disciples, Etienne Dumont, so there is a question concerning the extent to which these texts are authentically Bentham’s, as opposed to Dumont’s and the translator’s.12 Finally, the edition’s densely typeset text makes it a chore to use, and its biography of Bentham has been described as ‘one of the worst biographies in the [English] language, out of materials which might have served for a masterpiece’.13 There was, then, until relatively recently, no adequate edition of Bentham’s works which accurately represented his writings as he envisaged them. An attempt to rectify this deficiency began in 1959 with the foundation of the Bentham Project at UCL, which is engaged in producing the new, critical edition of the Collected Works of Jeremy Bentham, based on both Bentham’s published works and his unpublished manuscripts, and returning to what Bentham himself actually wrote. It seems an almost Sisyphean task: UCL’s Bentham collection runs to some 60,000 manuscript folios (estimated to contain c. 30,000,000 words), while the British Library holds a further 12,500 folios (c. 6,250,000 million words). Thirty of an estimated 70 volumes of the new edition have been published, and a total of around 28,000 folios have been transcribed. The majority of the Bentham Papers therefore remain untranscribed and their contents largely unknown, save for an outline index,14 and the greater part of 10.Schofield et al., Rights, Representation and Reform (CW). 11.Bentham’s writings on sexual morality were published in Schofield et al., Of Sexual Irregularities (CW), in 2014. For a summary, see Schofield, Jeremy Bentham: Prophet of Secularism. The Bentham Project has published online a preliminary text of the third, unpublished volume of Bentham, Not Paul, but Jesus. 12.Schofield, Bentham: A Guide for the Perplexed, 19–43. 13.Stephen, The English Utilitarians, 225. 14.Milne, Catalogue of the Manuscripts of Jeremy Bentham. the Collected Works has yet to be published. As a result, we only have a partial understanding of the true extent of Bentham’s thought, as well as its historical and contemporary significance. The purpose of Transcribe Bentham is threefold. First, it produces transcripts of Bentham manuscripts of sufficient quality for uploading to UCL’s free-to-access digital repository for access, searching and to ensure the collection’s long-term digital preservation and curation.15 Second, it allows volunteers from around the world to contribute to humanities research: their transcripts will act as a starting point for editors of future volumes of the Collected Works, and volunteers will be fully credited in the volumes to which they contribute.16 Furthermore, as many manuscripts have not been read since Bentham wrote them, there is also the potential for exciting new discoveries to be made which could change our perception of Bentham’s thought. For example, the work of volunteers has shown that a substantial unpublished portion of ‘Panopticon versus New South Wales’,17 Bentham’s attack on convict transportation, exists in manuscript.18 Thirdly and finally, Transcribe Bentham was formulated as an experiment. The task required of volunteers is perhaps more complex and challenging than in many other crowdsourcing projects, and demands a high degree of concentration and engagement with a source material which is not, in many instances, the most immediately accessible or attractive. Would volunteers – who may not have had any palaeographical training, or have previously encountered historical manuscripts – manage to read and decipher Bentham’s handwriting? Would they be able to identify the structural and compositional features of the manuscripts and mark these up in Text Encoding Initiative (TEI)-compliant Extensible Mark.up Language (XML), while also navigating Bentham’s idiosyncratic style, along with his often challenging ideas? In addition, would the work of volunteers be of sufficient quality to act as a basis for editorial work, and for uploading to a digital repository for public access? And would Transcribe Bentham prove to be worthwhile both in terms of cost and time? After almost three years’ experience, we are delighted to say that the answer to all of these questions is, to varying degrees, ‘yes’, as we will subsequently discuss. 15.http://www.ucl.ac.uk/library (last accessed February 5, 2013). 16.See Causer et al., ‘Transcription Maximized’. 17.Bentham, ‘Panopticon versus New South Wales’, in Bowring, vol. 4, 173–248. Bentham wrote this work in 1802, and it was privately printed in 1803. It was not published for public consumption until 1812, and was reproduced in the 1838–43 edition of Bentham’s works. 18.For volunteers’ discoveries, see Causer and Terras, ‘Crowdsourcing Bentham’. Transcribe Bentham: What? Transcribe Bentham is coordinated by UCL’s Bentham Project,19 in partnership with UCLCentre for Digital Humanities,20 UCL Library Services,21 UCL Creative Media Services22 and the University of London Computer Centre (ULCC).23 In October 2012, the British Library24 joined the project consortium. Transcribe Bentham has, thus far, had three ‘phases’. Period 1: Design and Establishment (March 2010 to 8 March 2011) Transcribe Bentham has its foundations in metadata compiled between 2003 and 2006 for the Bentham Papers Database Catalogue.25 The Catalogue records 15 fields of information, including dates, headings and titles, for each of the 60,000 folios in the UCL Bentham Papers collection. It was initially conceived of as a resource for Bentham Project editorial staff and researchers consulting the UCL Bentham Papers, though it was hoped that one day it could be improved by adding transcripts and digital images. The Transcribe Bentham consortium was successful in securing a £262,673 grant from the Arts and Humanities Research Council’s (AHRC) Digital Equipment and Database Enhancement for Impact (DEDEFI) scheme, a one-off call to fund projects for 12 months.26 This was invested primarily in digitising around 12,000 folios, on the production of a collaborative transcription platform developed by the ULCC, and on the salaries of two full-time Research Associates to coordinate the programme.27 At the heart of the project is the ‘Transcription Desk’, a customised installation of the MediaWiki software application, which incorporates the transcription platform and other elements important to the project (see Figure 3.1). The use of MediaWiki is a key factor in Transcribe Bentham’s success: it is perhaps the world’s single most widely used collaborative open-source software for authoring online content, is stable, well-documented and has a global user base. Moreover, it is an interface which is instantly familiar to the millions of people who use 19.http://www.ucl.ac.uk/bentham-project (last accessed February 5, 2013). 20.http://www.ucl.ac.uk/dh (last accessed February 5, 2013). 21.http://www.ucl.ac.uk/library (last accessed February 5, 2013). 22.http://www.ucl.ac.uk/isd/common/creative_services (last accessed February 5, 2013). 23 http://www.ulcc.ac.uk (last accessed February 5, 2013). 24.http://www.bl.uk (last accessed February 18, 2013). 25.http://www.benthampapers.ucl.ac.uk (last accessed February 18, 2013). The database catalogue was compiled by Dr Deborah Colville and funded by the AHRC. 26.http://www.ahrc.ac.uk/FundingOpportunities/Pages/dedefi.aspx (last accessed February 5, 2013). 27.See Causer et al., ‘Transcription Maximized’, 121–2 for allocation of the AHRC grant. Wikipedia each day, is customisable, easily maintained and offers a full revision history for every individual page in case of malicious or accidental edits, or spam. As well as transcribing the text, volunteers also encode key features of Bentham’s manuscripts in TEI XML. Use of TEI has become best practice for systematically encoding texts, whether prose, poetry, drama, primary source material and more besides. All the elements of the text can be encoded and identified with varying degrees of granularity, ranging from entire paragraphs down to lines, clauses and words, thereby allowing computers to read, understand and represent both the content and the appearance of the text, and to facilitate complex searching and querying of the transcribed corpus. Transcribe Bentham volunteers can, for example, indicate Bentham’s deletions and interlineal additions through TEI mark.up, and these are represented in the rendered version of the transcript, providing an accurate digital representation of the original manuscript. TEI mark-up also has the added advantage of allowing the transcripts to be easily converted into any number of formats, and ensures their long-term preservation. Plain text transcripts of the material would be pointless: searching would be crude and imprecise, the transcripts would look nothing like the manuscript from which they were derived and conversion to other file formats would be labour- and time-intensive. It was recognised that transcribers may not have any experience of mark-up, let alone TEI, and so a method by which volunteers could easily encode their transcripts was devised: the Transcription Toolbar.28 Two MediaWiki extensions were developed by ULCC to facilitate the addition of mark-up by transcribers: JBZV, which adds an image frame next to an editing form, so that the manuscript could be transcribed into a text box and then saved; and JBTEIToolbar, allowing TEI mark-up to be automatically applied at the click of a button, and which renders the encoded transcript in the Wiki.29 Using the toolbar, volunteers can straightforwardly indicate structural features of the manuscripts such as line-breaks, page-breaks, paragraphs and headings, and compositional features such as underlinings, additions, deletions and marginal notes, without necessarily having to learn the minutiae of mark-up (see Figure 3.2).30 In practice, a volunteer is presented with a zoomable image of a manuscript, a plain-text data entry box into which they enter their transcript and the transcription toolbar. When satisfied with their transcript, the volunteer submits it for assessment by a Transcribe Bentham project editor, who checks for textual accuracy and consistency of encoding. Changes are made to the text and mark.up, if necessary, the key question being whether appreciable improvements are likely to be made through further crowdsourcing, and if the transcript is of the requisite quality for public viewing and searching, and as a basis for editorial 28.For detailed discussion of the use of TEI in Transcribe Bentham, see Causer et al., ‘Transcription Maximized’, 121–5. 29.See http://www.mediawiki.org/wiki/Extension:JBZV and http://www.mediawiki. org/wiki/Extension:JBTEIToolbar. 30.Causer et al., ‘Transcription Maximized’, 122–3. work. If approved – if there are few or no unclear words or gaps in the text – the transcript is locked. If there are a number of gaps in the text, or the text is only partially transcribed, then the transcript remains available for editing. In either circumstance, an acknowledgement message is left on the submitter’s user page. Though an unavoidably impressionistic and subjective judgement, the quality-control process does ensure that locked transcripts are a reliable guide to the contents of the manuscripts, and encourages volunteers by providing feedback and an acknowledgement of their work. Transcribe Bentham was launched to the public on 8 September 2010 for a six month testing period, which ran until 8 March 2011.31 During the first three months, the rate of transcription was steady but unspectacular: by 23 December 2010, 350 users were registered with the project, and 439 manuscripts had been transcribed or partially transcribed (see Figure 3.3). Only one volunteer regularly participated, while others took part on a sporadic basis. At this stage, the project did not seem all that successful, but a December 2010 New York Times article about Transcribe Bentham and crowdsourcing in the humanities had a transformational effect.32 By way of illustration, from 8 September to 23 December 2010, an average of 25 manuscripts (c. 12,500 words) were transcribed or partially transcribed each week, whereas from 24 December 2010 to 8 March 2011, this increasedto an average of 57 manuscripts (c. 28,500 words) per week. In short, the New York Times article and associated media coverage gave Transcribe Bentham momentum which has remained with the project ever since. By the end of the testing period, Transcribe Bentham was in good shape. In total 1,222 volunteers had registered an account, and 1,009 manuscripts (c. 504,500 words) had been transcribed or partially transcribed, of which 559 (55 per cent) were complete. Period 2: Consolidation (9 March 2011 to 30 September 2012) Though the AHRC grant continued until 30 April 2011 to allow for reporting, full-time staffing of the Transcription Desk ceased on 8 March 2011, and this was communicated to volunteers.33 We anticipated that this would result in a much reduced rate of transcription, and our fears appeared to be confirmed when all but three of the then seven regular transcribers ceased participating. However, these concerns were ultimately misplaced, as the 18 months after 9 March 2011 proved to be a highly successful period for Transcribe Bentham, despite the project running only on small-scale funding provided by UCL, covering web storage costs and two days per week of staff time (there was no money for further 31.For Transcribe Bentham’s testing period, see Causer and Wallace, ‘Building a Volunteer Community’. 32.Patricia Cohen, ‘Scholars Recruit Public for Project’. 33 http://blogs.ucl.ac.uk/transcribe-bentham/2011/03/08/six-months-later/ (last accessed February 20, 2013). Note: Gaps in the data are owing to staff being away during UCL vacations. digitisation or for modifications to the transcription interface). By 30 September 2012, 1,939 users had registered an account with Transcribe Bentham, and 4,412 manuscripts (c. 2,200,000 words) had been transcribed or partially transcribed, of which 4,185 (94 per cent) were complete.34 An average of 42 manuscripts were worked on each week during this period, and the transcription rate was particularly high from mid-September 2011 to mid-March 2012, most likely owing to a Sunday Times article on scholarly crowdsourcing of 11 September which mentioned Transcribe Bentham.35 Also that month, Transcribe Bentham received a major international prize: an Award of Distinction in the Digital Communities category of the 2011 Prix Ars Electronica.36 Despite Transcribe Bentham’s successes and the continuing engagement of a core group of volunteers, without further investment there was a danger that the project might stagnate if issues raised by volunteers in a survey of early 2011 were not addressed. Some of the survey’s most important findings were in understanding volunteer motivations, and what dissuaded participants from transcribing more (or at all). Survey respondents reported that they took part mainly owing to interests in: Bentham’s life and thought; history and philosophy; crowdsourcing and the technology behind the project; and a sense of altruism, taking part in something which will ultimately benefit the wider community. On the other hand, respondents told us that the main factors which limited their participation were: a lack of time in which to learn how to transcribe Bentham’s handwriting; various issues with the Transcription Desk; the difficulty of deciphering Bentham’s hand; and the TEI mark-up was considered by several volunteers as an aggravation to an already demanding task. A failure to address these very real concerns ran the risk of alienating regular Transcribe Bentham participants, and of limiting the recruitment of a wider pool of volunteers.37 Period 3: Expansion (1 October 2012 – onwards) Fortunately, the Bentham Project and the Transcribe Bentham team were successful in securing further funding. For two years from 1 October 2012, the initiative is supported by a grant of £336,157 ($538,000) from the Andrew W. Mellon Foundation’s ‘Scholarly Communications’ programme, for a wider scheme entitled the Consolidated Bentham Papers Repository, with the British Library joining the project consortium. This funding will, we believe, allow Transcribe 34.The large increase in completed transcripts was owing to project staff working through all partially transcribed manuscripts, and increased proficiency of Transcribe Bentham’s regular participants. 35.Kinchen, ‘One Stir, Then I’ll Discover a Galaxy’. 36.See http://archive.aec.at/#42434 for Transcribe Bentham’s citation, and http:// www.aec.at/prix/en/ for more about the Prix Ars Electronica (both last accessed February 20, 2013). In 2009, this award was given to Wikileaks. 37.Causer and Wallace, ‘Building a Volunteer Community’. Bentham to achieve its full potential. Most of the remaining UCL Bentham Papers will be digitised, along with all of the 12,500 folios of Bentham manuscripts held by the British Library. Metadata will be compiled for the latter collection, and the 20,000 or so transcripts produced by the Bentham Project in Microsoft Word during the past 25 years will be converted to TEI XML. All of the manuscripts will be made available for crowdsourced transcription, and the images and transcripts will ultimately be stored in UCL’s digital repository, thus reuniting Bentham's papers for the first time since his death. Taking into account feedback from volunteers, and following work from ULCC, an upgraded Transcription Desk was launched on 15 July 2013, offering significant improvements to the user. Administrative processes, including the uploading of images, mapping them to the relevant metadata and the tiling of images for incorporation into the image viewers have also been automated. The code for this interface is available on an open-source basis, as a documented package for others to use and customise.38 The upgraded Transcription Desk aims to make transcription more straightforward for volunteers. Changes include an image viewer which allows volunteers to rotate the manuscript image, given Bentham’s not uncommon habit of writing into or up the margin of a page, and at unusual angles (see Figures 3.4 and 3.5).39 In order to take advantage of as much screen space as possible, ‘maximise’ and ‘minimise’ buttons have been added to the transcription interface; clicking the former clears from the screen all extraneous matter to show as much of the image as possible.40 Other features are forthcoming, including making it more straightforward to select material to transcribe, and automated reporting of project statistics, which are currently manually compiled. Perhaps the key change is the introduction of a tabbed transcription interface (see Figures 3.4 and 3.5): the ‘Wikitext’ tab displays the transcription area; ‘Preview’ generates a live preview of how the encoded transcript will look when saved; and the ‘Changes’ tab displays a highlighted list of changes the volunteer has made to the transcript. We hope that this will allow volunteers to understand better and more easily how the TEI mark-up works by allowing them to switch, at the click of a button, between their encoded transcript and a rendered preview. In the previous version of the Transcription Desk, participants had to save their work and leave the transcription interface to see their transcript rendered, causing a potential loss of concentration and making it much more difficult to compare the 38.https://github.com/onothimagen/cbp-transcription-desk (last accessed July 15, 2013). 39.For an extreme example, see http://www.transcribe-bentham.da.ulcc.ac.uk/td/ JB/079/047/001 (last accessed July 30, 2013). 40.Bentham coined the terms ‘maximise’ and ‘minimise’. Other Benthamic neologisms in general usage include ‘international’and ‘codification’; less widely adopted were, for example, ‘circumgyration’ and ‘jentacularisation’ (both for jogging, of which Bentham was a proponent). mark-up with the representation of the text. Likewise, comparing changes made to an earlier version of a transcript involved leaving the interface, and then entering the given page’s revision history. By making it more straightforward to see the functioning of the mark-up, it is hoped that the tabbed transcription interface will reduce the number of encoding errors, and further increase the efficiency of the quality-control workflow, particularly when dealing with lengthy manuscripts with a complex structure. Early indications suggest that the quality-control process is indeed quicker with the tabbed interface, though data need to be gathered over a significant period before firm conclusions can be drawn. We anticipate that the upgraded interface will help to increase user recruitment and retention, making it easier for new volunteers to participate, while also supporting the work of experienced transcribers. Feedback from volunteers suggests that they regard the upgraded website as cleaner, faster and more inviting. According to several transcribers, the text in the transcription box is better spaced and easier to read, the tabbed interface allows much greater flexibility and the rotatable image viewer is a boon. In particular, the ‘maximise’ button and the expanded transcription area it provides are much appreciated, especially by those using laptop computers. While the improvement work was carried out, volunteers continued to transcribe at a healthy pace. As of 19 July 2013, 2,934 accounts were registered with Transcribe Bentham.41 A total of 5,799 manuscripts (c. 2,800,000 words) had been transcribed or partially transcribed, of which 5,528 (95 per cent) were complete. Although, the transcription rate slowed a little during this period to an average of 34 manuscripts (c. 17,000 words) each week, Transcribe Bentham is currently in a healthier state than ever before.42 In November 2012, the initiative received another award, coming second in the ‘Platforms for Networked Innovation’ competition, run by KNetworks project.43 The methodology and expertise developed in Transcribe Bentham will also be utilised and tested further in a connected project, entitled tranScriptorium.44 This scheme is funded by the European Commission’s Seventh Framework Programme (1 January 2013 to 31 December 2015), in the ‘ICT for Learning and Access to Cultural Resources’challenge, and aims to develop innovative, efficient and cost-effective solutions for the indexing, searching and full automated transcription of manuscript images, using Handwritten Text Recognition (HTR) technology. tranScriptorium is led by the Universitat Politènica de València (Spain), with 41.This does not include project staff, robots and 647 blocked spam accounts. Spam on the Transcription Desk manifests as the creation of pages with links to commercial websites. All spam accounts and pages are blocked and deleted. 42.The latest progress statistics are updated on a weekly basis at http://blogs.ucl. ac.uk/transcribe-bentham. 43.http://blogs.ucl.ac.uk/transcribe-bentham/2012/11/22/transcribe-bentham-receives-award-in-knetworks-competition/. 44.http://transcriptorium.eu (last accessed February 20, 2013). a consortium comprised of the University of Innsbruck (Austria), the National Center for Scientific Research ‘Demokritos’ (Greece), the Institute for Dutch Lexicology, UCL and ULCC. UCL will provide images and TEI transcripts of Bentham manuscripts and will, with ULCC and the other partners, develop and implement a crowdsourcing platform in which automated HTR transcripts of English, Dutch, German and Spanish manuscripts will be made available. Volunteers will be asked to correct these transcripts and help ensure that the software’s future results are more accurate. Incorporation of HTR technology into Transcribe Bentham affords exciting possibilities, though it is not without risk for a project with an established group of users. There is, for example, the danger that regular transcribers might feel their skills and role are devalued, and become alienated, if they believe they are being replaced by a machine. However, initial discussions with regular Transcribe Bentham participants suggests they would view HTR technology as being complementary to their work, and that it may even encourage an element of productive competition, as volunteers attempt to ‘beat the computer’ for accuracy. Early results suggest that the HTR software can produce accurate transcripts of legible and standardised Bentham material, but it may cope less well with more complex manuscripts and their innumerable deletions and additions, and with material composed towards the end of Bentham’s life when both his eyesight and handwriting deteriorated. In this scenario, we envisage that incorporating HTR into Transcribe Bentham will allow two tasks to be offered: full transcription of manuscripts by engaged volunteer transcribers, as happens now using the existing transcription interface; and correction of HTR-generated transcripts by text correctors who may not have as much time to devote to the project. There may be significant cross-over between the two groups: transcribers may wish to do the more straightforward task as light relief or when time is short, while the text correctors may wish to test their skills by moving on to full transcription. More generally, tranScriptorium’s HTR software promises to be an extremely exciting development in making vast swathes of digitised manuscripts discoverable to the public, and we are delighted to be part of the project. Participation Crowdsourcing projects, from Wikipedia, to Galaxy Zoo, to the National Library of Australia’s newspaper text-correction programme, have found that although they may have thousands of registered volunteers, most work is in fact done by a minority of users. Transcribe Bentham is no different: though 2,934 users had registered with the project by 19 July 2013, only 382 (13 per cent) had transcribed a manuscript, or a part thereof. Of those who did participate, almost two-thirds worked on only a single manuscript (see Table 3.1). Every single contribution to Transcribe Bentham is greatly appreciated, whether it is the transcription of a sentence or an entire page. But the fact Table 3.1 Number of manuscripts worked on by volunteers, 8 September 2010 to 19 July 2013 No. of manuscripts worked on No. of volunteers (percentage) 0 2,552 (86.9) 1 238 (8.1) 2 68 (2.3) 3 25 (0.9) 4 6 (0.2) 5 to 20 26 (0.9) 21 to 50 5 (0.2) 51 to 100 6 (0.2) 101 to 200 2 (<0.1) 201 to 999 3 (0.1) 1,000+ 3 (0.1) Total 2,934 (100) Note: ‘Worked on’ is defined as the volunteer having transcribed at least some part of a manuscript, and having clicked ‘save’ at least once to register their edit/edits. remains that the great majority of the work has been carried out by a core of 17 ‘Super Transcribers’ (see Table 3.2), of whom 10 currently participate. These expert volunteers sustain the project, and it is one of Transcribe Bentham’s great strengths that such dedicated, skilled participants submit high-quality work on a regular basis; in several cases, Super Transcribers have now transcribed more Bentham manuscripts than some Bentham Project staff. For instance, by 19 July 2013 volunteer Jfoxe had worked on 1,444 manuscripts (c. 722,000 words), Diane Folan 1,201 (c. 600,500 words) and Lea Stern on 1,044 transcripts (c. 522,000 words). However, heavy reliance upon Super Transcribers does leave Transcribe Bentham in a precarious position: if one or more ceased participating, then the transcription rate would decrease precipitously. That the overall level of participation among all registered users is so low would indicate that a great many have found the task at hand to be too complex (a conclusion supported by our user survey), and that improvements had to be made to the user interface to attract more volunteers willing and able to participate regularly.45 The reliance upon Super Transcribers also suggests that to say Transcribe Bentham is crowdsourcing is a misnomer, as the project does not have thousands of active users carrying out small tasks. Transcribe Bentham might be better described as ‘crowd-sifting’: that 45.Causer et al., ‘Transcription Maximized’, 127, and 130–33, and Causer and Wallace, ‘Building a Volunteer Community’. is, beginning with the traditional open call associated with crowdsourcing, and then encouraging the emergence of a self-selecting, smaller number of individuals with the skills, desire and time to complete a complex task on a regular basis. We continue, as a result, to cast our net as wide as possible in the attempt to find more participants to join the cohort of Super Transcribers. Table 3.2 Contributions of Transcribe Bentham’s Super Transcribers, 8 September 2010 to 19 July 2013 Username  Began  Currently  Location  No. of  Average  participating  participating?  manuscripts  no. of mss  transcribed  worked on  (percentage of  per week  5,799)  (rounded)  Diane Folan  22 Sept 2010  Yes  UK  1,201 (20.7)  8  Carno  28 Dec 2010  No  US  89 (1.5)  7  Lidunn  28 Dec 2010  No  US  75 (1.3)  2  Mfoutz  29 Dec 2010  No  US  91 (1.6)  1  Clarabloomer  30 Dec 2010  No  US  71 (1.2)  8  RexL  31 Dec 2010  Yes  US  112 (1.9)  1  Lea Stern  4 Jan 2011  Yes  US  1,044 (18)  8  Jancopes  4 July 2011  Yes  US  201 (3.5)  2  Duyfken  1 Aug 2011  No  Australia  38 (0.7)  38a  Jillybean  15 Aug 2011  No  UK  106 (1.8)  4  Calico-pie  11 Sept 2011  No  France  37 (0.6)  3  Ohsoldgirl  11 Sept 2011  Yes  UK  320 (5.5)  4  Jfoxe  20 Sept 2011  Yes  UK  1,444 (24.9)  15  OlgaNM  9 Oct 2011  Yes  UK  69 (1.2)  1  Petergh  11 Nov 2011  Yes  UK  518 (8.9)  6  KeithThompson  21 Jan 2013  Yes  UK  90 (1.6)  4  Robmagin  29 May 2013  Yes  Canada  37 (0.6)  5  Note: aDuyfken transcribed 38 manuscripts over a two day period, accounting for the high weekly average. Time and Money Well Spent? Amajor concern about crowdsourced transcription is whether it is ultimately worth the effort. Would the time and money required to develop and deliver a platform, to recruit and manage volunteers and to check that crowdsourced submissions are of a sufficient standard not be better off invested in simply employing experts to do the job? For example, staff at the University of Iowa’s Civil War Diaries transcription project found that they spent a significant amount of time checking the work of volunteers, but did point out that this was not a prohibitive amount of time, and that the project as a whole would have been impossible without the cost-savings afforded by crowdsourcing. Sharon Leon (see also Chapter 4) likewise noted that while crowdsourcing ‘makes new kinds of work for existing staff’, engaging the public with otherwise underused resources is a worthwhile endeavour which could outweigh concerns over cost.46 In this section, we will discuss the time and effort required to ensure that volunteer-produced transcripts meet the required quality standards. In a paper published in 2012, we suggested that though Transcribe Bentham’s early results were encouraging and that the project facilitated engagement with an important resource and raised the profile of Bentham studies, checking submissions and managing the website was labour-intensive and time-consuming. During Period 1 (see earlier discussion), we found that the project’s two full-time Research Associates each spent the equivalent of a month’s full-time work checking submissions. Had they instead been employed to transcribe Bentham manuscripts on a full-time basis, they ‘could have transcribed about 5,000 manuscripts between them over twelve months, or two and-a-half times as many as the volunteers would have produced had they continued transcribing at the same rate’.47 (This calculation was made on the basis that volunteers then transcribed or partially transcribed an average of 35 manuscripts per week, whereas a full-time researcher could be expected to transcribe 40 to 50 manuscripts per week.) Based on these findings, David Weinberger was undoubtedly correct to suggest that: ‘For now…the results of the Benthamproject cannot be encouraging for those looking for a pragmatic way to generate high-quality transcriptions rapidly.’48 However, our finding is now out of date: Transcribe Bentham’s more recent results paint a much healthier picture. As mentioned above, at the end of Period 1 staff time spent on Transcribe Bentham was scaled back owing to the expiry of the AHRC grant. During Period 2, one Research Associate was responsible for Transcribe Bentham on a 0.4 Full-Time Equivalent (FTE) basis, that is, two days per week, one of which was spent on quality control. Despite the reduction in staffing, the transcription rate was higher than during Period 1: in Period 2, 3,403 manuscripts were transcribed or partially transcribed, at an average rate of 42 per week (see Table 3.3). At this rate, volunteers would produce 2,184 transcripts per year, whereas had the Research Associate devoted the two days per week he spent on Transcribe Bentham to transcription, he would produce only between 870 and 1,046 transcripts 46.Quoted in Zou, ‘Civil War Project Shows Pros and Cons of Crowdsourcing’. 47.Causer et al., ‘Transcription Maximized’, 130–31. 48.David Weinberger, ‘Crowdsourcing Transcription’. Table 3.3 Time spent on quality control process, 8 September 2010 to 19 July 2013 Period  Period length, Manuscripts days (weeks) worked on (total)  Average no. of manuscripts worked on, daily (weekly)  No. of days spent checking transcriptsa  Period 1: 8 Sept 2010–8 March 2011  182 (25)  1,009  6 (40)  50  Period 2: 9 March 2011–30 Sept 2012  572 (81)  3,403  6 (42)  81  Period 3: 1 Oct 2012–19 July 2013  291 (41)  1,305  4 (32)  17  Note: aAssuming one working day = 7.5 hours. a year.49 So, compared to Period 1, not only were manuscripts transcribed at a faster rate during Period 2, but the quality control process was more efficient. The amount of staff time spent checking transcripts for Periods 1 and 2 are, however, only estimates. From 1 October 2012, a Research Associate returned to working on Transcribe Bentham on a full-time basis,50 and since then the quality-control process has been measured in depth by recording several metrics. These are: the number of words transcribed, including and excluding TEI mark-up; the number of changes made to both the text and mark-up; the time spent checking each transcript before it was accepted; and the time spent correcting any errors in the mark-up after converting the transcript to an XML file using the oXygen XML Editor. From 1 October 2012 to 19 July 2013, 1,394 manuscripts were worked on, and a total of 1,305 transcripts (94 per cent) were successfully submitted by volunteers for checking, at a rate of 31 submissions per week. The rate of transcription slowed a little during this period, compared to Period 2, owing to two main factors: the Christmas and New Year break, and three Super Transcribers – including the second-most prolific participant –took part less frequently than in Period 2, owing to personal and work commitments. These findings cover a nine-and-a-half month 49.Assuming: 52 weeks of five working days, less UCL’s annual leave allowance of 27 days, and a further 15 days of public closures and bank holidays in 2013, at 0.4FTE = 87.2 days of transcription, at a rate of 10 to 12 transcripts per day. 50.Staffing was modified slightly from 1 March 2013: another member of staff began working on Transcribe Bentham on a 0.2 FTE basis, and the original Research Associate now works on a 0.8 FTE basis. period, and though they might be regarded as provisional there are enough data to suggest reasonably that the trends identified below should continue. Fifty-one individual volunteers submitted transcripts during Period 3, having collectively transcribed 419,464 words, or an average of 321 words per transcript not including mark-up; taking the mark-up into account, volunteers submitted a total of 588,203 words, or an average of 450 words per transcript. That the mark-up increases the number of words transcribed by almost 30 per cent is clear evidence that encoding is no small task. Table 3.4 Summary of quality control process, 1 October 2012 to 19 July 2013 No. of individual submitters  No. of words, (incl. mark-up)  No. of alterations made to text  No. of alterations made to mark.up  Time spent checking transcripts (seconds)  Time spent correcting errors in XML file (seconds)  51  588,203  5,262  10,406  464,463  6,391  Transcribe Bentham’s Research Associates spent the equivalent of 129 hours and 6 minutes’ worth of labour checking submissions during Period 3, plus another 1 hour and 46 minutes correcting any errors which remained after converting the transcripts to XML files. On average, a given transcript took 357 seconds (5 minutes 57 seconds) to check, though there is, of course, great variation depending upon the length and complexity of the manuscript, the experience of the transcriber and the consistency of the mark-up. For example, the transcript of JB/051/271/00351 comprised of 95 words (including mark-up) and took a mere 52 seconds to check: Bentham’s handwriting is legible, the manuscript layout is unremarkable and the transcript was excellent. No changes were required to either text or mark-up. At the other extreme, 1,931 seconds (32 minutes 11 seconds) were spent checking JB/050/135/001 (see Figure 3.6),52 with 35 changes made to both text and mark-up. This was not because the transcript was of a poor quality; far from it. Rather, it was because the transcript was some 1,269 words long (including mark.up), and the manuscript is difficult to navigate owing to the numerous deletions, nested interlineal additions and multiple marginal annotations. After the transcript was converted to XML, it then took a further 14 minutes to locate a line-break 51.http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/051/271/003, revision dated 16.33, October 1, 2012. 52.http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/050/135/001, revision dated 15.37, October 15, 2012. which had been mis-typed (not by the volunteer, we hasten to add), resulting in oXygen XML Editor refusing to validate the file until the error had been located. However, even in an extreme case like this, it is still far quicker for us to check a transcript than transcribe it ourselves. That a transcript can generally be checked in less than 6 minutes is extremely encouraging (see Figure 3.7): it is a rare Bentham manuscript that we could Note: Data were unavailable for six transcripts, owing to a software crash. transcribe – let alone encode – in that space of time. However, about half the checking time is generally expended on ensuring that the XML mark-up is both well formed and valid. That such a relatively mundane and mechanical task can take such a disproportionate amount of time is less encouraging for Transcribe Bentham’s long-term cost-effectiveness, though the upgraded transcription interface (discussed above) should help to address this problem. In summary, the quality-control process is now much more efficient than before. Quality: Just How Good Is the Work of Volunteer Transcribers? Another regular concern about crowdsourced transcription is whether volunteers’ submissions can be of a reliably high standard in comparison to the work of experts. In a project like Transcribe Bentham, where the material is frequently complex in both content and composition, and the crowdsourced results are used for public access and editorial purposes, some form of manual quality-control process is essential. Automated comparisons of transcripts would be difficult, and in the case of a lengthy, complex manuscript – such as JB/070/231/00153 – it would be a waste of volunteers’ time and effort to ask several of them to transcribe the same one, and of staff time to check and compare multiple variants. We know that the vast majority of work done by Transcribe Bentham volunteers is of an extremely high quality, as 95 per cent of all transcripts have been approved by researchers experienced in reading Bentham’s manuscripts. However, this figure does not illustrate what exactly happens to submissions during the quality-control process, and what alterations are required before a transcript is accepted. Gaining an accurate representation of the manuscript text is the key task, and in this volunteer transcribers have proven more than able. Of the 1,305 transcripts submitted in Period 3, few required substantial changes to the text: 522 (40 per cent) were approved without alteration, and only 184 (14 per cent) needed eight or more changes before acceptance (see Table 3.5 and Figure 3.8).54 Transcripts requiring extensive alterations to the text were few and far between: JB/002/567/001 required 69 changes because the marginal notes were not transcribed, and JB/100/001/001 required 213 alterations to the text, as the bottom-left panel of the manuscript was 53.http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/070/231/001, revision dated 14.39, April 23, 2013. 54.Achange to the text is defined as: entering one untranscribed word; correcting one incorrectly transcribed word; moving an incorrectly placed word/portion of text. A change to the mark-up is defined as: addition/deletion of one piece of mark-up which only requires a single tag (e.g. <lb/>); addition/deletion of all or part of an opening and closing pair of tags (e.g. <p></p>). untranscribed upon submission.55 On average each submission received during Period 3 required only four alterations to the transcript’s text before being approved. The TEI mark-up, by comparison, causes more work for volunteers and staff: the 1,305 transcripts submitted during Period 3 required, on average, eight changes to the encoding. Only 313 (24 per cent) were approved without any modification of the mark-up, and 191 (15 per cent) needed only one change. A substantial number – 307 transcripts (24 per cent) required 10 or more alterations, and a disproportionate amount of time was spent on these lengthier submissions: 57 hours and 37 minutes – 45 per cent of all the time spent on moderation during Period 3 – was spent checking these 307 transcripts. It would be fair to say that for volunteers, the XML mark-up complicates participation, and it has undoubtedly dissuaded many from participating more fully, or at all.56 This conclusion should not detract from the efforts of Super Transcribers who have proven adept at text encoding, even though most had no experience of it prior to taking part in Transcribe Bentham. Rather, it is a warning Table 3.5 Editorial intervention in manuscripts submitted between 1 October 2012 and 19 July 2013 No. of words  No. of transcripts  Average no. of changes to text  Average no. of changes to mark-up  Average time checking (seconds)  0  12  0  0  <1  1 to 25  24  <1  <1  16  26 to 50  6  0  <1  99  51 to 100  31  2  3  92  101 to 200  168  1  3  128  201 to 500  693  3  7  274  501 to 750  191  6  9  488  751 to 1000  92  7  14  645  1001 to 2000  79  12  25  1,053  2001 to 2999  8  16  19  1,807  3000+  1  66  3  729  Total  1,305  4  8  357  55 http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/002/567/001, revision dated 14.52, October 22, 2012, and http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/100/001/001, revision dated 12.40, December 20, 2012. 56.See Causer and Wallace, ‘Building a Volunteer Community’for volunteer opinions about the transcription process. that though crowdsourcing projects should not underestimate the capabilities of their audiences, nor should they test their volunteers’ patience; the task at hand should be simplified as far as possible. But why do some transcripts require more editorial intervention than others? First, the experience of the volunteer in question matters a great deal. The first few attempts at transcription are the hardest: acquiring an eye for Bentham’s hand comes with timeand practice (as Bentham Project researchers know only too well). During Period 3, 38 new volunteers transcribed a total of 188 manuscripts, which required on average 10 changes each to the text, and 20 alterations to the mark.up, before the transcripts were approved. By comparison, a Super Transcriber transcript requires, on average, four changes to the text, and seven to the mark-up. The hand in which a manuscript was written makes a significant difference to the extent of editorial intervention. The majority of the Bentham Papers are written by Bentham, but substantial portions of the collection are neat, fair copies in the hands of secretaries and copyists, as well as portions of printed text (such as Acts of Parliament or newspaper clippings) which may or may not have been annotated by Bentham.57 Manuscripts written by Bentham generally take more time to assess, Table 3.6 Quality control of submitted transcripts, 1 October 2012 to 19 July 2013a In whose hand? Average no. No. of Average no. Average no. Average of words (incl. transcripts of changes of changes time mark-up) to text to mark-up checking (seconds) Bentham  492  873  5  7  421  Bentham mostly, 740 some by copyist  6  9  19  824  Copyist  335  343  3  9  207  Copyist mostly, some by Bentham  449  31  2  19  391  Printed text  581  29  3  10  222  Printed text, annotation by Bentham  635  12  <1  12  312  Notes: aTen submitted transcripts were of blank sheets. bThat manuscripts in the hands of copyists require an average of nine alterations to the mark-up before acceptance appears to be owing to the repeated misplacement of line-break tags. 57.For examples of manuscripts in the hand of a copyist, see http://www.transcribe.bentham.da.ulcc.ac.uk/td/Category:Box_107, folios 279–343, and of printed text, see http:// www.transcribe-bentham.da.ulcc.ac.uk/td/Category:Box_116, folios 650 to 652 (both last accessed February 21, 2013). usually contain more text and are often laid out in a less straightforward manner than the fair-copy manuscripts (see Table 3.5). Finally, and unsurprisingly, the length of the document impacts greatly upon how long it takes to check. Transcripts submitted during Period 3 were an average of 450 words in length including mark-up, though 456 were of above average length. The above-average length transcripts, generally in Bentham’s hand, required an average of seven changes to the text and 12 to the mark-up, and an average of 612 seconds (10 minutes 12 seconds) to check). By way of example, the transcript of JB/095/118/001 was 2,009 words long, difficult to follow, and required 32 minutes and 20 seconds to check.58 On the other hand, transcripts of up to 750 words in length are generally checked in 4 minutes and 34 seconds or less (see Table 3.6). Conclusion: The Worth of Crowdsourcing? The amount of work put into the initiative by Transcribe Bentham volunteers should not be underestimated: they have braved a new experience in learning to read and transcribe Jeremy Bentham’s handwriting, while at the same time adding TEI mark-up to their work. Transcribing Bentham is far from an easy task and we are extremely grateful to all of those who have committed to spending time with Bentham’s manuscripts, in order to contribute to scholarship and widen access to the material. The success of Transcribe Bentham is owed fundamentally to their work, and is not taken for granted. The first six months were undoubtedly the hardest for Transcribe Bentham (as they would be for any crowdsourcing project attempting to establish itself) and especially so as the material at hand is not necessarily well known or of immediate and popular appeal. Not only did volunteer transcribers experiment with a new system, but so, to an extent, did we: workflows for checking transcripts and maintaining the website had to be fully established, and as a result of this experimentation, the quality-control process proved more time-consuming than it otherwise might have been. Whilst a certain amount of time will always be spent checking submissions, this has been reduced dramatically as processes became easier and more familiar. There is also the possibility of recruiting experienced volunteers to moderate submissions as the project continues or if more volunteers take part regularly, but so far, the moderator/volunteer relationship has coped with the number and quality of submissions received. With a supply of new material to explore, an upgraded transcription interface which addresses the concerns of volunteers, and a core of Super Transcribers, Transcribe Bentham is now at the stage where it can fulfil its potential, and the initiative’s potential impact on the work of the Bentham Project (and, by extension, 58.http://www.transcribe-bentham.da.ulcc.ac.uk/td/JB/095/118/001, revision dated 11.38, December 18, 2012. other institutions wishing to crowdsource transcription) can be starkly indicated. As mentioned earlier in this chapter, the UCL Bentham Papers consists of 60,000 manuscript folios, while the British Library holds another 12,500 folios. Without entering into a detailed discussion about how many of the manuscripts are folia and how many are bifolia, we estimate that the combined collection will require about 100,000 transcripts before it is fully transcribed. Between 1984 and 2010, 20,000 folios were transcribed by Bentham Project staff – some 28,000 transcripts – at a rate of 1,076 per year, owing to the availability (or otherwise) of funding for editorial work. Assuming that sufficient funding was available for transcription to continue at this rate, the remainder of the UCL and British Library collections would not be fully transcribed for another 67 years. Between 8 September 2010 and 19 July 2013, Transcribe Bentham volunteers worked at a rate of 2,024 transcripts per year. At this pace, the remainder of the collection would be completely transcribed by 2049 which, though some way off, is considerably faster than had Transcribe Bentham never existed. However, if the upgraded Transcription Desk and ongoing publicity campaign can recruit enough volunteers to produce between 75 and 100 transcripts per week (c. 3,900 to 5,200 per year), in this hypothetical scenario, the remainder of the collection could be transcribed in between 12 and 16 years. This still seems distant, but with continued support and volunteer effort, it is entirely possible that full digital access to all of Bentham’s manuscripts and their transcripts could be provided within two decades, a prospect which was unthinkable just a few years ago. Significant amounts of money – some £598,830 – have been invested in Transcribe Bentham by the AHRC and the Mellon Foundation, primarily on digitisation, software development and staff salaries, and it is only right to question whether this expenditure is worthwhile.59 In the short term, Transcribe Bentham has produced nearly 6,000 transcripts in almost three years for this return, which does not sound all that impressive. Yet in the long run – and even taking into account a certain level of staff support to provide quality control, maintain the website and support volunteers – we estimate that should the remainder of the remaining 70,000 transcripts required to complete the collection be produced by volunteers, the Bentham Project could avoid staff costs of around £1,000,000, which will more than cover the investment in the initiative. Moreover, this does not take into account the incalculable public engagement value of Transcribe Bentham, and the creation of a hugely important searchable digital archive of Bentham’s manuscripts. Some institutions might consider outsourcing transcription, but for the Bentham Papers this is not a practical option, primarily because no organisation would ever give the BenthamProject sufficient funding to contract out the transcription of the tens of thousands of remaining untranscribed manuscripts: owing to their fragility and importance, the manuscripts themselves could not be sent offshore, and would still require digitisation (almost a third of the £598,830 invested in Transcribe 59.Robinson, ‘Why Digital Humanists Should Get Out of Textual Scholarship’. Bentham will be spent on digitisation). Given the difficulty in deciphering Bentham’s manuscripts, the likelihood of the results being satisfactory without Transcribe-Bentham-style staff support and quality control, is rather doubtful. It is difficult to overstate just how important and helpful it will be to the production of the Collected Works to have the content available in a digital, searchable format, providing an overview of the Bentham Papers which we have never had before. Nearly three million words have been transcribed by volunteers, providing us with a growing digital resource to build and experiment with, and it now seems time to consider using text visualisation techniques, intelligent search interfaces and text analysis to make full use of the richly encoded data created by volunteers, and discover new and insightful ways to explore the ever-growing corpus. Transcribe Bentham would, we hope, have met with Bentham’s approbation through the initiative’s efforts to democratise the creation of, and access to, knowledge and humanities research, and its use of modern technology to enable the task to be completed in as efficient and timely a way as possible. ‘Many hands make light work’, wrote Bentham in 1783, but ‘many hands together make merry work’. Transcribe Bentham continues to prove the truth of this particular maxim. Acknowledgements We are grateful to Kris Grint, Dr Michael Quinn and Professor Philip Schofield for comments on earlier drafts of this chapter, and to Mia for her endless patience. We wish to acknowledge the outstanding work of our Transcribe Bentham colleagues: Professor Philip Schofield (Principal Investigator), Kris Grint and Anna-Maria Sichani (UCL Bentham Project); Martin Moyle and Lesley Pitman (UCL Library Services); Tony Slade, Raheel Nabi, Alejandro Salinas Lopez and Miguel Faleiro Rodrigues (UCLCreative Media Services); Richard Davis, José Martin and Ben Parish (University of London Computer Centre); and Dr Arnold Hunt (British Library). Very special thanks are owed to Dr Justin Tonra and Dr Valerie Wallace, both formerly of the Bentham Project, and now respectively of the National University of Ireland, Galway, and Victoria University of Wellington. We would also like to acknowledge the support of colleagues at UCL Special Collections. Finally, we are indebted to Transcribe Bentham’s volunteers, without whom none of this would have been possible. Funding We would like to express our gratitude to the Arts and Humanities Research Council, and the Andrew W. Mellon Foundation, for providing the funding which made this research possible, and in particular to the Mellon Foundation for their ongoing support. In regard to tranScriptorium, the research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007– 2013) under grant agreement number 600707 – tranScriptorium. References Bentham, Jeremy. Auto-Icon and Last Will and Testament. Edited by Robert A. Fenn. Toronto: privately printed, 1992. Bentham, Jeremy. Not Paul, but Jesus. Edited by Philip Schofield, Catherine Pease-Watkin and Michael Quinn, 2013. http://www.ucl.ac.uk/Bentham-Project/ publications/npbj/npbj.html (last accessed May 1, 2013). Bentham, Jeremy, The Works of Jeremy Bentham, published under the superintendence of his executor, John Bowring, vol. 4. Edinburgh: William Tait, 1838–43. Referred to as Bowring in the footnotes. Causer, Tim and Valerie Wallace. ‘Building a Volunteer Community: Results and Findings from Transcribe Bentham’. Digital Humanities Quarterly 6, no. 2 (2012). http://www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html (last accessed February 6, 2013). Causer, Tim and Melissa Terras. ‘Crowdsourcing Bentham: Beyond the Traditional Boundaries of Academic History’. International Journal of Humanities and Arts Computing 8, no.1 (2014): 46–64. Pre-publication version available from http://discovery.ucl.ac.uk/1354678/). Causer, Tim, Justin Tonra and Valerie Wallace. ‘Transcription Maximized; Expense Minimized? Crowdsourcing and Editing The Collected Works of Jeremy Bentham’. Literary and Linguistic Computing 27, no. 2 (2012): 119–37. Cohen, Patricia. ‘Scholars Recruit Public for Project’. New York Times, December 27, 2010. http://www.nytimes.com/2010/12/28/books/28transcribe. html?pagewanted=all&_r=0 (last accessed July 26, 2013). Kinchen, Rosie. ‘One Stir, Then I’ll Discover a Galaxy’. Sunday Times, September 11, 2011. http://www.thesundaytimes.co.uk/sto/newsreview/features/article772703. ece (last accessed February 20, 2013) (paywall). Marmoy, C.F.A. ‘The “Auto-Icon” of Jeremy Bentham at University College London’. Medical History 11, no. 2 (1958): 77–86. Milne, A. Taylor, ed. Catalogue of the Manuscripts of Jeremy Bentham, in the Library of University College. 2nd Edn. London: Athlone Press, 1962. O’Sullivan, Luke and Catherine Fuller, eds. The Correspondence of Jeremy Bentham: Volume 12 (Collected Works of Jeremy Bentham). Oxford: OUP, 2006. Pitkin, H.F. ‘Slippery Bentham: Some Neglected Cracks in the Foundation of Utilitarianism’. Political Theory 18, no. 1 (1990): 104–31. Quinn, Michael, ed. Writings on the Poor Laws: Volume 1 (Collected Works of Jeremy Bentham). Oxford: OUP, 2001. Quinn, Michael, ed. Writings on the Poor Laws: Volume 2 (Collected Works of Jeremy Bentham). Oxford: OUP, 2010. Robinson, Peter. ‘Why Digital Humanists Should Get Out of Textual Scholarship’. Social, Digital, Scholarly Editing conference, University of Sasketchewan, July 11–13, 2013. http://www.academia.edu/4124828/SDSE_2013_why_digital_ humanists_should_get_out_of_textual_scholarship(last accessed July 30, 2013). Schofield, Philip. Bentham: A Guide for the Perplexed.London: Continuum, 2009. Schofield, Philip. Jeremy Bentham: Prophet of Secularism. London, 2012. http:// discovery.ucl.ac.uk/1370228/ (last accessed March 22, 2013). Schofield, Philip, Catherine Pease-Watkin and Cyprian Blamires, eds. Rights, Representation and Reform: Nonsense upon Stilts and Other Writings on the French Revolution (Collected Works of Jeremy Bentham). Oxford: OUP, 2002. Schofield, Philip, Catherine Pease-Watkin and Michael Quinn. Of Sexual Irregularities, and Other Writings on Sexual Morality (Collected Works of Jeremy Bentham). Oxford: OUP, 2014. Semple, Janet. Bentham’s Prison: A Study of the Panopticon Penitentiary.Oxford: OUP, 1993. Stephen, Leslie. The English Utilitarians: Volume I – Jeremy Bentham. London: Duckworth and Co., 1900. Weinberger, David. ‘Crowdsourcing Transcription’. Too Big to Know blog. http:// www.toobigtoknow.com/2012/09/04/2b2k-crowdsourcing-transcription-2/ (last accessed February 20, 2013). Zou, Jie Jenny. ‘Civil War Project Shows Pros and Cons of Crowdsourcing’. Chronicle of Higher Education, June 14, 2011 (updated June 21, 2011). http:// chronicle.com/blogs/wiredcampus/civil-war-project-shows-pros-and-cons-of.crowdsourcing/31749 (last accessed February 7, 2013). Chapter 4 Build, Analyse and Generalise: Community Transcription of the Papers of the War Department and the Development of Scripto Sharon M. Leon On the night of 8 November 1800, fire devastated the United States War Office, consuming the papers, records and books stored there. Two weeks later, Secretary of War Samuel Dexter lamented in a letter that ‘All the papers in my office [have] been destroyed’. From the perspective of historians, the loss was monumental since in many respects the documents lost in the fire constituted the first ‘national archive’ of the United States. The Papers of the War Department, 1784–1800 (PWD, wardepartmentpapers.org) is an ongoing digital editorial project at the Roy Rosenzweig Center for History and New Media (RRCHNM) (chnm.gmu.edu) that encourages historical scholarship of this lost period in the Early American Republic by restoring the archive online and making the entire collection accessible and fully searchable to a wide audience of scholars, students, teachers and the general public (see Figure 4.1). This website officially debuted in June 2008 with nearly 45,000 document images and basic metadata. Unfortunately, no realistic prospects existed to fund the transcription of this unique digital archive. These circumstances presented an opportunity for innovation in digital humanities work and software. With the support of the National Endowment for the Humanities Office of Digital Humanities (NEH-ODH) and the National Archives and Records Administration’s National Historical Publications and Records Commission (NHPRC), the team at RRCHNM devised a system to allow the existing user community to begin transcribing the materials from PWD. At the time that we launched the transcription project in March 2011, we knew very little about our user base. Over the course of the project, we have learned a great deal about those initial users and the many, many individuals attracted to the collection by the opportunity to transcribe. Together those users came to transcription with six areas of interest and motivation for their work: (1) a general interest in early American history; (2) a sense of civic duty; (3) a specific point of scholarly research; (4) genealogical and family history questions; (5) various educational assignments; and (6) a curiosity about how the transcription tool and process worked. Based on the lessons learned through developing the community transcription tool and working with our PWD volunteers, we generalised the software into an open-source tool. Scripto (scripto.org) is available for use by other projects through a customisable version or through extensions for a range of popular content management systems. RRCHNM’s foray into community transcription with PWD and the development of Scripto offers some significant lessons for cultural heritage institutions and professionals who want to engage with their constituents in meaningful ways. Primarily, we gained a dedicated and engaged audience for PWD, and a tremendous insight into theirmotivations. Equally important, the development process for the generalised tool, and its role in the larger ecosystem of open-source software that enables widespread user participation in cultural heritage projects, points to viable directions for the development of subsequent tools. Together the case study of PWD and the story of the creation of Scripto suggest that a wide range of cultural heritage organisations can launch and sustain lightweight transcription projects that encourage increased engagement with core audiences. Papers of the War Department, 1784–1800 The Papers of the War Department, 1784–1800 has never been a traditional documentary edition project. Decades in the making, the work on the collection began before many scholars were aware of the world wide web and certainly before the majority had even begun to consider the ways that the internet would change our relationship to research methods, content access and engagement with the larger public. The conditions under which the archive was reconstituted – assembling a collection of photocopies and high-resolution scanned document copies – allowed for an experimental approach. While the original documents resided in archives and special collections around the world, PWD itself has no original holdings. With a completely virtual collection, RRCHNM had the opportunity to err on the side of immediate and open access, making the digital copies of the documents directly available to the public via the web. At the dawn of the project, documentary collections and editions were the purview of academic researchers with access to well-funded research libraries. In 1989 when Ted Crackel, the first Editor-in-Chief for the project, proposed the idea of reconstituting PWD, he initially thought in terms of a traditional print edition.1 In 1993, Crackel began the initial planning to undertake the project himself with the support of East Stroudsburg University in Pennsylvania. With funding from the NHPRC and eventually the Department of Defense Legacy Project, beginning in 1994 Crackel and his staff visited over 200 repositories and consulted over 3,000 collections in the United States, Canada, England, France and Scotland, copying, scanning and processing nearly 50,000 documents. At this tremendous volume, Crackel soon realised that a print publication would be unreasonable, and turned his sights towards producing a CD-Rom that would include the document images. In 2004, when Crackel accepted the position as Editor-in-Chief of the George Washington Papers, PWD faced a crisis point: without Crackel at the helm, the project lacked the professional support and leadership to continue at East Stroudsburg. Responding to a call from the NHPRC and at Crackel’s urging, RRCHNM applied to adopt the project in the summer of 2005, and began work on producing the digital documentary edition in spring 2006 under the direction of Editor-in-Chief Christopher Hamner. RRCHNM built a website with nearly 45,000 documents that launched in June 2007. The archive contains materials ranging from several years before to several years after the heart of the materials, thus spanning 1781 to 1803. This includes 42,887 documents with scanned images and 2,482 additional citations for documents that do not have the accompanying image due to some rights or permissions issue. Moreover, the database includes listings for nearly 4,180 people or groups who were listed as sender or recipient of a document or were explicitly mentioned therein. Given the scope of materials and seeking to be realistic about the resources available to process the collection, the editorial team Crackel, ‘The Common Defence’. had to balance their approach to describing the materials. On the advice of Max Evans, then the Director of the NHPRC, the editors opted initially to index the documents with basic metadata about author, recipient and date. With this as their charge, Editor-in-Chief Hamner, Assistant Editor Ron Martin and nearly a dozen graduate research assistants from George Mason University’s History Department systematically reviewed and described the papers. By 2010, these basic metadata made the documents searchable to the extent that if a researcher knew what she was looking for, she could extract it from the corpus. For researchers with less concrete demands, the index proved less usable. Routes to access materials on a range of subjects in the history of the Early American Republic, such as the handling of Indian affairs, pensions, procurement, the relationship of the first American citizens with the new federal government, and conflicts including the Whiskey Rebellion and the Quasi-War with France, were extremely difficult to navigate. As a result, in the subsequent years, the editors and assistant editors on the project have added an additional layer of metadata – names, places, things, ideas mentioned and a brief abstract – for the most significant elements of the collection, roughly one-third of the documents. When the funding for the project came to a close in June 2013, the team of editors at RRCHNM had produced this two-tiered level of description for the entire collection. Looking for Precedents for Community Transcription While the staff at PWD would never have the capacity to transcribe a significant number of the documents in the archive, steady site traffic suggested that we had an untapped resource of scholars and researchers who could help with this task. Every day researchers examined documents in the archive, regularly making rough transcriptions to use in their own work. In the years before we launched the transcription project, the editors routinely got emails from researchers suggesting improvements to the archive’s metadata based on their work with the materials. Thus, we proposed to build an open-source transcription tool to allow users easily to submit those transcriptions and their knowledge back to the archive. The resultant tool would allow PWD, and eventually other digital archival projects, to draw upon the wisdom of the thousands of interested researchers, scholars and students who work with these materials. Gradually, users’ combined work would enhance the discoverability and usefulness of the archive without significantly adding to the costs of the project. We envisioned this tool as a response to Max Evan’s 2007 call for commons-based peer production as a way to create ‘Archives of the People, by the People, for the People’, where he points to ‘the concept of commons-based peer-production as a means of turning collections inside out. It encourages archival institutions to reinvent themselves, and, in collaboration with other archives and with other types of organizations, to organize archival work in concert with a curious and interested public’.2 Evans’ approach to openness and user engagement matched nicely with the philosophy of public history that undergirds all of RRCHNM’s work. Moreover, Evans was not alone in his vision for participatory archives. J. Gordon Daines and Cory L. Nimer’s ‘The Interactive Archivist: Case Studies in Utilizing Web 2.0 to Improve the Archival Experience’published by the Society of American Archivists in 2009, provided a useful summary of the interests of the archivists in social networking and the usefulness of tagging, commenting, reviewing and rating services, but did not mention strategies for incorporating users into the archival process at the transcription or description level that remains the domain of archivists. These pieces suggested that the team at RRCHNM was at the leading edge of an emerging push to encourage more significant user engagement in archival and documentary editing projects. While key individuals in the archival profession and the documentary editing world spoke directly to the needs of our papers project, in proposing to open the transcription process up to crowdsourcing, RRCHNM also drew upon the example and success of a host of successful ventures in the wider realm of digital culture. Though the goals and needs of digital archive and documentary editing projects are distinct from the goals of these community-driven ventures, they did offer promising glimpses of the efficiencies and outcomes for our purposes.3 Each tapped into the interests and passions of a segment of the public who contributed to the accumulated value and content of a project –whether software development or knowledge aggregation. The open-source software movement – with successes like the Linux operating system and Mozilla’s popular Firefox web browser – contains lessons for those of us interested in harnessing the expertise of a particular community to build a successful project. The developers who participate in open-source software projects do so for their own reasons, but they contribute to the common good by applying their expertise to the demands and problems raised by software innovation. There were also several content-focused examples of crowdsourcing that were particularly revealing. Wikipedia, the free online encyclopaedia that is written and maintained by users around the world, is by far the most well-known instance of successful crowdsourcing. Launched in 2001, using a simple authoring and versioning software to manage articles, Wikipedia thrives on the contribution of thousands of anonymous users. As of July 2013, the site had more than 77,000 active contributors who created and edited over 22 million articles in 285 languages – more than four million of which are in English.4 From its founding, teachers, parents and scholars have worried about the accuracy and content of Wikipedia, especially since articles from the free encyclopaedia have long been the first result to appear in most search engines. Yet, the source of the occasional errors and incoherence of Wikipedia is also its power – the tremendously fast-moving contributions and 2.Evans, ‘Archives of the People’, 387. 3.The term ‘crowdsourcing’ was coined by Jeff Howe in ‘The Rise of Crowds’. 4.‘About’, Wikipedia http://en.wikipedia.org/wiki/About_Wikipedia. collaboration of users means that errors are unlikely to remain for long before they are corrected. Similarly, the growth of articles and topics covered in the free encyclopaedia reflects the energy and interests of the open-source community of users. An assessment of the creation and revision history of articles on historical topics could provide a tremendous wealth of information about the concerns of the public with the past.5 In a similar, albeit much less extensive way, Flickr Commons provided another example of tapping the collectiveknowledge of interested members of the public.6 Beginning with a seed contribution by the Library of Congress in January 2008, cultural institutions from around the world, including the New York Public Library, the George Eastman museum, the Powerhouse Museum in Australia and the Smithsonian Institution, contributed images and associated metadata to the Flickr Commons collection. The collections are freely available to the public, who can tag and comment on each image. In October 2008, the Library of Congress assessed their participation in the pilot project, reporting that over 2,500 Flickr users had left more than 7,000 comments on almost 3,000 images. Library staff selected important corrections and additions to captions, titles and the identification of individuals that were then incorporated back into the metadata of more than 500 items by August 2008. Based on these positive interactions with the public, the staff advocated that the library continue to draw upon public knowledge through the Flickr Commons project and other Web 2.0 interactive projects, declaring: ‘The benefits appear to far outweigh the costs and risks.’The Library of Congress’ success helped to encourage the Smithsonian Institution to join Flickr Commons, and as a result, in the period between June and December 2008, their photographic contributions received over 625,000 views with many user comments and tags. Thus, trusted cultural heritage organisations were beginning to recognise the powerful ways that interested members of the public, scholars and educators can contribute to the knowledge base related to collections.7 Additionally, Zooniverse has supported a large number of ‘citizen science’ projects in the last several years. Most focus on the crowdsourcing of big data, such as the Galaxy Zoo identification projects, but others have a more historical focus. The Old Weather project (see also Chapter 2 in this volume) is helping scientists recover weather observations included in the logbooks of Royal Navy vessels during the First World War era. The results of this work help climate scientists build better models and provide historians with access to new data about the ships and their sailors. Similarly, the Ancient Lives project has made the fragmentary Greek texts, the Ocyrhynchus Papyri, available for transcription. This work will facilitate the identification of known texts and the isolation of new ones, in turn 5.Rosenzweig, ‘Can History Be Open Source?’. 6.Flickr Commons, http://flickr.com/commons/. 7.Springer et al., ‘For the Common Good’, 36; Kalfatovic et al., ‘Smithsonian Team Flickr’. contributing to a greater understanding of Greco-Roman Egypt. Zooniverse has released the code for their generalised transcription tool, Scribe.8 At the same time, a number of universities and national libraries began running several very successful crowdsourcing projects. First, in August 2008 the National Library of Australia started a project where members of the public corrected the results of Optical Character Recognition (OCR) software for their digitised newspapers. By then-project manager Rose Holley’s estimates, there were roughly 6,000 participants who had corrected over seven million lines of text by November 2009.9 That project then became part of the Trove project, which allows users not only to correct OCR, but also to contribute historic images, tag materials and link a host of other cultural materials.10 As of June 2013, Trove provided users with access to nearly 350 million digital items. The results of these National Library of Australia projects show the remarkable range of contributions that public users can make to historical material, especially when they have the capacity to work across large collections. Second, university and public libraries have also had good success with projects that ask users to participate in direct transcription of document images.11 Of these, the University College London’s Transcribe Bentham project is probably the most well known (Chapter 3 in this volume).12In the course of its public work, the project has allowed for the transcription of close to 2,000 manuscripts from Jeremy Bentham’s published and unpublished works. The transcriptions will form the foundation for future work on the Collected Works of Jeremy Bentham. MediaWiki is the system that underlies the Transcribe Bentham transcription work, and the project team has released the code for its MediaWiki plugins.13 This code does not allow MediaWiki to interact with existing content management systems. The project team at University College London, who began work on Transcribe Bentham in April 2010 and launched the site in September 2010, has offered ample insights from their work about the ways that it enhanced their relationship to various interest communities, even if it did not necessarily speed the process of transcription or reduce the cost.14 Their results point to a primary good of opening 8.Zooniverse, http://www.zooniverse.org/; Old Weather project, http://www. oldweather.org/; Ancient Lives project, http://ancientlives.org/; Scribe, https://github.com/ zooniverse/Scribe. 9.Holley, ‘Crowdsourcing’. 10 Trove, http://trove.nla.gov.au/. 11.See also Ben Brumfield’s ‘FromThePage’, http://beta.fromthepage.com/?ol=l_hd_ logo; New York Public Library’s ‘What’s on the menu?’, http://menus.nypl.org/; and the University of Iowa Libraries’‘Civil War Diaries and Letters Transcription Project’, http:// digital.lib.uiowa.edu/cwd/transcripts.html. 12 Transcribe Bentham, http://www.ucl.ac.uk/transcribe-bentham/. 13 Transcribe Bentham MediaWiki Transcription Desk toolbar, http://code.google. com/p/tb-transcription-desk/. 14.Causer et al., ‘Transcription Maximized’. community transcription work: building dedicated user communities for digital cultural heritage projects. All of these projects suggest thatcommunity-sourcing transcription for digital collections can provide significant benefits to cultural heritage institutions. First, and foremost, public contributions provide transcriptions where there once were none, and where there likely would be none in the future. Second, in the case of documentary editing projects, staff can draw on the publicly contributed transcriptions to form a base for their editorial work. Editors may start with a rough transcription provided by interested users, and then apply the techniques and expertise of their training to produce a corrected transcription quickly. Moreover, allowing the public to contribute document transcriptions to digital collections has real-time benefits for the accessibility of the materials. Each transcription contributed to the archive can then be made available to the collection management system’s search engine. The result is ever-improving discoverability. The full text of the documents allows the search engines to surface the most relevant documents by better weighting their results. Furthermore, as more technically astute scholars increase their reliance on computational text analysis, legions of documents with digital images that lack transcriptions will be off limits to their examination and processing. Growing the field of transcribed archival materials can only benefit humanities scholars. Community-contributed transcriptions also allow editors to understand better the ways in which some users interact with their archive. In a collection where the volunteers can select any document for transcription, often the documents that are of the most interest to users will be transcribed most quickly and fully. Thus, the public contributions can serve as a barometer of the most interesting materials within a particular collection. This convergence of volunteer interest and the collection coverage points to perhaps the most important reason for launching collaborative work with the public: community building. As Trevor Owens has noted, the concept of crowdsourcing and the related turn towards gamification, can seem like an effort by projects to take advantage of public contributions simply as a free labour force. But, transcription volunteers make the contributions that they do because they find the work meaningful.15 They are contributing to the usefulness of the collection and learning about history at the same time. This type of community building around collections increases investment in cultural heritage and points to long-term gains for the humanities. Implementing Scripto with the Papers of the War Department, 1784–1800 Given the positive outcomes from these early community-sourcing and open-source projects, RRCHNM decided in 2009 to apply for support from both the NEH.ODH and the NHPRC to design and build an open-source tool that would enable 15.Owens, ‘Meanification and Crowdscaffolding’. the community sourcing of transcription.16 The result of those applications was a Digital Humanities Start-Up Grant from NEH-ODH to build the basic tool and implement it with the Papers of the War Department, and a grant from the NHPRC to generalise the tool, do user testing and develop support and documentation so that other projects could launch community transcription projects. Together, these two grants enabled RRCHNM to build and refine Scripto, offering it as a customisable software library connecting a repository to an editing interface, and as extensions for three popular web-based content management systems (Omeka, omeka.org; Drupal, drupal.org; and WordPress, wordpress.org). The first step in designing the transcription tool for the Papers of the War Department was to map PWD’s idiosyncratic data model to Scripto. To do this, Jim Safley (RRCHNM Digital Archivist and Web Developer) wrote a small software function implementing Scripto’s adapter interface that responded to requests by the transcription service. He then linked the existing PWD website to a custom web application containing a document image viewer and input forms for transcription and discussion. Safley chose to use the software library OpenLayers as the image viewer because of its ability to render high-resolution image files directly in the web browser without the need for those files to be converted into another format for viewing.17 Safley purposefully developed a very limited feature set for Scripto so that it could easily be generalised for work beyond the PWD case. In addition to the image viewer and the transcription form, he included a discussion form, where transcribers could ask questions and clarify their work, and where administrators could answer questions and ask for clarification. Since PWD’s transcription application requires users to register and log in, transcribers can view a list of document pages to which they have contributed. This makes it easy for users to return to their previous work. Administrators have the authority to protect pages from further edits and to export the document transcription from the MediaWiki database to the PWD database. Beyond the core operational features of the system, the RRCHNM team did some work to assure that the tool meshed with the needs of the PWD archive. Thus, Ken Albers (RRCHNM Web Designer) created a number of mock-ups for the transcription interface. Using both paper prototypes and unstyled builds of the interface, Albers and Safley did user testing with PWD editors, RRCHNM graduate research assistants who had worked with the PWD archive and with several individuals who had no familiarity with the system. In August 2010, once Safley had sufficiently mapped out the functional requirements for the tool, the Scripto team met with the editorial team from the Papers of the War Department to review wireframe drawings and possible layouts for the tool’s functional web interface. Albers proposed a layout that was quite similar to the design mock.16.NEH-ODH, http://www.neh.gov/divisions/odh and NHPRC http://www.archives. gov/nhprc/. 17.OpenLayers, http://openlayers.org/. ups submitted with the initial grant proposal, which included a vertical split screen with the digital image to be transcribed in a left window and the editing/ transcription window on the right. After interacting with this basic layout the PWD editors expressed their concern about the narrow image viewer and the ways that it would constrain a user’s ability to view a whole line of script at once. This critical feedback resulted in the first revision to the tool’s user interface. As a result of this user testing, we fundamentally reoriented the transcription interface, rejecting the common side-by-side document and transcription window positioning in favour of a top and bottom orientation (see Figure 4.2). In September, Albers and Safley returned to the editors with a functioning mock-up of the tool, which included a horizontal split screen with the image viewer on the top and the editing/transcription window on the bottom. By positioning the OpenLayers image viewer on top with the transcription window below, we maximised the width of the viewer window. This decision dramatically increased the efficiency of transcribers by allowing them to view a complete line of text while zooming in on an image. Next, with this orientation, we narrowed the width of the transcription window to make it comfortable for typed text. Finally, we positioned the list of document page images to the left of the window, allowing a volunteer to proceed easily through multi-page documents. During this round of testing, the users offered suggestions about the location of the page navigation and the links to ‘help’materials such as the style guide. Albers and Safley integrated this feedback into the subsequent build of the tools, which they then integrated with the administrative interface of PWD. This implementation represented the third iteration of the initial user testing. Developing a functioning tool with a logical workflow was only part of the task of launching community transcription with PWD. We also had to create a support apparatus for contributors that meshed with the content and the character of the repository and its users. In preparation for launching Scripto with PWD, we created a registration workflow for new volunteers. Although MediaWiki can be configured to allow users to create their own accounts or to edit documents without being logged into the system, we felt strongly that it was important to maintain editorial control over the transcription system through user accounts and logins. This required login would give us the peace of mind that we would not have to deal with significant spam users and vandalism. Thus, we configured MediaWiki to prohibit document edits by anonymous users, and placed the process of account creation in the hands of the PWD editorial team. To manage that process, we created a Google form to gather registration information from volunteer transcribers. That form requires the minimal data for account creation (username and email address), but requests a full name, affiliation, country, zip code and the reason the user is interested in working with the PWD archive as optional fields. The results of these form submissions are gathered in a Google spreadsheet, which a PWD editor uses to hand-create MediaWiki accounts for each user. Once the user has verified the account by setting her password, she is set to begin transcription work. In addition to the basic transcription interface of the document viewer and the transcription window, the PWD implementation of Scripto required us to create a number of support structures. We developed a set of static text pages that offered potential contributors a clear and concise introduction to the transcription project and its role in the larger PWD project. These included both the invitation to participate as transcribers and the short guidelines for creating good transcriptions within the conventions of the Scripto system. Also, we provided contributors with a large list of potential documents that were good candidates for transcription. While many of our participants were drawn to the project by their own research interests and had clear ideas about the documents they wanted to work with, we realised that we were likely to attract many volunteers who simply wanted to aid the progress of the project and who would welcome our suggestions to direct their work. Thus, PWD assistant editors continually nominate documents for transcription from their ongoing work with the collection. At any given time, there are roughly 200 nominated documents for users to choose from if they do not have their own specific research interests. Each day a PWD editor spends some time working with the volunteers and the transcription submissions. First, he monitors the registration list and creates new accounts. Next he surveys all of the newly created page transcriptions, making some corrections and edits. Then, he reads the discussion pages associated with the transcriptions. Finally, he spends some time blogging and tweeting about the nominated documents, the completed transcriptions and other project progress. As a result, we have a very good sense of the volume of interest and activity amongst transcribers for the first two years of the project. Building the Community of Transcribers The efforts to recruit community members to participate in transcribing the Papers of the War Department was jump-started by early national press recognition that preceded the launch of the transcription facilities. In December 2010, Scripto received press coverage in the New York Times.18 The article generally dealt with efforts of documentary projects to experiment with crowdsourcing, and did not refer to Scripto by name, but this initial mention of the Papers of the War Department work generated a significant amount of interest from potential transcribers and members of the documentary editing community. Since the article was published before the release of the transcription functionality, the project did not reap the significant bump in participation that the Transcribe Bentham project gained from the exposure, but it certainly laid the groundwork for a successful launch. In March 2011, when we officially launched the transcription facilities with PWD, the website received visits from roughly 3,800 unique users, which was a fairly typical number for that point in the life of the project. From this base, we set out to attract a new set of users to the work by coordinating a publicity campaign that included blog posts, twitter coverage and direct messages to email discussion lists with high traffic from early Americanists, those teaching the US history survey and a full range of genealogical organisations. These efforts produced notice in some unlikelyplaces, such as a post by Curt Hopkins entitled ‘Crowdsourcing the Preservation of the U.S. War Papers’on Read, Write, Web, which placed the effort in front of an audience who primarily identified as being interested in technology rather than history. Due to this outreach and press recognition, the project got off to a swift start. Within the first week of launch, we had 120 transcribers request accounts and transcribe roughly a dozen documents. As the months progressed, the momentum continued. By May, the site had 170 transcribers who had completed 80 documents. Within six months, those numbers had increased to 308 users, roughly 70 of whom had been active transcribers, and who had finished 450 documents. By the close of the second year, the project included 1,345 registered transcribers, 227 of whom had been active within the last 90 days, and who had completed just over 2,000 documents. This range shows a relatively slow and steady increase in the number of active transcribers over time, but the amount of completed documents and registered users exhibits a more swiftly increasing rate (see Figure 4.3). Additionally, in March 2013, the website received roughly 11,450 unique visitors, an increase of over 7,600 users when compared to two years earlier. Generalising from these numbers is difficult, in part because of what 18.Cohen, ‘Scholars Recruit Public for Project’. constitutes a document within the PWD collection. Some documents consist of a single page with a few sentences and others are letter books with hundreds of pages. The degrees of difficulty in transcribing the material also varies greatly due to the fact that the documents come from many hands, and frequently include difficult to transcribe tabular data. Nonetheless, the important data generated over the course of the first two years of the project seem, unsurprisingly, to reveal a great deal of productivity on the part of a small number of dedicated volunteers. In the three months leading up to the two year mark, of the 226 active transcribers, only 17 had made more than 100 edits, with the most active contributor, Paulmd199, making more than 2,600, followed by transcriber HollyPBrickhouse with just over 1,500 edits, and then a drop to around 675 from Prosenbloom, and eventually a levelling off where approximately 200 somewhat active users had similar numbers of edits (see Figure 4.4). This curve is familiar to those who work with volunteer editors. As Ben Brumfield has noted, transcription projects and other crowdsourcing ventures tend to follow the power-law distribution, suggesting that 90 per cent of the edits are done by 10 per cent of the users.19 This projection is generally borne out with the participants in the PWD transcription project. 19.Brumfield, ‘Crowdsourcing IMLS WebWise 2012’. More significant, however, than the bulk numbers of transcriptioncontributions from our volunteers, is the range of important information they have offered us about themselves. RRCHNM’s previous extensive experience with digital collecting projects, such as the September 11 Digital Archive (911digitalarchive. org) and the Hurricane Digital Memory Bank (hurricanearchive.org), has taught us that requiring too much information from contributors is a sure-fire way to encourage them not to participate.20 As a result, we generally try to keep our sign-up forms to a bare minimum length, and only make the absolutely necessary fields required. In the case of the transcriber account sign-up, we only required volunteers to provide us with a username and an email address, but we requested a full name, zip code, an affiliation and the reason they wished to participate in the project. These optional fields on the form have provided us with a wealth of data about our contributors. Out of the 1,328 transcribers who had requested accounts by the close of the project’s second year, 74 per cent had offered some information about why they wanted to transcribe documents from PWD (see Figure 4.5). This is quite a remarkable response rate for a non-required field. Analysis of the content of those responses reveals six general types of volunteers. The largest group of contributors 20.For more on these projects, see Cohen, ‘The Future of Preserving the Past’; Brennan and Kelly, ‘Why Collecting History Online is Web 1.5’. (34 per cent) came to PWD with very specific historical research agendas, searching for material on a particular person, place or event. Those who were explicitly engaged in genealogical research (29 per cent) were a close second. Some 14 per cent of volunteers expressed a more general interest in the American Revolution and the early national period while a full 10 per cent of respondents noted that they felt they were making a civic contribution by working to help make the papers accessible. Many of the librarians, archivists and information professionals (8 per cent) requested accounts becausethey were interested in Scripto as a transcription tool. Finally, 6 per cent of accounts were requested by teachers designing activities for students or by students fulfilling an assignment. With transcription volunteers hailing from at least 19 non-Native American nations and 12 Native American communities, contributors cameto the documents with an array of research interests. Even focusing just on the 25 most frequently used words in the explanations for participation, several key areas of interest emerge (see Figure 4.6) – some more predictable than others. Those enthusiastic about family history list themselves as genealogists, independent researchers, descendents, relatives and members of the Daughters of the American Revolution, or the Society of the Cincinnati, organisations that focus on members having family ties to the American Revolutionary Era. Another cohort is focused on military history and the American Revolution, including the development of the Navy, and key events like the Whiskey Rebellion. This focus matches nicely the large number of volunteers who claim affiliation with the United States armed services, either as someone on active duty or a retiree. Those not conversant with the Papers of the War Department might be surprised by the factthat a significant concentration of transcription volunteers are focused on Native American history and Indian affairs, with a healthy interest in Cherokee and Creek tribal affairs. Of those mentioning an interest in Native American history, volunteers reported having affiliations with the Brothertown, Cherokee, Chicksaw, Choctow, Creek, Dakota, Miami, Mohawk, Notaweega, Seneca, Wyandot and Yuchi bands and nations. With key negotiations between the War Department and Native Americans taking place during this period, the repository is filled with important and revealing materials for research on these topics and of interest to these communities. In sum, the two years of experience with community transcription for PWD has yielded a number of important gains for the project. First and foremost, the project has brought many, many more people in contact with the papers, not all of whom have ended up as transcribers. Simply publicising the transcription work has yielded a tremendous increase in website traffic. In 2010, the site saw just over 36,500 unique users. In 2011, the first year of the transcription project, that number rose to just over 51,000, and in 2012 to nearly 90,000 unique visitors. In the first six months of 2013, the site has roughly 62,000 visitors, putting it on track to reach over 120,000 for the year. Second, the editors have a much better sense of the kinds of research interests that are driving the active users who contribute transcriptions. This information will be essential in the planning for the kinds of narrative historical interpretation RRCHNM plans to add to the papers in the coming years. Finally, every completed document transcription increases the ability of users to find the documents they are looking for because it gives the site search and presumably search engines like Google more data to work with, and those texts are also accessible to screen readers that support vision-impaired users. So, in many respects, these contributions offer every PWD user a slowly and steadily improved experience with their research. Generalising Scripto for Widespread Use While the data that RRCHNM has gained from opening the Papers of the War Department to community transcription has provided a range of insights about our users and their interest in the content, we remain committed to building tools that other scholars and cultural heritage organisations can use to advance their own digital work. As a result of that commitment, after the implementation of the transcription facility, the team began the process of enabling connections between the tool and several common content management systems. Unlike many of the other recently developed transcription tools, Scripto was designed specifically to work with existing content management systems, rather than to replace them with a second source repository. The rationale behind this choice was two-fold. First, we firmly believed that cultural heritage institutions need to employ standardised metadata systems when they provide web content. The structured data provided by a standardised metadata system dramatically increase the possibility that the data can be exported to a new system, thus making that content as interoperable as possible. We did not want to create a tool that would impede interoperability by forcing users to separate their source material from their metadata schema. Second, we wanted to offer the lowest possible barrier to use. Hence, the idea that users would have to duplicate their sources in a second system seemed like an unnecessary and unwise step. The software’s architecture resulted from a process of considering the needs of the Papers of the War Department project, and generalising from that case. At its most basic, Scripto is a software library that mediates the communication between a content management system, a custom Scripto application and MediaWiki. The content provider serves the content, usually a corpus of digital material (images, video, sound) that can be transcribed; the Scripto application juxtaposes that content (via a media viewer/player) with a transcription form; and MediaWiki serves as the transcription database, revision engine and user account administrator (see Figure 4.7). We designed the Scripto library to be compatible with potentially any content provider. We accomplished this in two ways. First, the library defines an adapter interface that is used to establish two-way data mapping between the content provider and Scripto. Second, the library normalises the content provider’s identification scheme (no matter how informal and inconsistent) to enable fail-safe data transport between the content provider and MediaWiki. We chose MediaWiki for the transcription database for several reasons: it is the most popular Wiki application and has a sizable and active developer community; Wiki mark-up is relatively easy to learn and there are useful editors available; it offers helpful features, such as discussion pages and user administration; and it comes with a powerful, fully featured application programming interface (API) that offers technical hooks which Scripto uses to interface with the content provider and transcriptionapplication. Transcriptions are stored in the MediaWiki database until an administrator exports them to the content provider’s database. This is done to ensure that transcriptions are complete and vetted before they are added to the content provider’s database. Each project can then evaluate the contributed transcriptions according to its own criteria. Scripto is interface-agnostic, meaning that content providers are not tied to a single user interface or a predetermined feature set. They have full control over how their transcription application looks and functions. This makes it possible to embed a fully customisable user interface into an existing context, such as a website or standalone application. It does require some technical proficiency to build a transcription application, but Scripto’s API is straightforward and well documented. Since the majority of projects do not have the time or technical expertise to create their own transcription application, under the Scripto flagship project, web developer Jim Safley created a set of connector scripts that enable users of common content management systems to implement Scripto with their work. The resultant scripts for Drupal, Omeka and WordPress are each available on Github as open-source code, and in zipped versions ready for installation from the Scripto site.21 The development community that coalesced around the software through a Google Groups developers’ discussion list offered crucial feedback on features and functionality during the connector development and testing process. The development group included 23 active members who participated in 18 support and feature conversations on the email list during the initial release of the testing versions of each connector. As a result, each connector had several releases and updates in response to developer community interaction with Safley. In support of the release of the stable connector scripts, the Scripto team fully redesigned the software’s website in June 2012 (see Figure 4.8). During the life of the project, the site had received just over 13,000 unique visitors, and the new ‘look and feel’ made the site a much more welcoming place for those visitors. The new design highlighted the software’s functionality and offered project administrators easy access to a ‘User’s Guide’for working with Scripto that details the installation process, the editor’s role and the transcriber’s role.22 The guide provides non-technical users a step-by-step introduction to working with Scripto in each of the content management system environments. It also contains tips on project organisation, volunteer management, transcription oversight and outreach. While the ‘User’s Guide’ offers potential users a great deal of information about the software and its implementation, our experience in software development tells us that users want to experiment with a system before they install it on their own sites. As a result, we also set up ‘sandbox’sites for users to work with both an Omeka implementation and a WordPress implementation.23 We did not offer a Drupal sandbox because the Drupal system is so flexible and customisable that 21 WordPress+Scripto plugin, https://github.com/chnm/scripto-wordpress-plugin; Drupal+Scripto extension, https://github.com/chnm/scripto-drupal-module; Omeka+Scripto plugin, http://omeka.org/add-ons/plugins/scripto/. 22 Scripto ‘User’s Guide’, http://scripto.org/documentation/. 23 Omeka+Scripto sandbox, http://scripto.org/omeka/; WordPress+Scripto sandbox, http://scripto.org/wordpress/. it is almost impossible to create an environment that would realistically mirror an actual individual user’s experience with that content management system. Since the public release of the connector scripts, many testing sites and Scripto implementations have sprung up across the web. The most extensive work has come out of university libraries. For example, while the University of Iowa Libraries might be best known for their initial crowdsourcing venture ‘The Civil War Diaries and Letters Transcription Project’, which launched in spring 2011 using a rudimentary transcription submission form, the Libraries have since launched a more extensive transcription project called ‘DIY History’, which uses Omeka and Scripto as its software platform (see Figure 4.9).24 The project offers users a chance to transcribe materials from the Libraries’ culinary manuscript collections and the Iowa Women’s Archives. To date, the site includes over 35,000 transcribed pages. Similarly, the University of Alabama Libraries has offered over 500 items from their collections for transcription using Omeka and Scripto. The materials are drawn from the Manly Family Papers collection, which relates to the early history of the university, and the Meriwether Family Papers collection, which centres on the correspondence between an Alabama Infantryman and his wife during the 24.University of Iowa Library’s ‘DIY History’, http://diyhistory.lib.uiowa.edu/. Civil War.25 On a somewhat smaller scale, the College of William and Mary’s Swen Library Digital Projects site uses Omeka and Scripto to add crowdsourced transcription to its materials. Their Special Collections have digitised materials from the Civil War period for the ‘From Fights to Rights: The Long Road to a More Perfect Union’ project and from a number of collections related to the College itself to build a site that enables the transcription of nearly 3,800 items.26 The site ‘Texas Manuscript Cultures’ is using Omeka and Scripto to offer crowdsourced transcription of a variety of handwritten manuscripts related to Texas social and cultural history before 1950. Currently there are over 75 manuscripts available for transcription, and the site is supported by staff from a wide range of Texas libraries and historical societies.27 Finally, Dr William B. Hafford, an archaeologist at University of Pennsylvania, has launched ‘Crowdsourcing Ur’, a site to solicit public assistance in transcribing the documents related to the excavations of Ur in Mesopotamia. The joint expedition of the British Museum and the University of 25.University of Alabama Libraries’ ‘Transcribe’, http://transcribe.lib.ua.edu/. 26.College of William and Mary’s ‘Swen Library Digital Projects’, http://scrcdigital. swem.wm.edu/. 27.‘Texas Manuscript Cultures’, http://writingstore.com/txmsc/. Pennsylvania Museum occurred between 1922 and 1934, and resulted in a cache of documents about the excavations.28 Together, these examples show the early work that the Scripto extensions have enabled for organisations with document collections. But community transcription need not be limited to documentary collections. Since Scripto was designed to be as flexible as possible, projects could implement it to assist with the transcription of any range of file types, including audio and video files. This flexibility points to a bright future for the tool and for the range of projects that might choose to engage their users and constituents in contributing to making cultural heritage more accessible. In turn, this engagement can lead to a strengthening of the bond between core audiences and cultural heritage institutions. And, if those institutions learnas much about their volunteers as the PWD editors learned about their transcribers, they will be in good stead to continue to develop programs and applications that address their users’ needs and interests. References Brennan, Sheila A. and T. Mills Kelly. ‘Why Collecting History Online is Web 1.5’, March 2009. http://chnm.gmu.edu/essays-on-history-new-media/essays/?essay id=47. Brumfield, Ben. ‘Crowdsourcing IMLS WebWise 2012’. Collaborative Manuscript Transcription, March 17, 2012. http://manuscripttranscription.blogspot. com/2012/03/crowdsourcing-at-imls-webwise-2012.html. Causer, Tim, Justin Tonra and Valerie Wallace. ‘Transcription Maximized; Expense Minimized? Crowdsourcing and Editing The Collected Works of Jeremy Bentham’. Literary and Linguistic Computing 27, no. 2 (2012): 119–37. Cohen, Daniel J. ‘The Future of Preserving the Past’. CRM: The Journal of Heritage Stewardship 2, no. 2 (2005): 6–19. http://chnm.gmu.edu/essays-on.history-new-media/essays/?essayid=39. Cohen, Patricia. ‘Scholars Recruit Public for Project’. New York Times, December 28, 2010. http://www.nytimes.com/2010/12/28/books/28transcribe.html. Crackel, Theodore J. ‘The Common Defence: The Department of War, 1789– 1794’. Prologue (Winter 1989): 331–43. Daines, J. Gordon, III and Cory L. Nimer. ‘The Interactive Archivist: Case Studies in Utilizing Web 2.0 to Improve the Archival Experience’. Societyof American Archivists, May 18, 2009. http://lib.byu.edu/sites/interactivearchivist/. Evans, Max J. ‘Archives of the People, by the People, for the People’. American Archivist 70, no. 2 (2007): 387–400. Holley, Rose. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3/4 (2010). Available at http://www.dlib.org/dlib/march10/ holley/03holley.html. 28.‘Crowdsourcing Ur’, http://urcrowdsource.org/omeka/. Hopkins, Curt. ‘Crowdsourcing the Preservation of the U.S. War Papers’. Read, Write, Web, March 18, 2011. http://www.readwriteweb.com/archives/ crowdsourcing_us_war_papers.php. Howe, Jeff. ‘The Rise of Crowds’. Wired 14, no. 6 (2006). http://www.wired.com/ wired/archive/14.06/crowds.html. Kalfatovic, Martin et al. ‘Smithsonian Team Flickr: A Library, Archives, and Museums Collaboration in Web 2.0 Space’. Archival Science, October 2009. http://dx.doi.org/10.1007/s10502–009–9089-y. Owens, Trevor. ‘Meanification and Crowdscaffolding: Forget Badges’. Playing the Past, March 17, 2011. http://www.playthepast.org/?p=1027. Rosenzweig, Roy. ‘Can History Be Open Source? Wikipedia and the Future of the Past’. Journal of American History 93, no. 1 (2006): 117–46. http://chnm.gmu. edu/essays-on-history-new-media/essays/?essayid=42. Springer, Michelle et al. ‘For the Common Good: The Library of Congress Flickr Pilot Project, Final Report’, October 30, 2008. http://www.loc.gov/rr/print/ flickr_report_final.pdf. This page has been left blank intentionally Chapter 5 What’s on the Menu?: Crowdsourcing at the New York Public Library Michael Lascarides and Ben Vershbow The New York Public Library (NYPL) has launched a number of web-based crowdsourcing projects in recent years, enabling audiences to help with map georectification, the transcription of historic documents, annotations on texts and more. A recent example is What’s on the Menu?, an award-winning website that has turned the Library’s collection of historical restaurant menus into a constantly updated database of dishes and prices. A large amount of crowd-generated data has now turned the menu collection into a tool for discovery and research at a previously impossible level of detail. What’s on the Menu? was designed in 2011 to transform roughly 9,000 digitised images from the Library’s significant Buttolph Menu Collection into a searchable database of historical culinary and economic trends. Because of difficulties in mechanically extracting quality text from the menus, and from a desire to build a structured data set of discrete dishes and prices, it was determined that manual transcription would be the best method for creating the database. As the task was far too large and time consuming to accomplish with internal resources, we saw an opportunity to explore crowdsourcing as a means of accomplishing the work. We decided to build a simple web application for transcribing the menus and see if members of the public would be willing to volunteer their time. Our original application operated on the theory that in order to sustain participation in an ambitious collections crowdsourcing project with a multi-year time horizon, it was essential to be creative with user engagement. We considered drawing upon game design principles to devise ways of keeping participants interested, rewarded and motivated. The initial plan was to launch the user-interface application as a no-frills beta, learn from the initial response and then work with a game design studio to revamp the site along more imaginative lines. But things turned out rather differently. Due to an overwhelming public response, the initial batch of menus was transcribed within about three months of launch, requiring us to seek additional resources, ramp up our digitisation efforts and re-factor other library workflows to keep fresh content flowing into the site. Most significantly, after settling down from the initial publicity spike, use of the site did not diminish. Instead, it relaxed into a steady engagement pattern that continues to this day. The experience has led us to question our assumptions about what motivates people to participate in collaborative projects, and has built our confidence that public organisations such as libraries and museums are uniquely positioned to tap into incentives and rationales for participation that are different from commercial enterprises. Our experiencewith What’s on the Menu? has also helped us to clarify several of the factors that make a crowdsourcing project a success or a failure. These factors span the lifecycle of the project, from planning at the outset, through building an application, to promoting and growing the community, to integrating the project with the rest of the Library’s systems. Why Is NYPL Interested in Crowdsourcing? In recent decades, libraries, archives and museums have been digitising large quantities of rare and unique materials, moving many of them out of the institution and onto the internet. While these efforts have greatly expanded basic access to certain collections, additional work is often required to realise their usefulness fully in the digital environment. Where descriptive metadata exist, they often require modification and normalisation against existing standards in order to surface the materials better in federated searches and other data-driven discovery mechanisms. In some cases, metadata must be created for the first time. In other instances, as with the menus project, materials must be intensively processed following digitisation in order to make their richest contents accessible to search engines, application developers and digital humanists. Historical maps must be ‘georectified’ to existing geographic coordinate systems in order to be searched and queried spatially like today’s digital maps. Books and newspapers must be optically processed to extract searchable text, the output of which often requires subsequent manual cleanup in order to become useful. Still other texts (such as old city and business directories, church registries, playbills, ship’s manifests, etc.) are in essence databases in printed form, and their contents must be laboriously transformed into structured information in order to be accessed and referenced against other data sets. Migrating the sum of human knowledge to the internet is daunting, especially for resource-strapped cultural organisations. But these institutions, while lacking the financial and engineering assets of big technology and media companies, do have one ace up their sleeve: their public mission. The web contains harbingers of new kinds of public libraries and museums, such as the Internet Archive, Project Gutenberg, Freebase, Wikipedia, Flickr Commons, the Creative Commons; their purpose is to provide knowledge and tools for the civic realm, to serve as free resources for a lifetime of learning. Most of the initiatives mentioned above were built collaboratively, often for no monetary or material reward, by a motivated corps of well-informed, diligent enthusiasts looking to devote their time to a cause bigger than themselves. Through networked collaboration on the internet and an inspiring mission, Wikipedia built an open-access encyclopaedia by tapping into the same civic impulses and 115 irrepressible curiosity that have propelled people through the doors of libraries and museums for generations. Now traditional institutions must engage this internet citizenry in new ways – and many already are. For NYPL, this suggested a new kind of public library, one built not only for, but in collaboration with, its publics. Increasingly, and in large part through the lessons learned in What’s on the Menu?, we are coming to see crowdsourcing not only as a way to accomplish work that might not otherwise be possible, but as an extension of our core mission. Often, the team refer to crowdsourcing projects interchangeably as ‘micro-volunteering’ projects, with the idea that patrons can contribute assistance to the library in as little as a few seconds if they like. As Trevor Owens, Digital Archivist at the Library of Congress, puts it: ‘it is about offering your users the opportunity to participate in public memory’.1 Our thinking was heavily influenced by two important new media thinkers: Clay Shirky and Jane McGonigal. Shirky has written on the opportunities and challenges faced by institutions seeking to take advantage of the internet to do work traditionally handled within an organisation’s walls.2 Shirky cautions that as the organising costs of participation networks fall, the difficulty of attracting participation (and then sustaining it when you have it) will rise precipitously. Along similar lines, Jane McGonigal asserts that for organisations to stay competitive in ‘the economy of engagement’, they should think of their projects along the lines of games.3 Games, according to McGonigal, have tremendous world-improving potential if harnessed for public-interest projects. What if the hours spent outrunning the cops in Grand Theft Auto or blasting Nazis in Call of Duty were channelled into figuring out peace in the Middle East, folding protein puzzles to aid scientific research or gathering historical weather data sets to predict better the climate’s future?4 We saw the Library as being a perfect context for building ‘games for good’. Prior Investigations and Crowdsourcing Forays Founded as a repository for predominantly print and paper-based materials, NYPLis now exploring how to convert an analogue knowledge base into a digital resource of comparable importance. Mass digitisation of the Library’s printed book corpus began nearly a decade ago with the Library’s involvement in the Google Library Project and the Open Content Alliance (now absorbed by the Internet Archive). Internal digitisation efforts have focused on special collections: prints; photographs; manuscripts; and other rare book and archival materials. This has also been the locus of NYPL’s experiments in digital collection building and 1.Owens, ‘Crowdsourcing Cultural Heritage’. 2.Shirky, Here Comes Everybody. 3.McGonigal, Engagement Economy. 4 PeaceMaker, http://www.peacemakergame.com; Foldit, http://fold.it/portal; Old Weather, http://oldweather.org/. data processing, which lately have been spearheaded by a recently formed team within the Library, NYPL Labs.5 The Library’s first major venture into crowdsourcing was the Map Warper (maps.nypl.org/warper), a web-based platform where users are invited to assist with the georectification of historical atlases to produce digitised historical maps that can be overlaid on contemporary digital maps and exported in layers for further study. The Map Warper project has enjoyed great success, with the number of georectified maps in the thousands. However, the task of transforming maps with the tool comprises multiple steps, and in practice, has worked best when accompanied by hands-on training or situated residencies – for instance, with a geography class from a local university – that occur over multiple weeks. Participation, while consistent, has been more modest and confined to a specialist audience. Another NYPL project that informed our thinking (albeit one that is not, strictly speaking, a crowdsourcing application) was the usability and survey tool Infomaki.6 Designed to be a very quick and lightweight tool for gauging patrons’ reactions to proposed web design concepts, Infomaki posed the question, ‘Do you have time to help the Library by answering a single question?’. Despite this modest sales pitch, the ease of responding led to a higher than expected engagement rate, and Infomaki demonstrated that an interface that required a tiny amount of effort on the part of the user had the potential to generate a lot of data. Outside Sources of Inspiration Outside of the Library, there were a number of successful antecedents we looked to in order to crack the code of what makes a successful crowdsourcing design. In the Trove newspaper project from the National Library of Australia (trove.nla. gov.au), we saw an active community, high participation rates and an easy-to-use transcription interface. It was a successful effort we could point to that had achieved results at scale. The Old Weather project (www.oldweather.org) demonstrated the power of immediate feedback, and showed how compelling it was to weave crowdsourced efforts together into a coherent narrative. The Guardian’s MP’s Expenses project (www.guardian.co.uk/politics/mps-expenses) and the Victoria & Albert Museum’s image cropping tool (collections.vam.ac.uk/crowdsourcing/) showed us the cumulative power of a single small gesture repeated by many thousands of participants. In addition to learning from their design, being able to point to the success of these sites and others helped us to build the case for our management that crowd-based collaborations were worth pursuing. 5.http://www.nypl.org/collections/labs. 6.Lascarides, ‘Infomaki’. Why the Menu Collection? The Library’s collection of historical restaurant and event menus, housed in the Rare Book Division, originated through the energetic efforts of Miss Frank E. Buttolph (1850–1924), who collected more than 25,000 menus on the Library’s behalf between the years 1900 and 1924. The collection, which is especially but not exclusively New York-related, has continued to grow through additional acquisitions, and now contains approximately 45,000 items dating from the 1840s to the present. About a quarter of these items had been digitised and made available in the NYPL Digital Gallery7 at the time What’s on the Menu? was conceived. The Buttolph Menu Collection was identified early on as a promising test-bed for experimentation: an ephemeral, underutilised collection that has historically been difficult to catalogue and preserve, yet which contains, in the aggregate, vast quantities of cultural data: a potential goldmine to historians, journalists, chefs, novelists and educators. The menu collection has the added advantage of popular appeal. Featuredin two major exhibitions at the Library over the past decade and a major source for various scholarly works, the collection has been a consistent draw for enthusiasts, educators and researchers, tapping into broad popular interest in food and the origins of dining culture. However, it was very difficult to search the menus for the greatest treasures they contain: specific information about dishes, prices, the organisation of meals and all the stories these things tell us about the history of food and culture. Embedded in ink and print (and more recently, pixels) are millions of interrelated data points that tell social histories as vast as the population of oysters in New York City and as particular as the price of a cup of coffee at Child’s Lunch Room on 130 Broadway in 1901 (5 cents). How best to extract these data? While some of the menus might have been transcribable via Optical Character Recognition (OCR) methods, the menus varied widely in their layout, presentation and legibility; many were handwritten, and few had the distinct lines of text you might find in a manuscript. Furthermore, we wanted to create a searchable database of dishes (as distinct from section headings and other descriptive text on the menus’ pages) complete with prices and currencies, so simply capturing all of the text by automated means, while somewhat adequate for searching, would not automatically build the research tool we had in mind. The Initial Design What’s on the Menu? was created to provide an experience that was simpler and less intimidating than the Map Warper, and to capitalise on popular interest in food to elicit greater participation. In order to attract the broadest possible audience, we strove to keep the task basic and clear (capture only dishes and prices); build the .NYPL Digital Gallery, http://digitalgallery.nypl.org. 119 most stripped-down tool possible (click on a region of a menu image, type what you see); and keep barriers to participation low (no registration or login required). The Library’s web team began designing and coding the beta website as a side project. Our plan was to launch the application in early 2011 with some modest outreach and to monitor the public response. We were confident that the project would attract attention, but less certain that participation would persist. The plan was to assess the basic viability of the tool (was the task sufficiently clear? the tools usable? would people participate?), make any needed refinements and fixes and then begin to devise interface and engagement elements to make the project ‘sticky’ over time. After some internal discussion and some whiteboard sketching, an initial prototype was constructed that stitched together the basic interactions: selecting a menu from a list of those available, choosing a dish to transcribe and automatically generating a stand-alone web page for each new dish and each restaurant or venue. On our first prototype, once a patron selected a dish to transcribe, they were asked to enter four pieces of information: the dish as written; the price for the dish (if any); the currency for the price; and the name of the section (‘Entrees’, ‘Dessert’, etc.) that contained the dish, if applicable. However, as we progressed some internal testing with the prototype, it became clear that this presented a great deal of repetitive effort, and a close look at the workflow suggested some important design refinements. For starters, we modified the prototype so that the currency could be set once for the whole menu (as could the option for whether or not the menu had prices listed at all). Currency, then, became a field that was set by the Library as part of the official menu metadata, before transcription. And if it was indicated that a menu had no prices at all, the field for transcribing a price did not appear. We also came to the realisation that, since we were saving the position of each dish on the page, we could always come back later and outline the shape of menu sections, and that this task did not need to be completed at the same time as dish transcription. Ultimately, we would launch without any menu section-making tools at all. This left the core transcription task as, ‘Click on a dish, and type what it says.’ As the perceived complexity of a task has a lot to do with the number of options presented to the user or the number of steps in a task,8 it was vital to the success of the project to make sure that this core interaction – the one that we wanted people to repeat over and over again – felt as simple and as natural as possible. By reducing the number of fields from four per dish to one (or sometimes two, if and only if there are prices on a menu), we lowered the cognitive load on the user significantly. By the end of our first month, the average visitor was transcribing 8.For a layman’s overview of the issues involved, see Julien, ‘Cognition & the Intrinsic User Experience’. For an in-depth discussion of the effects of number of choices on the perceivedcomplexity of a task, see Rouder et al., ‘An Assessment of Fixed-Capacity Models’. over 30 dishes per visit, which gave a strong indication that our efforts to reduce the awkwardness of the initial prototype had been successful. There were other refinements. Initially, when a menu was being transcribed by one user, all other users were locked out of that menu for five minutes. Once multiple users started using the prototype, however, it became clear that a small number of clicks from a few users could tie up all of the available menus, leading to a waiting game. The fix was to add an indicator that a single dish was being edited at the moment, while allowing a transcription to take place on any other dish on the page. Not only did this massively free up material for transcription, it led to the fascinating sight of two or three other people simultaneously transcribing a single page. Not all of the early edits were made to sharpen the transcription interface; other interface decisions were responses to the quality of data available. The original prototype, for example, had a page for each restaurant or venue, which had to be dropped when it became clear that the venue metadata from the original database (which had been itself transcribed by volunteers internally years earlier) was not yet in suitable shape to support this feature. Another more contentious omission was the lack of a user registration system. Due to a number of factors (including time pressures, a forthcoming but not quite ready library-wide sign-on system and simplicity of use), it was decided to try the original launch without any kind of sign-up system in place, and instead to engage with users only through social media and email newsletters. While this certainly lowered the barrier to participation, it was in retrospect perhaps a missed opportunity to identify and connect with the most committed transcribers in the community, as evidence from other projects has indicated that the top few most engaged users usually account for a disproportionate share of the total contributions. Practical and Legal Concerns The participation of patrons in the transcription and creation of data and metadata presents a new kind of challenge. Usually, you will want to share the content that results from their labours as broadly as you can. The question may come up from participating patrons whether they have any rights over the data. It needs to be made very clear at the outset that your library entirely owns the newly created data to do with whatever it wants, and that the participant willingly relinquishes any ability to restrict those rights. To this end, the NYPL drafted a Policy on Patron-Generated Data which says that the Library retains a ‘non-exclusive, perpetual, world-wide, irrevocable licence’ to any content created on its websites. By being clear up front about who has the rights to use, reuse and remix the crowdsourced data, we were able to move with confidence into later steps of the project such as a free, open and searchable application programming interface (API) for programmatically querying the data, and bulk downloads of the entire database. What’s more, the menu project’s success sparked discussion at the highest levels of the institution about how to treat and preserve user-contributed metadata, so it became all the more incumbent upon our team to create a viable data management model and process. Initial Launch and Reception Abasic first version of the participatory transcription application launched on 18 April 2011 and was an instant hit. Despite being announced with just a single tweet on the Library’s Twitter account, 100,000 dishes were transcribed in the first 10 days. The project was featured in the New York Times’‘Dining Section’, National Public Radio and a number of food, library and general interest blogs. In the following months, New York chefs such as Mario Batali (of Babbo, Del Posto, Eataly) and Doug Crowell (of Brooklyn’s Buttermilk Channel) publicly endorsed the project, and others (such as Rich Torrisi of Little Italy’s Torrisi Italian Specialties, and David Chang, creator of the Momofuku chain) created special menus derived from items in the collections. As a testament to the ease-of-use of the tools, a class of fourth graders in San Antonio, Texas used the site to practise typing skills while learning about historical cuisine. Clearly, the project was having no trouble keeping participants engaged. In fact, we ended up having a new problem: keeping up with demand! The project had begun with a portion of the collection that had been digitised some years back for the launch of the NYPL Digital Gallery (a little under a quarter of the approximately 45,000 menus). When the public made quick work of the initial batch, we found ourselves re-prioritising existing digitisation queues to ensure a steady flow of new menus. New resources had to be secured to pay a collections assistant in the Rare Book Division (where the menu collection resides) to process the menus by hand, creating high-level item records and insurance estimates box by box before shipping the assets off to our Digital Imaging Unit. But that was not the only hurdle. The collection’s data records (created, in an interesting anticipatory echo of What’s on the Menu? by a group of on-site volunteers a decade earlier) were inconsistently formatted or simply non-existent. In mid-summer of 2011, a meeting was held with the NYPL Labs and metadata teams where it was decided to retire the legacy menu database and to re-catalogue the menus according to a new, lighter-weight schema. All the while, we were focused heavily on managing our new community of volunteers. Without a user registration system, all contacts with users were initiated voluntarily through email and social media. We quickly set about recruiting a small team of interns in late 2011, who were trained to manage the day-to-day content administration duties: moving menus to the public transcription queue, checking contributions in the site’s ‘under review’ section (the intermediate stage in the menu transcription workflow) and responding to user queries via email. By early 2012, the one year anniversary of the project, it was still going strong. The site had won various accolades, including the prestigious Roy Rosenzweig Award for Innovation in Digital History from the American Historical Association and was widely touted as a model in the cultural heritage sphere. Since the initial frenzy around launch, participation levels had tapered but then settled into a steady pattern of deep engagement, with occasional attention spikes whenever significant new press or social media coverage occurred. Web analytics provide compelling clues as to the depth of engagement the site has fostered. In the 16 months following launch (18 April 2011 to 19 August 2012), page views per visitor have averaged nearly 23 (compare to 2.27 for visits to NYPL’s main website in the same period). That has led 163,690 visits to produce nearly four million page views. Similarly, per-visit session length on What’s on the Menu? has averaged at 6.36 minutes (compared to 2.38 on NYPL. org). This suggests deep immersion in the transcription activity and in exploration of the steadily growing trove of menu-derived data. Site Upgrade and ‘1.0 Release’ Although the What’s on the Menu? beta site was effective from a user standpoint, it had been built as a spare time project, intended to launch quickly and to test hypotheses before investing any further resources. The beta release successfully launched a transcription tool, but it lacked robust tools for browsing, searching and exploring the collection and its resulting data. Moreover, many core functions needed to be rewritten in the interest of sustainability and performance. In other words, in order to get off the ground, the project had incurred a certain amount of ‘technical debt’ that eventually had to be repaid by NYPL Labs in order to get the site into a more sustainable, fully featured condition. We decided to focus our new development efforts on an overhaul of the application code (paying down our technical debt), a new visual design and user interface improvements, new search engine and browsing features, and a public applications programming interface (API) to provide other application developers or digital researcher access to real-time data from the project. From a user engagement standpoint, the site is almost the same as it was. The primary transcription interface was working well and did not require more than a cosmetic update. We still do not require participants to create an account, and do not, in fact, even offer the option. All contributions are anonymous and communications with participants are all user-initiated via email and social media. However, the revamped What’s on the Menu? site represents a substantial improvement over the original beta application. In addition to a cleaned-up design, the ‘detail’ pages for each dish include better visualisations for the placement of the dish on the physical menu page and the dates of the menus in which that dish appears, as well as a way to browse and aggregate related dishes. Perhaps most importantly, the release of the menus data API is a milestone for NYPL, bringing the Library a step closer to the time when the bulk of our digital content and data (at least the portion in the public domain) is available programmatically for other technologists, designers and researchers to build upon. 129We have received a number of requests already from researcher and cultural heritage ‘hacker’ types to start playing around with it, and have been in discussions with a few publications about highlighting stories told by the data. And in accord with modern web engineering best practice, we are using the same API internally to run some of the key features of the What’s on the Menu? site. In March 2013, we launched the GeoTagger application as a companion to the main Menus site. The GeoTagger is a crowdsourcing tool designed and built to attach geographic metadata to items in the NYPL collection by locating relevant addresses on a map. As mentioned earlier, an original intent of the project was to map restaurants so that dishes could be tracked not only by time and price, but also by location. However, given that the locations in the original database were often inconsistent or inaccurate (for example, we once counted 187 distinct spellings/punctuations of ‘Waldorf-Astoria Hotel’), this proved impossible. Rather than correct this name data line by line, the GeoTagger adds the latitude and longitude coordinates directly to the menu’s venue, which will allow it to be placed automatically on a map. The technology running the GeoTagger is also written with the intent of reuse. By keeping the application separate, we can use the interface in any other project that requires geographic data to be added to digitised collection items. What’s Next? Eventually we’d like to offer a non-mandatory registration option so that we can begin to offer some recognition to our core participant base on a ‘leader board’ or ‘hall of fame’-type honour roll.9 More than that, we would like to someday create another tier of participation for more serious, dedicated users, entrusting them with more difficult tasks (such as defining menu sections or harvesting geographical data) and perhaps giving them other special privileges on the site or means of guiding and helping other users. We have not yet created a user account system in part because NYPL is still developing aspects of its digital infrastructure that will make implementation simpler and more sustainable, such as a ‘single sign-on’ account system that would traverse all NYPL web properties. As we have learned from this project, crowdsourcing at scale across the Library should not be attempted in the context of production-intensive, stand-alone apps like What’s on the Menu?, but should instead be addressed with a new generation of reusable tools that require less maintenance and serve a wider variety of purposes. For highly specialised tools such as the Map Warper, there may continue to be a rationale for freestanding projects, but document transcription and other manual data entry, such as tagging or item-level descriptions, could easily become a core function of an evolving digital 9.See Trove Australian Newspapers, Text-Correctors Hall of Fame, http://trove.nla. gov.au/ndp/del/hallOfFame collections platform. NYPL Labs was recently awarded a National Endowment for the Humanities grant for the development of Scribe, a crowdsourced transcription tool, which we hope will be a platform for supporting that function.10 Additionally, we are paying close attention to platform-scale efforts such as the Citizen Archivist Dashboard at the National Archives,11 and some of the things that may eventually emerge from the Digital Public Library of America. NYPL’s own recently launched Digital Collections site12 could very well evolve in this direction, continuing to provide online access to an ever-wider array of digitised material alongside a suite of crowdsourcing projects and tools for those who want to get more deeply involved. On the collection side, we are in early discussions with other leading menu repositories such as Cornell University, the Los Angeles Public Library and the University of Nevada, Las Vegas, about publishing their collections through our website and sharing the crowdsourced data with our respective repositories. Game-Like Behaviour and the ‘Gamification’ Debate Along the path of the development of What’s on the Menu?, the idea of ‘gameplay’ and ‘gamification’ has come up numerous times. We had several meetings on the subject, including one with Mary Flanagan of Dartmouth University, whose emerging ‘Metadata Games’ platform shows much promise,13 and explored the possibility that the NYPL Labs developers (some of whom had experience designing games) would implement a gameplay system internally. However, our current consensus is that a tool that can deeply engage patrons directly with a collection and stoke their imaginations does not need to have a layer of gameplay added to it in order to be successful. There may be scenarios where creating a full game-like experience is warranted, especially when inviting users to contribute their own original content. (See the Smithsonian Museum of American Art’s Ghosts of a Chance,14 or NYPL’s own Find the Future15 –created with Jane McGonigal –as examples.) But in the case of crowdsourced cultural heritage projects, where individuals are taking part in actual work carried out on collections, the incentives reside in the materials themselves and in the proposition of working in partnership with a public trust. This is also the reward. Involving the ‘crowd’ isn’t just a means to an end, it is an end in itself. That is the engagement we are looking for, and it turned out that What’s on the Menu? had it all along. Returning to Trevor Owens (Library of Congress): 10.National Endowment for the Humanities, ‘Announcing 6 Digital Humanities Implementation Grant Awards’. 11.http://www.archives.gov/citizen-archivist/. 12.http://digitalcollections.nypl.org/. 13.http://www.tiltfactor.org/metadata-games. 14.http://ghostsofachance.com/. 15.http://game.nypl.org/. What crowdsourcing does, that most digital collection platforms fail to do, is offer an opportunity for someone to do something more than consume information. When done well, crowdsourcing offers us an opportunity to provide meaningful ways for individuals to engage with and contribute to public memory. Far from being an instrument which enables us to ultimately better deliver content to end users, crowdsourcing is the best way to actually engage our users in the fundamental reason that these digital collections exist in the first place.16 This is in the case of the public sector. In commercial scenarios, where there is on some level an act of persuasion required in the invitation to participate, a sometimes slippery exhortation (or seduction) to consume, an entirely different set of motivators and incentive structures are at play. Cultural heritage crowdsourcing is about contribution, not consumption. It is less persuasion, more call to action. It resonates on a frequency instantly picked up by a portion of the public that is already well convinced of the value of memory organisations and will go to lengths to bolster them. And by offering new, internet-native ways of getting involved in the monumental task of migrating our heritage to digital media, we open ourselves to new audiences and collaborators who share these basic values, and who may begin to see libraries, archives and museums as peers of Wikipedia et al. – true organs of the open web. Should we recognise and reward participation? Absolutely. As collections crowdsourcing gains traction in cultural heritage organisations, it will be important to develop online community architectures not only to recognise, but also to manage and coordinate a growing participant base. But doing so does not make our projects games, and we would do well to cut through the confusion on this subject. Margaret Robertson, of London-based game design studio Hide&Seek, has written lucidly on the topic, making the distinction between gamification and what she calls ‘pointsification’: That problem being that gamification isn’t gamification at all. What we’re currently terming gamification is in fact the process of taking the thing that is least essential to games and representing it as the core of the experience. Points and badges have no closer a relationship to games than they do to websites and fitness apps and loyalty cards. They’re great tools for communicating progress and acknowledging effort, but neither points nor badges in any way constitute a game. Games just use them – as primary school teachers, military hierarchies and coffee shops have for centuries – to help people visualise things they might otherwise lose track of. They are the least important bit of a game, the bit that has the least to do with all of the rich cognitive, emotional and social drivers which gamifiers are intending to connect with.17 16.Owens, ‘Crowdsourcing Cultural Heritage’. 17.Robertson, ‘Can’t Play, Won’t Play’, emphasis in original. We in the library and museum trade should remember that the ‘rich cognitive, emotional and social drivers’ Robertson describes come pre-woven into cultural heritage crowdsourcing projects. And not just for non-profit organisations – hugely successful participatory initiatives launched by genealogy websites such as FamilySearch and Ancestry have demonstrated that this can hold true in the commercial sphere as well. In the case of What’s on the Menu?, there are people for whom deep interaction with the menu collection is a reward in itself. They have stayed with the project because they believe in its overall objectives: to build a new kind of research tool for better understanding these menus in historical and cultural context, and for revealing the many stories they tell. Moderation, Correction and Approval We shared the common worry that opening the door to patrons’ free entry of text on a website would result in an endless stream of 14-year-olds typing nasty things, causing public embarrassment and bringing the project to a crashing halt. At this early stage of the cultural heritage crowdsourcing field, there is precious little statistical data to make broad generalisations about the accuracy of its output. However, our experience on this front matches what we had been hearing anecdotally from the administrators of other crowdsourcing projects: digital vandalism is far rarer than you might expect. We took a wide-open stance and allowed any user to transcribe anonymously and any other user to correct existing transcriptions. Yet, a few months into the project (with approximately 250,000 dishes transcribed), when we ran the entire database of patron-generated text through a ‘bad words’ filter we found only a single instance of mild intentional vandalism. And while a formal study of the accuracy of the project’s output has not yet been undertaken, the level of accuracy has been higher than expected. Lessons Learned What’s on the Menu?, like other projects at the NYPL before it, was conceived and executed as an experiment to understand the challenges and rewards of direct engagement with our patrons. As a result of these experiments, we have come to realise that although there is no hard, fast set of rules governing what makes a successful crowdsourcing project, there are definitely some guidelines that increase the chances for success. • ‘Choose your parents wisely’: the old quip about the best way to become an Olympic athlete applies here – it is much easier to get patrons excited about participation in a project if they are already excited about the source material when they arrive. Although the NYPL has hundreds of different collections, the menu collection was chosen for its broad appeal to both the casual viewer and the serious scholar. • Engage the user on an emotional level: expose the stories behind a collection, and make them relatable to users. Use feedback to create narratives (as in Old Weather, where every entry transcribed moves the ship along on a map). • Appeal to the user’s better nature: we have found that participation increases dramatically when we frame our calls for participation in terms of helping the library. Calls to participation should be framed the same way that any other volunteer opportunity would be, even if it is one that only lasts 10 seconds. On What’s on the Menu?, the language is carefully calibrated to ask for the patron’s help in improving a vital collection. • Demystify the purpose: people want to understand why their contribution matters. People do not get excited about typing, but they will get excited about participating in the building of an important historical research tool. The ‘About Us’ page for the menus project lists a number of its potential uses. • Make the task as small as possible: this is perhaps the most important lesson learned from our design phase – closely examine the action you’re asking people to do, and make the task as discrete as possible. If the project involves transcribing a page of text, ask participants to transcribe a sentence, or a line, or even a single word. Complicated tasks with multiple steps run the risk of being abandoned half-complete. • Encourage continuation: once the tasks are broken down into small enough pieces, completing one will feel like a bite-size morsel that leaves the participant wanting more. Thank the participant immediately for their contribution, then immediately ask them to contribute a bit more. • Show results immediately: this is another critical point. When you are collecting input from participants, do not send the fruits of their labour off to some unseen holding queue; instead, post the result proudly as completed. Every time a patron transcribes a dish from a menu, the name of that dish becomes a clickable link leading to a page showing all menus where that dish appears and some facts about it from the dataset (earliest and latest appearances, high and low prices, etc.). If the dish was incorrectly transcribed, it can always be corrected later; for us, it was far more important to treat the contribution as official the moment it was transcribed, allowing users to see their transcription become part of the research tool instantly. • Lower the barriers to participation: a registration page would have allowed us to reward top participants and track participation. But a sign-up page can also be a barrier that may discourage the casual participant. A far better approach is to allow immediate participation without registration, and passively communicate the benefits of signing up as the user proceeds. • Encourage a feeling of shared ownership: while critics of Wikipedia claim that anyone can enter false or vandalising information, it is heartening that the converse is true: anyone can also correct and repair bad information. By allowing other users to proofread and correct what has previously been entered, you give participants a sense of pride that this is their ‘neighbourhood’ and it should be looked after. • Place the project in context: do not try to be the centre of the universe – link to other reference sources. The page for every dish on What’s on the Menu? contains links to searches for that dish on other sites from Google to the restaurant-listing site MenuPages to recipe sites to the library catalogue, encouraging immediate exploration elsewhere. Although this was as simple to build as filling in the search terms for a variety of search engines, it helped to weave the project’s crowd-generated pages into the rest of the web. • Reward effort: if it is possible to keep track of who is participating, give rewards. Hold a special reception with refreshments in your library, and only give the invitation to online participants. If you are not tracking the identity of participants (as was the case in our project), make sure the messages in any public communications or interfaces are loaded with gratitude. • Report results: let users know how the project is progressing. If the goal is to transcribe a collection, show how many documents are in that collection, and how many have been completed. For What’s on the Menu?, a simple count of the number of dishes transcribed and the number of menus available and completed is prominently displayed on the home page, acting as an indicator of the health of the project. • Share the fruits of labour: if content is created by the public, the resulting product should be made publicly available. From the outset, one of the goals of What’s on the Menu? was to make the entire database available for download, and expose the data through an API to encourage anyone to create new interfaces for the data or explore via her own research. Promote any works that people derive from your data, and encourage others to do the same. • Finally, build a community: getting patrons involved in a project is an ideal opportunity to unite people with a common interest around your collections. Use social networks, a blog with comments and/or an online forum to build a conversation with the people who are your top users. Listen to feedback, take suggestions and highlight their interesting findings. Our advice to another library or archive contemplating such an experiment is to take a step back and look at their collections. What items are unique to your institution? Of those items, which have demonstrated (or in your estimation, have potential) public appeal? Is there a problem that the public can help you solve to improve access to this collection? It does not matter if your idea involves a relatively marginal collection. Be specific. Items of local interestare always good, as people inherently love to learn about where they live. Topics with a broad, universal appeal, such as food and genealogy, are always popular. NYPL’s menu archive, though a treasure, was hardly at the top of the institution’s collection development priorities, but it provided an excellent test-bed for a new kind of library work, and its influence continues to ripple through subsequent efforts. If you have an idea clearly in mind, look around for available tools. And don’t worry about gamification. The appeal of your collections and the call of your mission are your most valuable assets. We would not suggest that anyone set out to do a crowdsourcing project just to do one. Crowdsourcing is a technique for solving problems, and has no inherent value–the fact that a project is crowdsourced is not a guarantee of quality, success or innovation. Abetter approachis first to look at the projects of others (either the examples provided here or others) and participate in them. Make some corrections on Wikipedia. Crop a few images at the V&A. Get a sense for what makes you feel proud to have participated and motivated to continue. Having done that, look at your own library and see if there are collections or tasks which would benefit from the participation of others. Crowdsourcing projects cover a lot of territory. While you can certainly hire a programmer to design and build custom software for you, that is certainly not a necessity. Depending on the type of project, you can have success using simple, off the shelf software solutions that do not require you to do any programming of your own. A key component of crowdsourcing is that it is built around the cultivation of a community. Without a base of avid participants, you will not get anywhere. Our experience with What’s on the Menu? was that getting the software launched and tested was just the start of the project, while finding and maintaining our community, and keeping them supplied with fresh content, was – and remains – the much more challenging part of the job. The power of the web to capitalise on the contributions of its users is, as we speak, outstripping our ability to take advantage of that power, creating a vast, untapped resource as long as we have engaged, loyal patrons who want to help us. It can be expected that the number of successful examples of projects will grow accordingly, and that new innovations will make such projects increasingly easy to set up. Through our experience with What’s on the Menu?, we have learned that the challenges involved in creating and maintaining our project have been outweighed by the payoff of learning how to build a collection collaboratively with our patrons. References Julien, Jordan. ‘Cognition & the Intrinsic User Experience’, March 6, 2012. http:// uxmag.com/articles/cognition-the-intrinsic-user-experience. Lascarides, Michael. ‘Infomaki: An Open Source, Lightweight Usability Testing Tool’, November 23, 2009. http://journal.code4lib.org/articles/2099. McGonigal, Jane. Engagement Economy. Palo Alto, CA: Institute for the Future, 2008. National Endowment for the Humanities. ‘Announcing 6 Digital Humanities Implementation Grant Awards (July 2013)’, July 25, 2013. http://www.neh.gov/ divisions/odh/grant-news/announcing-6-digital-humanities-implementation.grant-awards-july-2013. Owens, Trevor. ‘Crowdsourcing Cultural Heritage: The Objectives Are Upside Down’, March 10, 2012. http://www.trevorowens.org/2012/03/crowdsourcing.cultural-heritage-the-objectives-are-upside-down/. Robertson, Margaret. ‘Can’t Play, Won’t Play’, October 6, 2010. http://www. hideandseek.net/2010/10/06/cant-play-wont-play/. Rouder, Jeffrey N., Richard D. Morey, Nelson Cowan, Christopher E. Zwilling, Candice C. Morey and Michael S. Pratte, ‘An Assessment of Fixed-Capacity Models of Visual Working Memory’. Proceedings of the National Academy of Sciences 105, no. 16 (2008): 5975–9; published ahead of print April 17, 2008. Shirky, Clay. Here Comes Everybody: The Power of Organizing without Organizations. New York: Penguin, 2008. This page has been left blank intentionally Chapter 6 What’s Welsh for ‘Crowdsourcing’? Citizen Science and Community Engagement at the National Library of Wales Lyn Lewis Dafis, Lorna M. Hughes and Rhian James Introduction: Digital Community Engagement at the National Library of Wales The National Library of Wales (NLW) is based in Aberystwyth, on the west coast of Wales. This chapter discusses three crowdsourcing projects at NLW: the Cymru1900Wales place name gathering project; The Welsh Experience of the First World War collecting project; and the potential for transcribing Welsh Wills Online. NLW is a legal deposit library, housing a collection of over six million books and the varied archives of Wales, including photographs, letters and other documents. It is also the home of the National Screen and Sound Archive of Wales. The Library’s content is in many languages, reflecting its establishment in 1907 as a National Library of and for the Welsh nation.1 As with the establishment of similar institutions elsewhere,2 the Library was intended as the preeminent repository of information for Wales, offering a world class collection of documentary heritage, including numerous rare, valuable and significant works, in order to bring international research collections to the people of Wales. The Library attracts approximately 80,000 physical visitors a year. These numbers are impacted by the relatively remote location of the Library, which lies in the midst of a sparsely populated part of the country with poor transport links. This has influenced the development of a freely accessible ‘Digital Library of Wales’, facilitated by mass digitisation of core collections to support access, preservation, research and education. Significant digital resources include newspapers published in Wales from 1700 to 1918;3 Welsh language and Welsh interest journals from 1840;4Welsh wills from 1600 to 1850; photographic archives, including the Geoff Charles and John Thomas collections; and manuscripts and other unique materials. Usage of, and engagement with, these digital collections has been successful and 1.Jenkins, Refuge in Peace and War. 2.Line and Line, ‘Concluding Notes’, 317–18. 3 Welsh Newspapers Online, welshnewspapers.llgc.org.uk. 4 Welsh Journals Online, welshjournals.llgc.org.uk. each year the Library attracts over two million unique online users. Another factor that privileges online use of digital content is the fact that Wales is a bilingual country, and online provision serves remote users in both Welsh and English.5 In 2011, the Library established a Research Programme in Digital Collections6 to develop an understanding of the use, value and impact of the digital collections of Wales for research, education and public engagement purposes. Research is carried out around existing and emerging digital resources, through engagement with national and international academic communities and existing and emerging communities of practice. The programme has integrated digital humanities tools, methods and communities in building sustainable digital resources that have an impact on scholarship. These approaches have also informed the development of new, strategic digitisation initiatives that address specific research needs, especially aimed at interoperability and the re-use of collections. The Welsh Experience of the First World War project7 exemplifies this approach: it is a collaboration between NLWand the special collections and archives of Aberystwyth University, Swansea University, Bangor University, Cardiff University and University of Wales Trinity Saint David. Collections from all partners have been digitised and made accessible through an integrated digital archive, virtually unifying disparate collections that are nonetheless complementary and that were, in some cases, originally physically located together. Akey theme that underlies the programme’s activities is the idea that the use of digital collections for research, education and teaching increases their value and impact and therefore makes their long-term sustainability more viable.8 Through a number of externally funded projects and the work of the PhD students in the research programme, work has also been carried out to investigate the role of digital collections in fostering engagement with the public and expanding education, training and cultural and knowledge exchange around the collections of the Library. This research into new ways of engaging with, and contributing to, the digital heritage of Wales fostered an analysis of the sort of crowdsourcing projects that could enhance community engagement, while contributing practical elements of work. The process began with an understanding of the term in the context of the Library and its mission and objectives, which are: ‘To collect, preserve and give access to all kinds and forms of recorded knowledge, especially relating to Wales and the Welsh and other Celtic peoples, for the benefit of the public including those engaged in research and learning.’9This goal is consistent with the definition of crowdsourcing most frequently cited, that of Jeff Howe, a contributing editor of Wired magazine. On his blog, Crowdsourcing: Why the Power of the Cloud is 5.Green, ‘Big Digitisation’. 6.http://www.llgc.org.uk/research. 7.http://cymru1914.org/. 8.Hughes, Evaluating and Measuring the Value, Use and Impact of Digital Collections, 12. 9.NLW, The Agile Library. Driving the Future of Business, he says that he devised the term in conjunction with Wired editor, Mark Robinson, to mean: the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. This can take the form of peer-production (when the job is performed collaboratively), but is also often undertakenby sole individuals. The crucial prerequisite is the use of the open call format and the large network of potential laborers.10 Crowdsourcing seeks to utilise the multiple perspectives of the crowd. It is rooted in the idea that the capability of ‘the crowd’, based on collective intelligence, collaboration and the aggregation of knowledge, is often better than that of the individual.11AWelsh translationof the term highlights this aspect. Cyfrannu torfol, which roughly means the ‘contribution of mass’or ‘collective contributions’, is an interesting term to use in the context of crowdsourcing in a library setting, as it is consistent with concepts of collectivism and mass digitisation. This reworking of the definition is also interesting in that it brings to the fore the concept of social engagement, an area in which libraries have a tradition of investment and success, enabling the public to communicate and collaborate, and to carry out tasks that add value to existing library collections. Examples of such tasks include user tagging, editing and adding content through volunteer programmes. Crowdsourcing uses social engagement methods to help achieve a focused, shared and large goal that would not be achievable without a collective approach.12 Utilising the breadth of knowledge, expertise and interests of the crowd is a central component of crowdsourcing. This is not, of course, a new method. It has been used for centuries to elicit mass labour of non-experts in pursuit of a specific task. Notable early projects include the contribution to the development of the metric system by de Prony’s Bureau du Cadastre in revolutionary France, which utilised a staff of 90 untrained ‘computers’overseen by a group of ‘planners’, who took basic equations for the necessary trigonometric functions and reduced them to simple arithmetical tasks for the computers to carry out. Asimilar approach was used in the development of the Mathematical Tables project from 1938, part of the Unites States Works Programme Administration (WPA).13 What has made the method of crowdsourcing more feasible on a large scale has been the development of Web 2.0, with the move from static web pages to the creation of an online environment that supports greater interactivity and communication that is both easier to manage and record. This has reflected the change in how the world wide web is used, rather than any fundamental change in the technology itself. In a short 10.Howe, ‘Crowdsourcing’. 11.Surowiecki, The Wisdom of Crowds. 12.Holley, ‘Crowdsourcing’. 13.Grier, When Computers Were Human. time, this has revolutionised the use of the web, especially through social media, into a participatory culture. The Joint Information Systems Committee (JISC) report, ‘Capturing the Power of the Crowd and the Challenge of Community Collections’, summarises the change in attitude towards content and data in light of these developments: Users are no longer happy to be passive consumers of information and content, but instead want to be engaged with, contribute and, most importantly, create content. This is evidenced by projects that have successfully deployed co-operation, including the creation and maintenance of an online encyclopaedia like Wikipedia;14 the formulation of government policy, such as the Red Tape Challenge;15 or as a commercial service, for example Mechanical Turk16 offered by Amazon. It has become an integral part of how users expect to interact with online services and contribute to their development. Developing a Crowdsourcing Initiative at the National Library of Wales The benefits of crowdsourcing have been discussed in greater detail by Rose Holley.17Although these benefits are identified in a library context, they can also be applied to projects associated with archives, museums and academic departments more generally. She lists a number of perceived advantages to crowdsourcing as an approach, which can be grouped as follows: • savings in costs, time and staff resources; • improvements to access and the quality of the resource; and • opportunities to engage user communities. Crowdsourcing provides organisations with the ability to achievegoals that would otherwise be impossible because of constraints placed on the time, finances and resources at their disposal. As demonstrated by projects such as Galaxy Zoo18 and FamilySearch Indexing,19the scale of contribution by volunteers can be far greater than what can be achieved by employees alone. This leads to improved access, which, in turn, enables more sophisticated and accurate searching by a more diverse range of users. 14.http://en.wikipedia.org/. 15.http://www.redtapechallenge.cabinetoffice.gov.uk/home/index/. 16.https://www.mturk.com. 17.Holley, ‘Crowdsourcing’. 18.http://www.galaxyzoo.org/. 19.https://familysearch.org/volunteer/indexing. An important incentive for crowdsourcing is its impact on user communities. The library and archives sectors have a strong tradition of working with volunteers. This is reflected in the commitment made by the Archives and Records Association (UK and Ireland) (ARA) to strengthen links between repositories and user communities to deliver efficient and sustainable services.20 Volunteering, community participation and social inclusion are ongoing priorities in NLW and Welsh government agendas and are activities that are seen to ‘add value’to the services they provide.21 Volunteers are enlisted across the sector to assist on a wide range of tasks, from indexing, transcribing and preservation to more recent online collaborations. The availability of archive collections, catalogues and indexes online has allowed for even greater engagement between repositories and volunteers.22 Through crowdsourcing, repositories are able to build communities of loyal and dedicated users, collaborate with a variety of audiences and provide increased opportunities for interaction between users and professionals. Such projects also encourage a sense of public ownership and responsibility to cultural heritage collections by allowing users to contribute their own content.23 It is important, however, that volunteers do not feel taken advantage of and that their efforts are appropriately acknowledged and rewarded through public recognition, ranking tables and so forth. Another way to acknowledge their contribution is to emphasise their potential impact on scholarly endeavour. Crowdsourcing forms a part of ‘citizen science’, where non-experts are able to participate in authentic scientific research alongside professional scientists. Science educators are increasingly recognising the value of carefully planned crowdsourcing projects to science education as a means of lowering the barriers to participation and raising awareness of scientific methods and research.24 It has also been argued that the crowd is, in many ways, more effective than the individual expert in generating ‘cleaner’data and in producing research results that may not have been possible otherwise.25 This can, in some instances, lead to new and unanticipated volunteer-led research. Crowdsourcing, therefore, is a methodology that enables the Library to offer a service responding to the needs and wishes of its current and potential users. But there are other reasons for the Library to consider the use of crowdsourcing. The general approach to openness and participation that has seen the rise of crowdsourcing draws very often on the belief that there is more knowledge and wisdom to be gained from the crowd than even from individual experts in a particular field. Surowiecki argues this powerfully in The Wisdom of Crowds: ‘under the right circumstances, groups are remarkably intelligent, and are often 20.ARA, ‘Volunteering’. 21.NLW, ‘Volunteering at the National Library of Wales’. 22.The National Archives, ‘Volunteering at The National Archives’. 23.Holley, ‘Crowdsourcing’. 24.Jordan Raddick et al., ‘Exploring the Motivations of Citizen Science Volunteers’. 25.Lintott, ‘Crowdsourcing in the Humanities’. smarter that the smartest people in them’.26 This is something that equally applies to the Library’s collections. From its inception, the Library has seen that individuals are willing to contribute their own knowledge and research to help others better interpret and understand its diverse and wide-ranging collections. What is new is the ability to capture these contributions and make them available to others through systems that the Library itself maintains. Crowdsourcing can thus be seen as the logical development of a long tradition of research and engagement based on the Library’s collections. Using the power of the crowd to add to the effort and productivity of its own professional and paraprofessional employees allows the Library to enrich the service it can offer to its users as well as enrich the collections themselves. Practically, the emergence of crowdsourcing methods has come at an auspicious time, when libraries and other cultural heritage organisations face unprecedented budget cuts in a time of fiscal austerity. There is a limit to the resources the Library has at its disposal to invest in describing and interpreting its collections, and some tasks are beyond what can be done by the Library within those constraints. The prospect of obtaining the contribution of the crowd to enrich library services, in a project that has a modest institutional investment of resources, is an alluring one. Part of developing and nurturing contributions by the crowd is the process of creating a community of enthusiasm around specific projects or tasks. This presents an opportunity to build new relationships with existing users by allowing them to take a proactive involvement in the Library’s collections. Cooperation with the crowd enables the Library to reach a wider audience beyond its traditional one, those who do not ordinarily engage with the Library or contribute in other ways. In their study of the use of crowdsourcing by libraries, archives, museums and galleries, Johan Oomen and Lora Aroyo27 offer a classification of the different types and models of crowdsourcing that they encountered (see Table 6.1). Many of these activities happen at present within libraries, including NLW. It could be argued that many of the Library’s resources have been collected over the years on the basis of crowdsourcing through donations and legacies by users and friends. In the same way, individuals and groups have undertaken projects to transcribe and contextualise our resources and have contributed the fruits of their labour to the Library and have shared them with other users. The projects described in the case studies below address several of these themes in a Web 2.0 environment, namely the Cymru1900Wales place name gathering project, which contributes to the classification of maps and related resources to build collaboratively a gazetteer; the community content generation exercise around First World War material that adds to collections; and the experiments around community transcription of wills to explore how the crowd could contribute to the transcription of Welsh Wills Online. All three projects demonstrate key 26.Surowiecki, The Wisdom of Crowds, xiii. 27.Oomen and Aroyo, ‘Crowdsourcing in the Cultural Heritage Domains’. Table 6.1 Models of crowdsourcing projects in libraries, archives and museums, based on work by Oomen and Aroyo Tasks relating to correction and Users correct and/or transcribe the products of transcription digitisation processes Adding further information about the context Contextualisation of a resource, e.g. by writing or collecting oral evidence about a resource Looking for additional resources that can be Adding to a collection included in an exhibition (physical or virtual) or a collection Collecting descriptive metadata about resources Classification in a collection. A common example would be social tagging Using the inspiration/expertise non-professionalCo-curation curators to create (physical or virtual) exhibitions advantages to crowdsourcing approaches in libraries, specifically improvements to access, savings in cost, time and resources, better community engagement and the generation of new research. Crowdsourcing Projects at NLW Cymru1900Wales: Crowdsourcing Welsh Place Names The Library’s first crowdsourcing project was launched in September 2012. Cymru1900Wales (www.cymru1900wales.org) is a classification project to gather the place names of Wales from the georeferenced 1900 Ordnance Survey maps. It is a collaborative project between NLW, the University of Wales, People’s Collection Wales and the Royal Commission on the Ancient and Historical Monuments of Wales. Digitisation of the Ordnance Survey 6 inch maps from c. 1900 was undertaken by staff from People’s Collection Wales, whilst the project’s crowdsourcing platform was developed by the team behind Zooniverse. The list of names generated through the project will lead to the development of a definitive georeferenced gazetteer of Welsh place names, which will form an important part of the digital research infrastructure for Wales. Using place names to ‘unlock’the digital collections hosted by the Library, including Welsh Newspapers Online and Welsh Wills Online, will enrich this content for research, teaching and public engagement, and will enable maps in library collections to be accessed in all sorts of new ways. Transcribing the place names in these maps will provide an incredibly valuable research resource for historians, linguists and family historians. Names of places are key to unlocking the social and linguistic history of the land. They can recall agricultural practices and local industries, changed landscapes and lost settlements. They also preserve a rich heritage of Welsh-language forms across the country and can be used to chart the arrival of English and illustrate interactions between both languages. Compiling an accurate, definitive georeferenced gazetteer of Welsh place names is a project that would take many years using traditional data gathering approaches. Harnessing the power of the crowd is a simple, innovative form of collection through which all of the names from the georeferenced maps are automatically geolocated, making them available for a wide range of digital applications. The Cymru1900Wales gazetteer will form the backbone of a national collection of the country’s historic place names, comprising everything from the earliest medieval records to field names still known to modern farming families. In addition, there is huge potential to tap into local knowledge and memory. In light of this, the project also has a facility to gather personal stories and memories associated with place, whether they be tales about the origin of the name, other versions of that name or recollections of the same place going by a different name altogether. The project has been successful to date and a large number of place names were generated within hours of the resource being launched. The interface includes a ‘league table’ of place names generated, which enables contributors to compete with one anotherfor the title of ‘top contributor’. Anecdotal evidence suggests that this is a popular tool. The project has been an excellent opportunity for the Library to experiment with crowdsourcing within the context of a relatively ‘safe’topic. Place and location are always popular themes and the resource also taps into the interests of family and local historians, two particularly active user communities. The data generated is checked by academic experts, who also interact with the community of users through email exchanges, and comments on the project blog and Facebook page. The project is an example of how crowdsourcing can be used to connect with a disparate, remote audience, using only computer-mediated communications. A diverse user group has signed up to use the resource. People can engage directly with the crowdsourcing platform, or use email, Facebook or Twitter to engage more directly with the project team. Aproject that enables greater interaction with new and remote audiences that does not require face-to-face, local engagement is a useful paradigm for the Library to use for future projects. Targeted Crowdsourcing to Contribute to The Welsh Experience of the First World War In 2013, the JISC-funded mass digitisation project based at NLW, The Welsh Experience of the First World War (cymru1914.org), developed a community engagement component in the form of five events around Wales carrying out what the project has referred to as ‘targeted crowdsourcing’. This involved requesting materials from members of the public to complement and enrich the developing digital resource that was based on material from various archives, libraries and special collections across Wales. An extensive preparatory scoping of potential content to be included in the mass digitisation project had indicated that the content held by the community could complement and enrich the emerging digital resource and so a small amount of project funding was set aside for the community outreach activities. This sort of community content generation is, of course, not a new approach. It was pioneered by the Oxford-based Great War Archive project,28 and has also been used by Europeana 1914–1829 as a distributed approach to gathering content.30 A Welsh project funded by JISC, Welsh Voices of the Great War Online,31 ran from the summer of 2010 to early 2011, gathering material from the Welsh public relating to the First World War. Content gathered from this project was extremely diverse and included contemporary letters and diaries; visual material, such as photographs and sketches; and physical memorabilia, from decorated items brought home from places such as Mesopotamia to German weapons picked up on the field of battle. This material was catalogued and made available via People’s Collection Wales, a successful initiative funded by the Welsh government to build an online ‘People’s Museum of Wales’from community generated content, and to promote digitisation skills and information literacy around the country.32 What was different about the approach taken by The Welsh Experience of the First World War was the emphasis on actively seeking documentary heritage materials that would directly complement collections held in archives and record offices. An example of this sort of material is chapel records. Incomplete archives of these materials are held by institutions including NLW. As these records were often sent to each member of a congregation, there was a sense that the community could comb their attics and family papers to complete the official holdings of these and related materials. In order to accomplish this, the People’s Collection team arranged five events across Wales on behalf of the project to encourage people to bring items to be digitised and shared online. The team gave examples of the sort of materials that could complement the official records and encouraged the public to target these materials. The events took place at Galeri Caernarfon, Pontardawe Arts Centre, Brecon Library, Ruthin Library and Picton Community Centre, Haverfordwest, and all were full-day events. Before each workshop, publicity was sent out to relevant press and media contacts; posters were sent to each venue; People’s Collection staff contacted local heritage institutions, including libraries, archives and museums, and shared information and posters; local history societies and groups were informed of the events; adverts were placed in local papers; and contact 28.http://www.oucs.ox.ac.uk/ww1lit/gwa. 29.http://www.europeana1914–1918.eu/en. 30.Hopkins, ‘Physical and Online Crowdsourcing Documents the Real First World War’. 31.http://www.jisc.ac.uk/whatwedo/programmes/digitisation/content2011_2013/ welshww1.aspx. 32.http://www.peoplescollectionwales.co.uk/. was made with members of the Western Front Association, British Legion and Royal Welsh Fusiliers. An average of 10 people attended each event, and the total amount of digital content created was over 700 digital images, approximately 350 items. These items were varied and diverse in nature. Interestingly, many of the contributors focused on documentary heritage items, while the call for items relating to the whole of the Welsh experience of the War on the home front, on the attitudes of those left behind and how they dealt with the experience, went largely unheard. The majority of items contributed related to soldiers and their involvement in the War. This indicated that, to succeed in getting highly specialist materials from a community engagement exercise of this nature, the marketing and promotion would need to have been far narrower and directed at likely sources of the required materials, casting the net of target locations for the events far wider to include, for example, chapels or denominational bodies. Another approach would have been to work with local record offices to identify significant gaps, and have archive services assist in gathering material on an ongoing basis. One project that has managed to strike this balance is the JISC-funded My Leicestershire project,33 which took a targeted approach – sourcing local history material about Leicestershire from selected local heritage organisations – that was also highly mediated, working with academics to identify content of interest. Nonetheless, a number of very interesting items emerged from the ‘roadshow’ events and valuable materials were obtained. One of the most significant finds was a body of material from MI7b, a military propaganda outfit, which was brought to the workshop in Brecon by its owner and has attracted great attention in the media.34In another collection, a diary by a Frank Sheppard from Caerphilly gives a detailed account of his trainingafter joining the 11th Reserve Cavalry Regiment at Tidworth, 1915–17, giving an important insight into how Welsh soldiers were trained, and complementing the official records. Other material of interest, again from Brecon, related to the life of the pioneering Dr Mary Elizabeth ‘Eppynt’ Phillips, a relative of the owner who brought it to the roadshow. Dr Phillips was born in Merthyr Cynog, Powys, and led a life of adventure and service as a doctor at home and abroad. She was the first woman to qualify as a doctor from Cardiff University College at the turn of the century and became known as Mary ‘Eppynt’Phillips, taking the name from the mountain near where she was born. Her involvement in the First World War was extensive. She was involved in running the Typhoid Hospital at Calais before joining the 2nd Serbian Unit as a Senior Physician at the Scottish Women’s Hospital at Valjevo, Serbia, in April 1914. She undertook a lecture tour in Britain to raise funds for this Women’s Hospital. Returning to the continent in April 1916, she travelled to Corsica where she was the Chief Medical Officer at the hospital in Ajacci on the 33.http://www.jisc.ac.uk/whatwedo/programmes/digitisation/communitycontent/ leicestershire.aspx. 34.See Flood, ‘Winnie the Pooh Author AAMilne Was First World War Propagandist’. See also Milne, ‘Winnie the Spook’. Source: image contributed to The Welsh Experience of the First World War by Sandra Crane. By permission of NLW. island, until 1917. Dr Phillips is mentioned in literature about the involvement of women in the First World War but the collection of additional items (including photographs and telegrams) until now hidden in a family collection, contribute significantly to this knowledge. Organising these events with People’s Collection Wales, which has a good deal of expertise in this area, was key to their successful organisation and outputs. Four members of staff were required at each event to ensure that contributors could be dealt with efficiently. Metadata and rights information were also gathered as part of the process. Another important aspect was that representatives from local organisations and societies were invited to attend each event, creating partnerships and making use of their existing local networks and expertise. Local records offices were important partners, attending several events with displays about their work and providing advice to the public on the storage and preservation of their materials. Some contributors also deposited their items at their local repository following their digitisation. Finally, academics from the project board provided valuable support and advice before and during the events. This support for organisation emphasised the first rule of digital engagement: an enormous amount of analogue-human-effort needs to go into the preparation and support of community engagement activities of this nature if they are to be a success.35 Welsh Wills Online The National Library of Wales holds a collection of over 190,000 handwritten wills and associated records, including inventories, letters of administration and accounts, which were proved in the Welsh ecclesiastical courts before the introduction of civil probate on 11 January 1858. These date from the late-sixteenth century onwards and cover most of Wales, excluding 15 border parishes that fell under the jurisdiction of the Episcopal Consistory Court of Hereford. Wills frequently comprise several folios, so the total archive consists of approximately 800,000 pages. Several hundred wills in the collection are in the medium of Welsh. Between 2000 and 2008, the collection was scanned in its entirety and page images of the wills are now freely accessible online via the NLW website.36 The images are accompanied by an index and limited metadata, enabling the wills to be searched by diocese, parish, date, testator and his/her occupation. Full-text searching and browsing of the digital collection is, however, not available. Optical Character Recognition (OCR) software cannot currently be successfully applied to a collection of this nature because of the number of hands present and the complexity of the material. The Library is, therefore, consideringa crowdsourcing approach to the transcription of the wills to harness community interest and effort, making the full text of the documents available for use and re-use and to support widest resource discovery. Probate records hold great potential as research resources, provided that they can be made more accessible and useable for a wider audience. The NLWcollection is rich in content and is an important source of information on significant aspects of social, cultural and economic history from a national perspective. Wills are currently used extensively by family and house historians, by local and community historians and by historians and researchers across the disciplines for a multitude of purposes. They provide researchers with important insights into a wide variety of topics, including demographics and population change; family, kinship and patterns of inheritance; business networks; material culture; historical linguistics and literacy levels. Understanding future use – anticipated and unanticipated – of the content is key and articulating the main humanities research questions that will help us to access the collection using digital methods is central to this. Being able to browse and search across this material as a full text corpus, and cluster results, would have tremendous research impact. A fully transcribed and encoded collection would facilitate searching and analysis over the entire archive, thus introducing the material to new audiences, encouraging new research and opening up the history of Welsh citizens over a 35.Brant et al., ‘Strandlines’. 36.http://cat.llgc.org.uk/cgi-bin/gw/chameleon?skin=profeb&lng=en. period of significant societal change and a vast geographical area. The Library is currently investigating and scoping approaches to creating a full text corpus of the resource, and user requirements for making this collection available for research, teaching and public engagement. This research includes developing an understanding of the technical and historical issues that arise in communicating such content in a digital environment, applying structured mark-up to the text and making it accessible through a user-friendly interface that enables searching, browsing and the implementation of ICT tools and methods for detailed content analysis. This process has included discussing these issues with the extensive community of users already using the wills for research and understanding how their use of the resources could inform the development of the online resource. In 2013, as part of a University of Wales PhD research, interviews were carried out with a representative selection of the communities who have relied on the wills as primary evidence. This included economic and cultural historians, local and family historians and archivists and information professionals. This process helped ascertain how they have used wills and probate records in the course of their research and what difficulties they have encountered in accessing such material. The work has also been a valuable opportunity to assess the feasibility of crowdsourcing in developing transcriptions of the content. Comparatively little work has been done to date on the viability of such an approach in relation to archives and manuscripts, so the intentionwas to assess the practicalities of crowdsourcing the transcription of the wills, particularly in light of some of the challenges posed by the source material, and the potential overhead in supporting a transcription project of this nature and scale.37Several projects implementing crowdsourcing in an archival setting, for example, the Papers of the War Department, 1784–180038 project at George Mason University (see Chapter 4 in this volume), have noted that crowdsourcing creates new work for existing staff.39 Crowdsourcing initiatives must, therefore, ensure that the rate and quality of transcription is sufficient to compensate for the time and effort spent by staff in checking, moderating and maintaining contributions. The feasibility of crowdsourcing as an approach to manuscript transcription relies heavily on its effectiveness and the ratio between volunteer output and cost. The user study led to some interesting findings. A proportion of the wills have already been transcribed by various researchers and research teams. This includes full transcriptions of all Welsh language wills in the collection, which was undertaken by a professional historian, and wills from the Machynlleth area in Powys by a local research project, which included several archivists and experienced researchers. Initial enquiries have shown that numerous 37.SeeTranscribe Bentham (http://blogs.ucl.ac.uk/transcribe-bentham/), also discussed in Chapter 3 and Ancient Lives (http://ancientlives.org/) for examples of crowdsourced manuscript transcription projects. 38.http://wardepartmentpapers.org/. 39.Zou, ‘Civil War Project Shows Pros and Cons of Crowdsourcing’. local history and antiquarian groups, family history societies and enthusiastic individuals have already carried out transcriptions of wills relating to their own families or areas of interest. It is important that any transcription project engages with these existing user communities and provides a platform through which their contributions can be easily incorporated. The development of a ‘meta.crowdsourcing’ project along these lines will allow for existing transcripts to be gathered and converted into suitable formats for inclusion and integration into a full corpus of the material. This will be an ongoing process of building relationships and developing links, including continued engagement with researchers working on this material for current projects. Crowdsourcing as an approach to manuscript transcription was regarded with a degree of caution by many of the users interviewed. One participant commented: ‘I do worry about crowdsourcing … I like the idea in principle but worry that the outcomes might not be brilliant.’ Issues of accuracy and trust in the product were repeatedly raised, particularly when considering the difficulties associated with accessing the content. The wills, especially those dated pre.1750, are complex in nature, and present particular challenges with regards to legibility and comprehension. All participants raised questions as to whether the supposedly inexperienced and untrained public, regardless of their enthusiasm and good intentions, would be able to transcribe the wills to a sufficiently high standard without the necessary skills and support. Some of the problems identified by participants in using and transcribing the wills include the difficult and inconsistent handwriting; archaic, regional and specialist terminology; the presence of Welsh and Latin throughout the collection; unfamiliarity with Welsh personal, place and field names, and patronymics; the expansion of abbreviations; a lack of understanding of the diplomatic structure and formulaic nature of the wills, and their physical condition. Such difficulties were believed to impact adversely levels of accuracy and brought into question the quality of contribution and the effectiveness of such an approach in relation to the cost, time and effort expended in developing a platform and checking submissions. Nonetheless, communities of specialist transcribers are available.40 Despite their scepticism, the majority of interviewees acknowledged that users exist who would be both interested in and capable of carrying out transcriptions via a crowdsourcing platform, provided that their skills and interests are suitably considered and that appropriate support was provided. Several users noted that dividing the collection according to date would greatly facilitate the process of transcription. As Figure 6.3 illustrates, the early wills are generally more difficult to read and understand in comparison to the more standardised wills of the late-eighteenth and early-nineteenth centuries. It was suggested that it may be possible to directtranscribers with some palaeographical experience to those more difficult, early wills, leaving the less experienced to gain familiarity and develop their skills working on the later part of the collection. This implies a more directed 40.Ebner et al., ‘Virtual German Charter Network’. Figure 6.3a Part of will of David ap John ap John, 1609, St Asaph Probate Records, SA1609–96 Source: NLW. Figure 6.3b Part of will of John Scurlock, Waungaled, Abergwili, Carmarthen, 1851, St David’s Probate Records, SD1851–276 Source: NLW. approach to crowdsourcing, whereby the expertise, background and interests of contributors are taken into account. It is also focused on the development of skills, with support potentially being provided from NLWand the wider user community, including scholars, experienced researchers and related organisations. Such an approach is not so much crowdsourcing but, as the Medici Archive project has described, ‘community-sourcing’, based on levels of hierarchy and a sense of shared responsibility.41 Welsh Wills Online is a potentially important space where communities can not only contribute to knowledge production but also gain a greater understanding of the historical content through an active and meaningful engagement with the material itself. Transcription of the content will be challenging. However, there is much to be gained in pursuing crowdsourcing as an approach, as long as expectations about what is achievable via community-generated transcription remain realistic and practical. The lessons learned from projects like Transcribe Bentham show that this is not an easy solution for rapid generation of high-quality transcriptions.42 Instead, it should be seen as part of a process for the Library to engage better with its distributed user community, rather than directly expecting an easily quantified ‘product’. Developing a platform that will enable user groups to transcribe the content, and have that transcription contribute to the creation of knowledge, fulfils both the mission of the Library and the goal of digitising content,namely enabling greater engagement with primary sources than was previously possible, and making history accessible by inviting students, researchers, teachers and the public to investigate, understand and experience the past.43 Employing crowdsourcing methods for the Welsh Wills project is an important opportunity to democratise the research process and encourage use and interaction with the collection in ways that were not possible before. Conclusions: Successful Implementation of Library Crowdsourcing Projects The case studies outlined above have been early experiments in the Library’s application of crowdsourcing approaches in its work. They do suggest that NLW is well placed to take advantage of crowdsourcing a number of tasks to our distributed audience in a way that helps to overcome our geographic location. Other prospective crowdsourcing projects have been identified in the categories outlined in Table 6.1 above. Contextualisation can be achieved through the identification of individuals in the D.C. Harries collection of photographs of First World War soldiers. Correction would enhance the OCR in Welsh Newspapers Online. Many of the Library’s collections, including the David Lloyd George archive, could be enhanced by an initiative to source materials from elsewhere 41.Allori and Kaborycha, ‘Opening Aladdin’s Cave or Pandora’s Box?’. 42.Causer et al., ‘Transcription Maximized’. 43.Owens, ‘Crowdsourcing Cultural Heritage’. in order to add to the collection. Classification of images in Welsh Newspapers Online would again enhance the use of the digital collection. Finally, co-curation could be carried out by expanding the targeted crowdsourcing approach to collect materials specifically for an online exhibition or teaching resource. There are, of course, a number of concerns that must be incorporated into the planning and adoption of all Library crowdsourcing projects, including managing the overall quality and accuracy of contributions; managing copyright and intellectual property rights; overcoming issues of ‘trust’; planning the technological approach to be used, recognising that there is no ‘one size fits all’tool for all the different types of projects described above; and managing, supporting and engaging with ‘the crowd’as an ongoing process. Resources need to be allocated to these aspects of any crowdsourcing project. However, these potential challenges, if considered at the outset, are the key to successful projects. Afundamental observation is the fallacy that crowdsourcing requires fewer resources to achieve similar or even better results than would be possible with existing Library resources. In order for projects to succeed a number of elements must be in place that require a resource commitment, including: • careful planning of the project from the start; • creation of communities of interest around a specific project or task; • support for those communities until the tasks are completed, remembering that it is possible for individuals to leave the project any time they choose to do so; • managing those who can contribute; • training individuals for very specific tasks within a project; • evaluating, moderating and editing contributions but at the same time ensuring that individuals remain engaged with the project or the tasks to be undertaken; • selecting and implementing suitable technology platforms; and • developing specific Library workflows to manage contributions in accordance with the different models of crowdsourcing, and integrating them into the Library’s existing workflows. However, this investment can pay off for clearly defined projects –not necessarily always in terms of getting quantifiable outputs from volunteers, but in effecting transformation of the experience of immersive interaction with library collections, and encouraging the public to collaborate in the production of new knowledge around library collections. Crowdsourcing has the potential for libraries to build new relationships with their audiences, and in doing so, to add to the value of their collections and to the organisation as a whole. References Allori, L. and L. Kaborycha. ‘Opening Aladdin’s Cave or Pandora’s Box? The Challenges of Crowdsourcing the Medici Archives’. Abstract of paper presented at the Digital Humanities 2013 Conference, University of Nebraska-Lincoln, July 17, 2013. http://dh2013.unl.edu/abstracts/ab-312.html. Archives and Records Association (ARA) (UK and Ireland). ‘Volunteering’, 2013. http://www.archives.org.uk/campaigns/volunteering.html. Brant, C., S. Dunn, D. Green, P. Methven and H. Wolf. ‘Strandlines’, 2011. http://www.jisc.ac.uk/media/documents/programmes/digitisation/strandlines_ finalreport.pdf. Causer, T., J. Tonra and V. Wallace. ‘Transcription Maximized; Expense Minimized? Crowdsourcing and Editing The Collected Works of Jeremy Bentham’. Literary and Linguistic Computing 27, no. 2 (2012): 119–37. http:// llc.oxfordjournals.org/content/27/2/119. Ebner, D., J. Graf and M. Thaller. ‘Virtual German Charter Network: A Virtual Research Environment for the Handling of Medieval Charters’, 2011. http:// www.hki.uni-koeln.de/sites/all/files/VdUSDH11–02.pdf. Flood, A. ‘Winnie the Pooh Author AAMilne Was First World War Propagandist’. Guardian, April 26, 2013. http://www.theguardian.com/books/2013/apr/26/ milne-first-world-war-propaganda. Green, A. ‘Big Digitisation: Where Next?’. Paper presented at the Digital Resources for the Humanities and Arts Conference, Belfast, September 8, 2009. http:// www.llgc.org.uk/fileadmin/documents/pdf/darlith_big_digitisation_where_ next.pdf. [Published in revised form as ‘Big Digitisation: Origins, Progress and Prospects’. International Journal of Humanities and Arts Computing 4, no. 1–2 (2010): 55–66.] Grier, D.A. When Computers Were Human. Princeton, NJ: Princeton University Press, 2006. Holley, R. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3–4 (2010). http://dlib.org/dlib/march10/holley/03holley. html. Hopkins, C. ‘Physical and Online Crowdsourcing Documents the Real First World War’. ReadWrite, June 16, 2011. http://readwrite.com/2011/06/16/combining_ physical_and_online_crowdsourcing_to_doc. Howe, J. ‘Crowdsourcing: A Definition’. Crowdsourcing: Why the Power of the Cloud Is Driving the Future of Business blog, June 2, 2006. http:// crowdsourcing.typepad.com/cs/2006/06/crowdsourcing_a.html. Hughes, L.M., ed. Evaluating and Measuring the Value, Use and Impact of Digital Collections. London: Facet, 2011. Jenkins, D.A. Refuge in Peace and War: The National Library of Wales to 1952. Aberystwyth: National Library of Wales, 2002. JISC. ‘Capturing the Power of the Crowd and the Challenge of Community Collections’, 2010. http://www.jisc.ac.uk/media/documents/publications/ programme/2010/communitycollectionscrowd.pdf. Jordan Raddick, M., G. Bracey, P. Gay, C. Lintott, P. Murray, K. Schawinski, A. Szalay and J. Vandenberg. ‘Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9, no. 1 (2009). http://aer.aas.org/ resource/1/aerscz/v9/i1/p010103_s1. Line, M.B. and J. Line. ‘Concluding Notes’. In National Libraries, 315–19. London: Aslib, 1979. Lintott, C. ‘Crowdsourcing in the Humanities’. Paper presented at the Digital. Humanities@Oxford Summer School, University of Oxford, July 2, 2012. Milne, A.A. ‘Winnie the Spook’. Harper’s Magazine, September 2013. http:// harpers.org/archive/2013/09/winnie-the-spoke/. National Archives, The. ‘Volunteering at The National Archives’, n.d. http://www. nationalarchives.gov.uk/documents/volunteering-at-the-national-archives.pdf. National Library of Wales (NLW). ‘Volunteering at the National Library of Wales’, 2010. http://www.llgc.org.uk/fileadmin/documents/pdf/2010VolunteeringPolicy. pdf. National Library of Wales (NLW). The Agile Library: The Library’s Strategy 2011–2012 to 2013–2014. Aberystwyth: National Library of Wales, 2011. http://www.llgc.org.uk/fileadmin/documents/pdf/Strategy2011–12_2013–14. pdf. Oomen, J. and L. Aroyo. ‘Crowdsourcing in the Cultural Heritage Domains: Opportunities and Challenges’, 2011. http://www.cs.vu.nl/~marieke/ OomenAroyoCT2011.pdf. Owens, T. ‘Crowdsourcing Cultural Heritage: The Objectives Are Upside Down’. Trevor Owens: User Centred Digital History, March 10, 2012. http://www. trevorowens.org/. Surowiecki, J. The Wisdom of Crowds: Why the Many Are Smarter than the Few. London: Little, Brown, 2004. Zou, J.J. ‘Civil War Project Shows Pros and Cons of Crowdsourcing’. Chronicle of Higher Education, June 14, 2011 (updated June 21, 2011). http://chronicle. com/blogs/wiredcampus/civil-war-project-shows-pros-and-cons-of.crowdsourcing/31749. This page has been left blank intentionally Chapter 7 Waisda?: Making Videos Findable through Crowdsourced Annotations Johan Oomen, Riste Gligorov and Michiel Hildebrand As galleries, libraries, archives and museums have been publishing their vast collections on the web, they are discovering that the scale of their collections raises new issues: users cannot find things that they are searching for in big collections that lack adequate descriptions; where descriptions exist, they are often written from a specialist perspective. This creates a very big opportunity for crowdsourcing, not only to source more metadata (descriptions of collection objects) but also to increase end-user engagement. The Netherlands Institute for Sound and Vision, one of Europe’s largest audiovisual archives, and VU University Amsterdam have developed the social tagging game Waisda? that pairs up users and lets them compete by tagging the same video simultaneously, awarding points for both speed and accuracy of tags. This chapter presents the design decisions behind the game and elaborates on results of extensive evaluations carried out in this long-term research project from 2009 to 2013. The research included two large-scale pilots involving thousands of end-users. Over one million tags have been contributed to date. This work demonstrates the potential impact of participatory culture on the daily operations of institutions, their business models and end-user engagement. Introduction The web is increasingly social. Clay Shirky notes that the concept of ‘cyberspace’, where computers and networks are regarded as somewhat alien, is now disappearing: ‘Our social media tools aren’t an alternative to real life, they are part of it’. He adds that [these tools] ‘are increasingly used as the coordinating tools for events in the physical world’.1 Henry Jenkins notes that ‘the growth of networked communication, especially coupled with the practices of participatory culture, provides a range of new resources and facilitates new interventions for a variety of groups who have long struggled to have their voices heard’.2 New platforms create openings for social, cultural, economic legal and political change.3 This 1.Shirky, Cognitive Surplus, 37. 2.Jenkins, Spreadable Media, xiv. 3.Johnson, Future Perfect. has an enormous impact on present day society.4 On the web, users are active creators, creating and sharing for instance stories, photographs and videos. These are examples of the transformation of the web to what Howard Rheingold calls the ‘social web’.5 Galleries, libraries, archives and museums (abbreviated hereafter to ‘GLAMs’) are also making use of the opportunities presented by the social web. The mass digitisation of analogue holdings creates the potential for GLAMs to become an integral part of the web. In the case of fragile media (such as magnetic tapes and chemical film) digitisation is a means to ensure long-term preservation of the information. Digitisation is also a precondition for creating new access routes to collections. Once published on the web, cultural artefacts can be shared, recommended, remixed, mashed, embedded and cited. Collections become an integral part of what Tim Berners-Lee calls the Giant Global Graph6 by adding metadata to information objects such as web pages and images to enable links, and creating the relationships that conceptually or semantically link the information objects to each other. Through publication and linking online, attention can be brought to even the most obscure artefacts. One of the unique properties of GLAM collections is their richness in the breadth and variety of objects and topics they cover, and the quality of contextual data about them. The web provides the opportunity for this richness to surface and to satisfy needs not only based on popularity, but also based on ad hoc interests. In an online context where sharing is the norm, it becomes almost a necessity for GLAMs to make their collections available online in order to retain and support community interest. In effect, GLAMs and their audiences are now also part of what Abraham Bernstein et al. call the ‘global brain’, the intelligent network formed by users, together with the information and communication technologies that connect them.7 New services are being launched that explore opportunities this brings to GLAMs.8 This chapter highlights a case study in the audiovisual heritage domain. The next section ‘The Social Tagging of Audiovisual Heritage’ positions the project in the wider context of user engagement in GLAMs and subsequently focuses in on audiovisual heritage. In ‘The Waisda? Video Labelling Game’, we discuss relevant related work and elaborate on the design of the Waisda? video tagging game. The ‘Evaluation of the Pilots’ section provides an overview of the main findings of the continuous evaluations conducted in the period 2009–2012. The final two sections respectively summarise the main results and outline the future work research agenda. 4.Rushkoff, Present Shock. 5.Rheingold, The Virtual Community. 6.Berners-Lee, ‘Giant Global Graph’. 7.Bernstein et al., ‘Programming the Global Brain’, 41. 8.Van Den Akker et al., ‘From Information Delivery to Interpretation Support’. Waisda?: Making Videos Findable through Crowdsourced Annotations 163 The Social Tagging of Audiovisual Heritage This section elaborates on the ways GLAMs are exploring crowdsourcing, and how one specific type of crowdsourcing, social tagging, has been applied to meet concrete challenges of audiovisual archives online. The term outsourcing – finding labour outside the organisation – has been redefined on the web as the crowdsourcing phenomenon: ‘the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call’.9 One key attribute is that most crowdsourcing activities are small (micro-) tasks that can be carried out by large numbers of people. GLAM crowdsourcing initiatives today also aim to have long-lasting effects on the way institutions operate. For instance, users are being challenged to add tags to collections, help with transcribing historical texts, share contextual knowledge in collaborative wiki environments and so on. Crowdsourcing has a profound impact on the workflows of heritage institutions through identifying micro-tasks that can be outsourced to the crowd.10 These activities can be carried out by end-users remotely and can reduce operational costs. New forms of usage of collections (beyond access) can also lead to a deeper level of involvement with the collections.11 One of the key success factors for these practices is shaping and executing them so that both the users and the institutions find them beneficial. In earlier work, the authors have analysed a great variety of cultural heritage crowdsourcing projects, and defined a classification of six types of crowdsourcing (Table 1).12 Table 7.1 Classification of crowdsourcing initiatives Type Description Correction and transcription Inviting users to correct and/or transcribe outputs of tasks digitisation processes Contextualisation Adding contextual knowledge to objects, e.g. by telling stories or writing articles/wiki pages with contextual data Complementing collection Assembling additional objects to be included in a (web) exhibit or collection Classification Gathering descriptive metadata related to objects in a collection. Social tagging is a well-known example Co-curation Using inspiration/expertise of non-professional curators to create (web) exhibits Crowdfunding Collective cooperation of people who pool their money and other resources together to support efforts initiated by others .Howe, ‘Wired 14.06’. 10.Lankes et al., Participatory Networks. 11.Huvila, ‘Participatory Archive’. 12.Oomen and Aroyo, ‘Crowdsourcing in the Cultural Heritage Domain’. The Digital Content Life Cycle model used by the National Library of New Zealand serves as the baseline model for mapping these types to common work processes at GLAMs.13 This model encapsulates the main activities carried out by heritage organisations, from selecting to creating, managing, discovering, using and reusing (including licensing) to preservation. The model is cyclical, but it needs to be noted that the order can often vary in daily practice. For instance, the cataloguing process of adding descriptive metadata to heritage objects (i.e. Classification) is often executed in consecutive phases, as part of the initial ingest and when reused later in different contexts. As the Waisda? use case will show, metadata can also be added by end-users as part of publishing collections online. Mapping the stages in the Digital Content Life Cycle model and the types of crowdsourcing (Figure 7.1) shows that crowdsourcing can play a role in all stages of the model: from selection and creation of content to describing, discovery and use. This underlines the enormous potential of crowdsourcing for GLAMs. The Waisda? project is an example of ‘Classification’, in this context often also referred to as ‘social tagging’. For clarity’s sake, the term ‘social tagging’ is used 13.Make it Digital – DigitalNZ, http://www.digitalnz.org/make-it-digital. Waisda?: Making Videos Findable through Crowdsourced Annotations 165 throughout this chapter. Since the first experiments in this field by steve.museum,14 Flickr Commons15 and others, social tagging has become a common practice in GLAMs, who are exploring how to establish the emerging participatory practices as a permanent part of their assets management workflows. In the next section, we elaborate on the potential of social tagging, by focusing on the mutual benefits for GLAMs and their audiences. Motivational Factors for GLAMs In analysing the drivers of social tagging, we distinguish between motivations for participation for GLAMs and for end-users. Four motivations for GLAMs to initiate or engage in social tagging projects can be identified:16 1. Bridging the ‘semantic gap’ between the terminology used by professionals and the search terms used by audiences. Professional cataloguers are familiar with the structure and contents of the vocabularies used by the heritage organisations, particularly specialist thesauri. However, end-users are not aware of the structure and content of these, and thus have a disadvantage in searching. For instance, someone might query for ‘stapler’ but receive no results if the ‘broader’concept ‘office supplies’was used by the cataloguer. This difference between the terms that professionals assign from a controlled vocabulary and the search terms that the general public uses in search queries is called the semantic gap.17 Jgensen writes: ‘While natural language composes a searcher’s query, indexing languages typically employ highly precise and specific terms relevant to the community that uses the indexing language.’18 So, one way of measuring the existence of this gap is by determining the information need of end-users through the analysis of query logs of information retrieval systems, in which the terms entered by users are kept (e.g. looking at orders for content from audiovisual catalogues). By comparing the differences between the terms from these query logs to the keywords used by professional cataloguers the semantic gap can be determined. 2. Contributing additional factual and contextualised information. A community can cumulatively be smarter than a selective group of experts. James Surowiecki underlines this statement in his book The Wisdom of Crowds. Individually, the ‘masses’ will not know more than the trained experts, because they simply do not have the experience, or make irrational 14.Steve.Museum | Steve: The Museum Social Tagging Project, http://www.steve. museum/. 15.Flickr: Commons, http://www.flickr.com/commons. 16.Oomen et al., ‘Emerging Practices in the Cultural Heritage Domain’. 17.Melenhorst et al., ‘Tag-Based Information Retrieval for Educational Videos’. 18.Jörgensen, ‘Image Access’. choices based on emotions, but as many case studies and experiments that Surowiecki describes have proven, ‘despite all these limitations, when our imperfect judgements are aggregated in the right way, our collective intelligence is often excellent’.19 Thus, even though experts can provide a good array of keywords based on traditional taxonomies, allowing the general public to tag can result in even better keywords since cumulatively there is more knowledge in the world than in an archive itself.20 3. Defining the future workflow of memory organisations. User annotations can play an important role in the way memory organisations manage their information, and more specifically, the cataloguing processes. The influx of born-digital information is growing at an immense pace and at the same time, large-scale digitisation projects are resulting in more accessible collections.21 However, these collections often lack the supporting metadata that allow users to find them. Institutions often do not have the resources to add metadata manually to all their material to the level of detail required to fulfil every potential information need, especially in the case of digitised audiovisual heritage where adding metadata to time-based media is a particularly time-consuming task. While it varies according to the level of detail in the description, a cataloguer spends one hour for each hour of audiovisual material when describing it in some detail. Thus, since the temporal nature of the material adds a level of complexity, it is a lot more work to provide ample metadata for audiovisual materials than for static objects. Therefore archives are seeking alternative ways to create annotations, for instance by using technologies such as speech recognition, and collaborating with their end-users.22 These technologies can be complementary; speech recognition for instance can be used to find quotes, whereas tags can describe objects visible on-screen. 4. Increasing connectedness between audiences and the archive. GLAMs are aware of the power of the social web and create services that aim to connect to their online constituencies. Hasan Bakhshi identifies several broad categories of innovation that are common to cultural institutions.23 One of the more fundamental innovations is the creation of value for end-users. Tagging is an excellent way for heritage organisations to tap into the enthusiasm of external users.24 For instance, they can create tagging activities in the form of games that are fun to play, or encourage 19.Surowiecki, The Wisdom of Crowds, xiv. 20.Van Den Akker et al., ‘From Information Delivery to Interpretation Support’. 21.Huurnink et al., ‘Search Behavior of Media Professionals at an Audiovisual Archive’. 22.Kemman et al., ‘Who Are the Users of a Video Search System?’. 23.Bakhshi, Culture of Innovation. 24.Trant, ‘Tagging, Folksonomy and Art Museums’. Waisda?: Making Videos Findable through Crowdsourced Annotations 167 participation by affirming to users that they are part of a highly valued community of experts. Motivations for End-Users to Participate in Social Tagging Projects Mutual connections (as mentioned above) already link benefits for memory organisations and more direct benefits for end-users. Motivations for users to engage in crowdsourcing vary considerably and may be either extrinsic or intrinsic.25 Intrinsic motivation is built on the premise that users and memory institutions have a mutual interest in collaborating on a project or campaign, based on a common set of values. On the merits of tapping in to intrinsic motivation, Clay Shirky notes: ‘Amateurs are sometimes separated from professionals by skill, but always by motivation; the term itself derives from the Latin amare – to love. The essence of amateurism is intrinsic motivation: to be an amateur is to do something for the love of it.’26 As such, it fosters profound engagement.27 Shirky also points to the work of Benkler and Nisembaum who ‘divide social motivations into two broad clusters – one around connectedness or membership and the other around sharing and generosity’.28 In addition to the social motivations listed above, altruism, fun and competition are also regarded as important incentives for users to engage.29 The mutual benefit for users and institutions can be made explicit in the design of online services. For instance, the tagline of Flickr Commons is ‘Help us catalog the world’s public photo archives.’30 The Games with a Purpose initiative uses a similar catchphrase: ‘When you play a game at Gwap, you aren’t just having fun. You’re helping the world become a better place.’31 Amazon’s Mechanical Turk is probably the most famous example of a platform that is built around interactions based on extrinsic motivations.32 Mechanical Turk employs individuals (‘workers’ in Amazon’s jargon) to perform simple tasks in return for monetary payment. Annotating Audiovisual Objects Audiovisual archives are investing in large-scale digitisation of their analogue holdings and, in parallel, ingesting an ever-increasing amount of born-digital files into their digital deposits. In 2005, UNESCO estimated that world audiovisual 25.Strohmaier et al., ‘Understanding Why Users Tag’; Oomen et al., ‘Emerging Practices in the Cultural Heritage Domain’. 26.Shirky, Cognitive Surplus, 82–3. 27.Leadbeater, We-Think. 28.Shirky, Cognitive Surplus, 78. 29.Ibid., 78–80. 30.Flickr Commons, http://www.flickr.com/commons. 31.Games with a Purpose, http://www.gwap.com/gwap/. 32.Ipeirotis, ‘Analyzing the Amazon Mechanical Turk Marketplace’. holdings totalled 200 million hours.33 As many archives have a mission to disseminate their collections to a wide audience, more and more of this material will become available online. Digitisation opens up new access paradigms and encourages re-use of audiovisual content. Audiovisual content is created both by professionals and, increasingly, by everyday users. Besides the newly created material, a large body of existing, analogue material is being migrated to digital files and managed by digital libraries. Query log analysis of usage of the Netherlands Institute for Sound and Vision catalogues by media professionals showed that the majority of orders by broadcast professionals for content to be reused are for small segments of a few minutes.34 These logs also show shortcomings of the current workflows which mainly rely on manual annotations. Recent research concluded that increasingly ‘user groups require and demand access to video fragments rather than entire programmes – video fragments accounted for 66 per cent of purchases in one recent study of a broadcast archive. Fine-grained manual annotation of video fragments is prohibitive, as the work involved is inevitably tedious, incomplete, and costly’.35 Increasing the use of the collections while managing the cost of collection creation (curation and annotation) requires research to understand user requirements better, to hasten the development of better search functionality for external users and to help reduce cataloguing costs, among other things. Furthermore, advances in multimedia information retrieval make it possible to offer fine-grained access to content on the shot or fragment level rather than the entire video or programme level. This is in line with the specific needs of user groups. These systems can support exploration of digital libraries that go beyond ‘just’ retrieval.36 For instance, it is now possible to examine distributed collections in a digital library across time, space, genre and other dimensions such as colour, origin and so on: these offer exciting possibilities for leveraging maximum benefit from the collection. We can now, in close collaboration with end-users, explore the ways best to deploy these technologies. The Waisda? Video Labelling Game This section describes the design and development of the Waisda? video labelling game, within the context of its major stakeholder: the Netherlands Institute for Sound and Vision. 33.UNESCO, ‘International Appeal for the Preservation of the World Audiovisual Heritage’. 34.Huurnink et al., ‘Search Behavior of Media Professionals at an Audiovisual Archive’. 35.Huurnink et al., ‘Today’s and Tomorrow’s Retrieval Practice in the Audiovisual Archive’. 36.McCarty, ‘Beyond Retrieval?’. Waisda?: Making Videos Findable through Crowdsourced Annotations 169 Organisational Setting: Sound and Vision The Netherlands Institute for Sound and Vision is a large-scale audiovisual archive, managing an ever-growing collection that currently comprises more than 750,000 hours of audiovisual material. Dutch Public Broadcasting is one of the major sources of content but it is not the only source. Currently, born-digital material is ingested straight from the broadcast production environment directly into the Sound and Vision asset management systems. Similar to many other audiovisual archives, Sound and Vision is engaged in large-scale digitisation efforts, migrating collections from analogue carriers into digital formats. As the investments in digital libraries can only be justified if the hosted material is successfully accessed and re-used, offering seamless access routes to the content they hold is of crucial importance for archives. Reliable, fine-grained and scalable annotation, indexing and search are thus prerequisites for providing meaningful and efficient access routes to the increasing body of content. Sound and Vision offers services to diverse user groups. As Sound and Vision is the business archive for Dutch public broadcasters, broadcast professionals (for instance documentary makers, journalists and news editors) are traditionally an important user group. These users mainly seek to re-use material in new broadcast productions. A second user group includes students and scholars in the humanities and social sciences who aim to use audiovisual archives as a source for diverse types of inquiry. Strongly related to this user group are educators who use the audiovisual archive to search for relevant footage that they can use to support a specific course or lecture. Finally, there is an increasing population of home users who access and explore the archive for personal entertainment or a learning experience. These diverse user groups have a broad range of search needs. Queries can be based on the subject of the programme, what is visible in the shots or both; they can be targeted towards broad categories of topics or genres, a specific programme or a single shot. Some users know exactly what they are looking for, while others have only a vague idea. The needs of television professionals are related to the genre and developmental stage of the programmes they make. A journalist who searches for a shot to illustrate an item in tomorrow’s news bulletin may only have time to scan the descriptions of a few programmes quickly for a usable shot, while a documentary-maker may have time to view multiple complete programmes before selecting a shot with the right content, atmosphere and aesthetic qualities. Years of experience at the customer service department of Sound and Vision have led to the following broad categorisation of three types of user queries:37 37.Hollink et al., ‘AMultidisciplinary Approach to Unlocking Television Broadcast Archives’. 1. Known item queries: for example, ‘The item about health care in the NOS news broadcast of the 15th of June 2008’, ‘The documentary by Henk de By about the Dutch painter Mondrian’. 2. Subject queries: – General areas of interest: for example, ‘all programmes about the Dutch economy’. – Recognised areas of interest: ‘housing problems of Spanish immigrants in Amsterdam during the sixties’. 3. Sequences, shots and quotes: – Specific: ‘shots of George Bush announcing war with Iraq’. – General: ‘shots of sunsets’; ‘shots of Newfoundland’. Annotating shots to support the range of queries discussed is very time consuming. Therefore, one of the main motivations for designing the Waisda? video labelling was to use the tags contributed by users in order to support these types of user queries. Designing Waisda? Adding tags to multimedia content is now a regular feature on media sharing platforms. Media-focused sites such as YouTube, Vimeo and Tumblr allow end-users to add tags to multimedia content they share online. Other platforms focused on professionally created content, such as Last.fm, offer end-users the ability to add tags to content created by third parties. Experimental systems such as LabelMe,38 CoVida39 and Anvil40 allow the creation of time-based, fine-grained annotations. A number of platforms use games as a means to engage users to annotate time-based media. Examples include Yahoo!’s Video Tag Game,41 PopVideo42 and TagATune.43 However, these examples have not matured beyond the experimental stages and are not deployed widely. The concept of the Waisda? video game was conceived by the Netherlands Institute for Sound and Vision and Amsterdam’s VU University in the context of the PrestoPRIME research project and the game was developed by an external software development company, Q42. So far, two pilots have been executed: each with a single broadcaster as the supplier of the collection and marketing power. 38.Russell et al., ‘LabelMe’. 39.Weber and Zimmermann, ‘CoVidA’. 40.Kipp, ‘ANVIL’. 41.Sigurbjörnsson and Van Zwol, ‘Flickr Tag Recommendation Based on Collective Knowledge’. 42.Gligorov et al., ‘An Evaluation of Labelling-Game Data for Video Retrieval’. 43.Díaz et al., ‘mTagATune’. Waisda?: Making Videos Findable through Crowdsourced Annotations 171 As in the ESP Game44 developed by Von Ahn, players receive points if their tag matches a tag that their opponent has typed in.45 In this chapter, we refer to this game mechanic that tracked when two players had entered the same tag as ‘user agreement’. On the home page (Figure 7.2), visitors are invited to choose any of the four TV genre-based channels. In each of the channels (reality TV, talk show, newsreels, documentary) a number of videos are running in a continuous loop. After the player chooses a channel, the game interface is launched (see Figure 7.3). Here, users are encouraged to tag what they see and hear. Waisda? is a so-called serious game, that aims ‘to be both fun and playable […] but at the same time be useful for a non-entertainment purpose’.46 Gameplay focuses on reaction time and precision. In the literature, this kind of gameplay is classified as ‘Twitch gameplay’.47 From the point of the archive, the underlying assumption is that tags are probably valid if there is mutual agreement between players. In the context of Waisda? these tags are referred to as ‘verified’ tags. 44.Jain and Parkes, ‘A Game-Theoretic Analysis of the ESP Game’. 45.Siorpaes and Hepp, ‘Games with a Purpose for the Semantic Web’. 46.Frank, ‘Balancing Three Different Foci in the Design of Serious Games’, 567. 47.Kerr, The Business and Culture of Digital Games Gamework/Gameplay. Waisda?: Making Videos Findable through Crowdsourced Annotations 173 In designing the interface, Malone’s heuristics of enjoyable game interfaces was a major point of reference.48 For instance the interface was designed so that it instantly provides performance feedback about how close the user is to achieving their goal of agreeing tags and as a result scoring as many points as possible. Users can immediately see how they are performing in relation to other players. Scorekeeping is split into several categories: ‘all-time heroes’; ‘fastest typers’; and so on. The interface also uses visual effects, for instance to show the scores given to individual tags. If there are no other players in the game environment, visitors play against so-called ‘bots’ based on earlier sessions that have been recorded. As mentioned, two large-scale pilots have been executed so far. Figures 7.2, 7.3 and 7.4 are screenshots of the current (July 2013) version of Waisda?. Waisda? is built in the Java programming language, the tags are stored in a MySQL database and the JW Player49 is used for video playback. JavaScript is used to synchronise tags with the time track of the video in the JW Player. Evaluation of the Pilots In this section we look at the evaluation results of the two Waisda? pilots to date: one ended in 2010, and one in 2012. In the first pilot, we focused on evaluating the concept of video tagging in a game environment with end-users and the general usefulness of the tags created. In the second pilot, a more fine-grained analysis of the tags was conducted, focusing on the added value of user tags for video search. First Annotation Pilot: 2009–10 The first pilot was executed in 2009 in collaboration with the Dutch television organisation, KRO Broadcasting. The subject of our analysis is the data collected between the launch in May 2009 and January 2010. During this period, the game amassed over 46,000 unique tags ascribed to approximately 600 videos by roughly 2,000 different players. The number of total tags entered in the game exceeded 420,000. The majority of players (1,051, or 45.8 per cent) added between one to 10 tags. A smaller number of players (810, 35.3 per cent) added between 10 and 100 tags, and less than half of that number (372, 16.2 per cent) added between 100 and 1,000 tags. Only a few players added more than 1,000 tags (63, 2.7 per cent), but together, they were responsible for adding the largest number of all contributed tags. This indicates that a project like Waisda? should not only aim for a wide audience, but should also find a way to target these ‘super-taggers’ specifically.50 48.Malone, ‘Heuristics for Designing Enjoyable User Interfaces’, 65. 49.‘JW Player Overview | Best HTML5 & Flash Video Player | LongTail Video’, http://www.longtailvideo.com/jw-player/. 50.Trant, ‘Tagging, Folksonomy and Art Museums’. Extensive qualitative and quantitative evaluation was carried out. This has been documented in two earlier publications.51 We highlight the findings below. Summary of the qualitative analysis After undertaking structured interviews with 10 players, we found that altruism is an important motivation for playing Waisda?. Also, the evaluation showed that the video content itself is also a motivational factor for players to start playing the game. Users have a particular interest in popular talk shows reflecting on recent events. Although Waisda? can be played in solitude (against ‘bots’), user research has shown that the vast majority of players prefer playing against others. This shows the importance of a substantial and active community of players. Summary of the quantitative analyses To estimate the lower bound of the fraction of user tags that are ‘proper’ Dutch words (i.e. words that really exist in the Dutch language), the overlap between the tags and a general lexicon of the Dutch language was computed. To determine if users and professionals use different vocabularies when describing videos, the overlap between all user tags and a typical domain thesaurus used by professionals in the cataloguing process was investigated. Cornetto WordNet52 was used as lexicon of the Dutch language. This is a lexical semantic database of Dutch that contains 40,000 entries, including the most generic part of the language. Cornetto organises nouns, verbs, adjectives and adverbs into synonym sets called synsets. A synset is a set of words with the same part of speech that can be interchanged in a certain context. Synsets are related to each other by semantic relations like hyperonomy, hyponomy, meronomy, etc., which may be used across part of speech. The GTAA (the acronym for the Common Thesaurus Audiovisual Archives in Dutch) was used as a domain vocabulary. The GTAA thesaurus is used by professional cataloguers in the Sound and Vision documentation process. It contains approximately 160,000 terms divided into six disjoint facets (that is, non-overlapping categories).53 All user-contributed tags were compared to these two vocabularies. Only a small percentage (8 per cent) of the unique tags are found in GTAA. A larger number (23 per cent) of the tags are found in Cornetto. The overlap between GTAA and Cornetto is larger for the verified tags (as above, tags used by two players for the same content). Almost 44 per cent of the verified tags are found in Cornetto, whereas only 14 per cent are found in GTAA. Of the verified tags, 9 per cent are found both in Cornetto and in GTAA. In other words, at least 21 51.Oomen et al., ‘Emerging Practices in the Cultural Heritage Domain’; Gligorov et al., ‘On the Role of User-Generated Metadata in Audio Visual Collections’. 52.Vossen et al., ‘The Cornetto Database: Architecture and User-Scenarios’. 53.These are: ~3,800 Subjects, ~97,000 Persons, ~27,000 Names, ~14,000 Locations, 113 Genres and ~18,000 Makers. Waisda?: Making Videos Findable through Crowdsourced Annotations 175 per cent54 of the verified tags are proper Dutch words but would not be used by a professional cataloguer. In addition, we observe that the verified tags are more often valid Dutch words than the non-verified ones. Considering all the unique tags, the majority, approximately 59 per cent, are neither found in Cornetto and GTAA nor are they verified. Further analyses revealed that almost half of this sub-set of tags is comprised of more than one word. While this could to some extent explain why they were not found in Cornetto and GTAA (these vocabularies predominately have single words) and they were not verified (likelihood of reaching a tag agreement among players decreases as the length of the tags increases) it is still uncertain if they are, in fact, meaningful. To get an answer to this question, an additional analysis was performed, using Google as a semantic filter. For each tag a phrase search was executed (with the tag enclosed in quotes, ‘’) and the number of hits (pages) that were returned was recorded. A tag is deemed meaningful only if the number of hits returned is more than one. For approximately 84 per cent of the tags not found verified or not found in a vocabulary, Google returned a positive number of hits. We sampled 200 tags from the group with no hits (labelled ‘zero-sample’) and 200 tags with the group with a positive number of hits (‘pos-sample’) for further analysis. The tags in the zero-sample could be divided into three groups: garbled text with no meaning whatsoever; seriously mistyped words (bordering on garbled text); and entire sentences or excerpts from sentences that were mostly grammatically incorrect. The pos-sample, on the other hand, contained morphological variations of proper words, proper words combined with characters that are not letters, slang, names, idioms and phrases and other common collocations.55 This analysis showed that the user tags complement the vocabulary used by professional cataloguers. Using the overlap with the vocabularies provides a first classification of the tags. Using the different facets in GTAA we can distinguish different types of tags, such as subject terms, locations, person and organisation names. In addition Cornetto makes it possible to distinguish between different types of words, such as noun and verb. Table 7.2 shows the distribution of user tags over the GTAA facets and Cornetto synsets. This shows that most tags are matched with subject terms from GTAA, but also a large number of tags could be matched to locations and names. The tags that are found in GTAA are predominantly subject terms, but also include locations and names. We also found evidence that user agreement filters out sloppy tags, as the verified tags are more often valid Dutch words than the non-verified ones. However, a large part of the non-verified tags could still be potentially useful, as some of them were found in GTAA or Cornetto. Moreover, the majority of non-verified tags were ‘deemed’ meaningful by Google. 54.There is overlap between the GTAA matches and Cornetto, hence ‘at least’. 55.As defined as a pair or group of words that are habitually juxtaposed. For instance ‘strong tea’ and ‘heavy drinker’. Table 7.2 Waisda? tag distribution over GTAA facets and Cornetto synset types Facet  Tags  Subject  1,199  Location  613  GTAA  Genre  52  Person  118  Maker  4  Name  673  Types  Tags  Noun  7,222  CORNETTO  Verb  2,090  Adjective Adverb  1,693 171  The usefulness of tags for the archive In addition to the quantitative analysis, a senior cataloguer has assessed the usefulness of the tags added to the two episodes from the set available in the game: the most tagged episode (from the popular Dutch reality TV show Boer zoekt Vrouw), with 19,322 tags, and an episode that was tagged with an average number of tags (Westerman’s New World, a documentary series about a former Dutch news correspondent situated in the US returning to the Netherlands), with 738 tags. The aim here was to compare the user tags against the in-house cataloguing rules as defined and used by Sound and Vision staff. Expert cataloguers were asked to rank the tags as provided by the users in different categories as: not useful; useful; very useful. The results for the best-tagged episode were as follows: 45 per cent of the tags were deemed useful with 27 per cent having a low and 12 per cent having a high accuracy, that is, they were deemed very useful. Of the tags for the other episode, 73 per cent were deemed useful with 26 per cent having a low and 19 per cent having a high accuracy. The senior cataloguer noted that in general the useful tags described the episodes differently to the keywords added by cataloguers. Firstly, tags added by users focused on describing what was seen and heard within a programme, while the professional metadata for audiovisual content focused on the topical subjects to which a programme referred.56 The user tags also describe moments from a programme, instead of a logical segmented part and or entire episode. The fact that 56.This observation is also supported by the quantitative analysis. See Gligorov et al., ‘On the Role of User-Generated Metadata in Audio Visual Collections’. Waisda?: Making Videos Findable through Crowdsourced Annotations 177 the tags collected by Waisda? differ from professional metadata is no surprise, and possibly even an indication that the tags contribute to bridging the semantic gap. To describe the episode as a whole, only two tags from the top 20 most added tags of the documentary episode proved to be useful. For the best-tagged episode none of the top 20 tags were deemed useful to describe the complete episode. We found that tags added to the documentary series episode were notably more often useful than tags added to the reality show. They were more descriptive and more specific. The reality show contained more general tags and the tags for the episode lacked specificity. These findings contradict the assumption that the more times a tag is added to an episode, the more useful the tag is to the audiovisual archive. Second Annotation Pilot: 2011–12 After the first large-scale deployment of Waisda? a number of improvements were made, leading to a second annotation collection pilot that ended in 2012. These improvements include: • giving players additional points for tags that matched terms in controlled vocabularies (GTAA Person and Geographical names); • support for matching synonyms, based on the structure in the Cornetto lexical database; • a game recap screen, with a summary of player activity, including an overview of all tag entries and the reasoning behind the awarded points for the entries; • home page redesign to include a tag cloud based on the tag entries to navigate to the underlying archive; • extending user profiles and including a game history. Since November 2011, Waisda? has been played with content from the human-interest television series ‘Man Bijt Hond’ (in English, Man Bites Dog; abbreviated here as MBH), a popular daily show that has been aired for the past 11 years. Each episode consists of seven to eight unrelated, self-contained fragments where each fragment typically comes under a recurring heading. The total collection consisted of 11,109 fragments. In the second evaluation period, 4,484 videos were given at least one tag. The number of players that have added at least one tag is 1,363. The evaluation of the second pilot focused on determining the value of user tags for video search. In order to assess the added value of user tags for video search the so-called ‘quantitative system evaluation’ methodology was used.57 This methodology requires a collection of documents (in our particular case a document will be a video fragment tagged in Waisda?), a set of representative queries denoting information needs and relevance judgements indicating which 57.Voorhees, ‘The Philosophy of Information Retrieval Evaluation’. documents in the collection should be returned for each query. We created this evaluation dataset though the following steps: (1) selecting a collection of video fragments tagged by players in Waisda?; (2) selecting a set of user queries from real-life query logs from the MBH website; and (3) creating relevance judgements. In order to collect relevance judgements for the query set and the fragment collection an online experiment was set up so that participants could take part in creating relevance judgements via a website. When a participant accessed the website they were presented with a welcome page which contained a description of the task they were required to perform. Before starting the task, the participants filled out a questionnaire that assessed their familiarity with Waisda?, the MBH TV series and the MBH website. After this initial step, the participant proceeded to the task page that listed the complete query set from the evaluation dataset and displayed the video to be reviewed; the participant watched the video and indicated which of the concepts denoted by the queries were shown in the video. We asked users to judge a video to be relevant for a query if it depicted the concept denoted by the query. Each participant was asked to evaluate at least five videos. The participants in the experiment were recruited mainly from the Waisda? online community and MBH series fan-base by distributing a call for participation containing a link to our web application through the major social networking services Facebook and Twitter. In total 107 participants started the experiment, and 83 of them evaluated at least one video.58 After the gold standard of relevant queries for different videos was created we created a computer program that mapped queries to relevant videos, and proceeded with the evaluation. To this end, a number of search systems were created. Each used the same state-of-the-art retrieval algorithm BM2559 but varied the metadata types used as inputs for the search query, namely; Waisda? tags, captions (subtitles) or professional in-house tags added by NCRV broadcasting (the producers of MBH) or combinations of two or more metadata types. Therefore, any difference in the retrieval performance of these systems can be attributed exclusively to the effect of the metadata. For example, by comparing the performance of the search engine that indexes the Waisda? tags against the search engine that indexes the captions it was possible to assess if user tags are indeed better suited for retrieval than the captions. To measure the performance of the search engines standard information retrieval measures such as mean average precision, precision and recall were used. For a given query, search precision is defined as the fraction of the retrieved instances that are relevant and search recall is the fraction of relevant instances for the query that is retrieved. Mean average precision is a slightly more complex performance measure that considers the rank of the retrieved documents.60 58.Gligorov et al., ‘An Evaluation of Labelling-Game Data for Video Retrieval’. 59.Robertson and Zaragoza, ‘The Probabilistic Relevance Framework: BM25 and Beyond’. 60.Evaluation of ranked retrieval results, http://nlp.stanford.edu/IR-book/html/ htmledition/evaluation-of-ranked-retrieval-results-1.html. Waisda?: Making Videos Findable through Crowdsourced Annotations 179 The results from the analysis are rather positive for the Waisda? tags. In fact, search based solely on user tags was more effective than search based on other types of metadata. In particular, the Waisda? tags outperformed the in-house (NCRV) tags by 69 per cent and captions by 39 per cent. Thus if any of the other metadata types are unavailable or costly to acquire, relying only on a sufficient number of user tags for search could yield equal or even better results. Moreover, combining user tags with the other metadata types proved to be beneficial for search. In fact, the program that exploited all available metadata performed best, in large part due to the contribution of the user tags: the observed performance improvement was 33 per cent compared to the second best, which used all metadata sources except the Waisda? user tags. Using only verified user tags (i.e. where there was mutual agreement) for search gives poorer performance than search based on all user tags. This is due to the fact that while search based on verified tags yields higher precision, it also has lower recall compared to all user tags. In fact, for most of the queries non-verified tags provided relevant results not found by the verified tags. This proves that considering only verified tags is too conservative: the application of a filtering criterion that would discard non-verified user tags caused the removal of tags that were valid video descriptors and are thus useful for search. Lastly, search performance steadily increases as more user tags are collected. This is true for both verified and non-verified tags. Moreover, search based on all tags consistently outperforms search based only on verified tags. When the average number of tags is slightly more than two tags per second of video footage, the search using all tags outperforms all search engines that exclude user tags. This estimate of tags per second of footage could be used as an indicator of whether a video has been tagged sufficiently. Lessons Learned These pilots have proven the usefulness of social tags created by users. Using a game as a platform to have users playfully execute these micro-tasks was a successful format. Over one million tags have been added to date in the context of two pilots, and the pilots managed to attract a core group of ‘super-taggers’ that are responsible for adding a considerable amount of the total number of tags. Results of these studies show that most of the tags contributed by users are useful and that they can complement any professional classification. The future work (see next section) mainly looks at approaches to improve usefulness of the tags for future retrieval. Waisda? has matured into a stable platform that can be repurposed for new tagging campaigns quite easily. The code is available in the open source software repository Github61 and it is encouraging to see how third parties are making use of the platform. For instance, the main European community for film archives, 61.GitHub, https://github.com/beeldengeluid/waisda/. European Film Gateway, have started using Waisda?.62 The platform will also be connected to Europeana,63 the gateway to Europe’s digital resources, enabling all audiovisual archives that contribute to Europeana to appear in the game. One ongoing challenge is attracting a sizeable number of players who commit to playing the game over extended periods of time. Sound and Vision teamed up with KRO and NCRV broadcasting to advertise the game through their communication channels. In addition to direct access to the ‘fans’ of the television programmes, social media (Twitter, Facebook) were used to encourage users continuously to play the game. Also, prizes were awarded regularly and in April 2013 an event was organised at the Sound and Vision premises, inviting in players for a full day of tagging. Carrying out myriad marketing efforts is key in attracting a constant number of players. For executing successful crowdsourcing campaigns with programmes with a smaller existing fan-base than the show Man Bites Dog, it will be important to change this approach and to focus on identifying a group of potential ‘super-taggers’and target them specifically rather than relying on mass appeal. The Zooniverse citizen science web portal presents an alternative model. The Citizen Science Alliance, who coordinate the portal, have managed to bring together a sustainable ‘army’ of users who participate in their various projects.64 Similarly, GLAMs could build a core community of users to participate in video labelling across many programmes. Social tags will be an important contribution to providing access to audiovisual collections, along with other sources of data that can be indexed, such as the catalogue entries for the overall programmes, closed captions, objects detected by content-based algorithms, related tweets and so on. Sound and Vision is currently integrating the social tags with their asset management systems. On the user-interface level, showing data provenance will be of key importance. Users can deal with some ‘fuzziness’as long as the interface states that tags are generated by non-experts.65 For instance, colour coding could be used to distinguish between the origins of the tags. Future Research Directions for Crowdsourcing Annotations The evaluation results show that, because they were not limited to controlled terms, the tags allow for multiple interpretations: named entities such as people typically contain only part of the name (e.g. only the first or last name), and the subject terms are limited to those available in the vocabulary used by the players of the game, which might not coincide with the vocabulary of a searcher. A typical method for defining a textual value is linking it to a concept defined by a vocabulary publicly available on the web, a process also known 62 Waisda? – European Film Gateway, http://prestoprime.cs.vu.nl:9092/. 63 Europeana – Home page, http://www.europeana.eu/. 64.Citizen Science Alliance, http://www.citizensciencealliance.org/. 65.Carmichael et al., ‘Multimodal Indexing of Digital Audio-Visual Documents’. Waisda?: Making Videos Findable through Crowdsourced Annotations 181 as ‘reconciliation’. For instance, the text ‘Prince Bernhard’ would be linked to an identifier for the more specific value ‘Prince Bernhard of Lippe-Biesterveld’ as published in Wikipedia. Typically, this is a semi-automatic process where a reconciliation service suggests a number of candidate concepts and the user selects the most appropriate one. As part of future enhancements of Waisda?, an interface for reconciliation was created. The current interface reconciles user tags with the structured data set published on the Freebase website. The initial results are promising.66 Linking these user tags to shared semantic concepts will add another level of meaning that can be exploited in search tasks. A final area of future work is to study whether certain tag features such as reputation of the tag author and provenance can be used to detect and exclude non-useful non-verified tags thereby increasing the search precision without sacrificing the recall. Over the past four years, Waisda? has matured into a rich platform for tagging of audiovisual content. As indicated in Section 2 ‘The Social Tagging of Audiovisual Heritage’, there are ample motivations for GLAMs to explore new ways to engage with their users. Projects such as Waisda? not only result in useful tags to be used for future retrieval, but they also support memory organisations in defining their position in an online context where sharing is the norm. Acknowledgements This research has been supported by the NWO project Agora and the EU FP7 project PrestoPRIME. The authors thank Lora Aroyo, Lotte Belice Baltussen and Maarten Brinkerink for their valuable contributions. References Bakhshi, Hasan. Culture of Innovation: An Economic Analysis of Innovation in Arts and Cultural Organisations. London: NESTA, 2010. Berners-Lee, Tim. ‘Giant Global Graph | Decentralized Information Group (DIG) Breadcrumbs’. Blog, November 21, 2007. http://dig.csail.mit.edu/breadcrumbs/ node/215. Bernstein, Abraham, Mark Klein and Thomas W. Malone, ‘Programming the Global Brain’. Communications of the ACM 55, no. 5 (2012): 41–3. doi: 10.1145/2160718.2160731. Carmichael, J. et al. ‘Multimodal Indexing of Digital Audio-Visual Documents: A Case Study for Cultural Heritage Data’. In International Workshop on 66.Hildebrand and Van Ossenbruggen, ‘Linking User Generated Video Annotations to the Web of Data’. Content-Based Multimedia Indexing, 2008. CBMI 2008, 93–100. June 18–20, 2008, London. doi:10.1109/CBMI.2008.4564933. Díaz, Francisco Javier, Claudia Alejandra Queiruga, Alejandro Ferraresso and José Luis Larghi. ‘mTagATune: Mobile TagATune – Audio Files Tagging Mobile Game’. In Proceedings of the 2011 10th International Conference on Mobile Business, ICMB ’11, 331–9. Washington, DC: IEEE Computer Society, 2011. doi:10.1109/ICMB.2011.39. Dovey, Jon and Helen W. Kennedy. ‘Game Cultures: Computer Games as New Media’. European Journal of Cultural Studies 11, no. 2 (2008): 197–202. Frank, Anders. ‘Balancing Three Different Foci in the Design of Serious Games: Engagement, Training Objective and Context’. In Conference Proceedings DiGRA 2007, 567–74. Tokyo: DiGRA, 2007. http://www.digra.org/dl/ db/07312.29037.pdf. Gligorov, Riste, Michiel Hildebrand, Jacco Van Ossenbruggen, Guus Schreiber and Lora Aroyo. ‘On the Role of User-Generated Metadata in Audio Visual Collections’. In Proceedings of the Sixth International Conference on Knowledge Capture, K-CAP ’11, 145–52. New York: ACM, 2011. doi:10.1145/1999676.1999702. Gligorov, Riste, Michiel Hildebrand, Jacco Van Ossenbruggen, Lora Aroyo and Guus Schreiber. ‘An Evaluation of Labelling-Game Data for Video Retrieval’. In Proceedings of the 35th European Conference on Advances in Information Retrieval, ECIR’13, 50–61. Berlin: Springer-Verlag, 2013. doi:10.1007/978– 3-642–36973–5_5. Hildebrand, Michiel and Jacco Van Ossenbruggen. ‘Linking User Generated Video Annotations to the Web of Data’. In Proceedings of the 18th International Conference on Advances in Multimedia Modeling, MMM’12, 693–704. Berlin: Springer-Verlag, 2012. doi:10.1007/978–3-642–27355–1_74. Hollink, Laura, Bouke Huurnink, Michiel Van Liempt, Maarten De Rijke and Arnold W.M. Smeulders. ‘A Multidisciplinary Approach to Unlocking Television Broadcast Archives’. Interdisciplinary Science Reviews 34, no. 2–3 (2009): 253–67. Howe, Jeff. ‘Wired 14.06: The Rise of Crowdsourcing’. June 2006. http://www. wired.com/wired/archive/14.06/crowds.html. Huurnink, B., L. Hollink, W. Van Den Heuvel and M. De Rijke, ‘Search Behavior of Media Professionals at an Audiovisual Archive: A Transaction Log Analysis’, Journal of the Association for Information Science and Technology 61, no. 6 (2010): 1180–97. doi:10.1002/asi.v61:6. Huurnink, B., C.G.M. Snoek, M. De Rijke M and AWM Smeulders. ‘Today’s and Tomorrow’s Retrieval Practice in the Audiovisual Archive’. In Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR ’10, 18–25. New York: ACM, 2010. http://doi.acm.org/10.1145/1816041.1816045. Huvila, Isto. ‘Participatory Archive: Towards Decentralised Curation, Radical User Orientation, and Broader Contextualisation of Records Management’, 2008. http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-112786. Waisda?: Making Videos Findable through Crowdsourced Annotations 183 Ipeirotis, Panagiotis G. ‘Analyzing the Amazon Mechanical Turk Marketplace’. XRDS 17, no. 2 (2010): 16–21. doi:10.1145/1869086.1869094. Jain, Shaili and David C. Parkes. ‘A Game-Theoretic Analysis of the ESP Game’. ACM Transactions on Economics and Computation 1, no. 1 (2013): 3:1–3:35. doi:10.1145/2399187.2399190. Jenkins, Henry. Spreadable Media: Creating Value and Meaning in a Networked Culture. New York: New York University Press, 2013. Johnson, Steven. Future Perfect: The Case for Progress in a Networked Age. New York: Riverhead Books, 2012. Jgensen, Corinne. ‘Image Access, the Semantic Gap, and Social Tagging as a Paradigm Shift’. Advances in Classification Research Online 18, no. 1 (2007). doi:10.7152/acro.v18i1.12868. Kemman, M., M. Kleppe and H. Beunders. ‘Who Are the Users of a Video Search System? Classifying a Heterogeneous Group with a Profile Matrix’. In Image Analysis for Multimedia Interactive Services (WIAMIS), 2012 13th International Workshop, 1–4. May 23–25, 2012. doi: 10.1109/WIAMIS.2012.6226765 Kerr, A. The Business and Culture of Digital Games Gamework/Gameplay. London: SAGE, 2006. http://search.ebscohost.com/login.aspx?direct=true&s cope=site&db=nlebk&db=nlabk&AN=251513. Kipp, Michael. ‘ANVIL: The Video Annotation Research Tool’. http://www.anvil.software.org/#. Lankes, R. David, Joanne Silverstein and Scott Nicholson. Participatory Networks: The Library as Conversation. Commissioned technology brief for the American Library Association’s Office of Information Technology Policy. Syracuse, NY: Information Institute of Syracuse, 2007. http://iis.syr.edu/. Leadbeater, Charles. We-Think. London: Profile, 2008. Malone, Thomas M. ‘Heuristics for Designing Enjoyable User Interfaces: Lessons from Computer Games’. In Proceedings of the Conference on Human Factors in Computing Systems. New York: ACM Press, 1982. McCarty, Willard. ‘Beyond Retrieval? Computer Science and the Humanities’, 2007. http://www.mccarty.org.uk/essays/McCarty,%20Beyond%20retrieval.pdf. Melenhorst, Mark, Marjan Grootveld and Mettina Veenstra. ‘Tag-Based Information Retrieval for Educational Videos’. EBU, 2008. tech.ebu.ch/docs/ techreview/trev_2008-Q2_social-tagging.pdf. Oomen, Johan and Lora Aroyo. ‘Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges’. In Proceedings of the 5th International Conference on Communities and Technologies, C&#38;T ’11, 138–49. New York: ACM, 2011. doi:10.1145/2103354.2103373. Oomen, Johan et al. ‘Emerging Practices in the Cultural Heritage Domain: Social Tagging of Audiovisual Heritage’. Proceedings of the WebSci10: Extending the Frontiers of Society On-Line.April 26–27, 2010, Raleigh, NC. Rheingold, Howard. The Virtual Community: Homesteading on the Electronic Frontier. Cambridge, MA: MIT Press, 2000. Robertson, Stephen and Hugo Zaragoza. ‘The Probabilistic Relevance Framework: BM25 and Beyond’. Foundations and Trends in Information Retrieval 3, no. 4 (2009): 333–89. doi:10.1561/1500000019. Rushkoff, Douglas. Present Shock: When Everything Happens Now. New York: Current, 2013. Russell, Bryan C., Antonio Torralba, Kevin P. Murphy and William T. Freeman. ‘LabelMe: A Database and Web-Based Tool for Image Annotation’. DSpace@ MIT, 2005. http://hdl.handle.net/1721.1/30567. Shirky, Clay. Cognitive Surplus: Creativity and Generosity in a Connected Age. New York: Penguin Press, 2010. Sigurbjnsson, Bkur and Roelof Van Zwol. ‘Flickr Tag Recommendation Based on Collective Knowledge’. In Proceedings of the 17th International Conference on World Wide Web, WWW ’08, 327–36. New York: ACM, 2008. doi:10.1145/1367497.1367542. Siorpaes, Katharina and Martin Hepp. ‘Games with a Purpose for the Semantic Web’. IEEE Intelligent Systems 23, no. 3 (2008): 50–60. doi:10.1109/MIS.2008.45. Strohmaier, Markus, Christian Körner and Roman Kern. ‘Understanding Why Users Tag: A Survey of Tagging Motivation Literature and Results from an Empirical Study’. Web Semantics 17 (2012): 1–11. doi:10.1016/j.websem.2012.09.003. Surowiecki, James. The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. New York: Doubleday, 2004. Trant, Jennifer. ‘Tagging, Folksonomy and Art Museums: Early Experiments and Ongoing Research’, 2009. http://dialnet.unirioja.es/servlet/oaiart?codigo=3700863. UNESCO. ‘International Appeal for the Preservation of the World Audiovisual Heritage’, 2005. http://portal.unesco.org/ci/en/ev.php-URL_ID=17859&URL_ DO=DO_TOPIC&URL_SECTION=201.html. Van Den Akker, Chiel et al. ‘From Information Delivery to Interpretation Support: Evaluating Cultural Heritage Access on the Web’. In Proceedings of the 5th Annual ACM Web Science Conference, WebSci ’13, 431–40. New York: ACM, 2013. doi:10.1145/2464464.2464491. Voorhees, Ellen M. ‘The Philosophy of Information Retrieval Evaluation’. In Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, CLEF ’01, 355–70. London: Springer-Verlag, 2002. http://dl.acm.org/citation. cfm?id=648264.753539. Vossen, P.T.J.M., K. Hofmann, M. De Rijke, E. Tjong Kim Sang and K. Deschacht. ‘The Cornetto Database: Architecture and User-Scenarios’. In Proceedings of 7th Dutch-Belgian Information Retrieval Workshop DIR2007, 89–96. Leuven: University of Leuven, 2007. http://hdl.handle.net/1871/11109. Weber, M. and T. Zimmermann. ‘CoVidA: Pen-Based Collaborative Video Annotation’. In ACM International Conference Proceeding Series. New York: ACM, 2012. doi:10.1145/2304496.2304506. Chapter 8 Your Paintings Tagger: Crowdsourcing Descriptive Metadata for a National Virtual Collection Kathryn Eccles and Andrew Greg The Public Catalogue Foundation (PCF), the National Inventory Research Project (NIRP) based at the University of Glasgow1 and the BBC all share the aim of making information on the painting collections of the United Kingdom’s museums and galleries more readily accessible. Over the past few years they have been working closely together to share their expertise and data in order to create two ambitious, exciting and innovative heritage projects: Your Paintings and the Your Paintings Tagger. Your Paintings Tagger invites members of the public to provide metadata about the paintings to enhance the discoverability of the collection, including using descriptive ‘tags’. This chapter presents the background to the projects, decision-making in the design of Your Paintings Tagger, a profile of the taggers and a snapshot of current activity. Background The Public Catalogue Foundation (www.thepcf.org.uk), a London-based charity, was set up by Fred Hohler in 2003 to record and publish details and images of all 200,000 oil paintings in public ownership in the United Kingdom. The PCF has a broad definition of public ownership, casting its net far wider than museums and galleries to include other obvious institutions in which to find paintings, such as town halls, libraries and schools, but also universities, hospitals, lighthouses, fire stations and charitable institutions. The original intention was to publish fully illustrated regional volumes, mostly organised on the basis of traditional counties and regions, but there are also institutional volumes such as those dedicated to .The National Inventory Research began work in 2001 and published its online database, the National Inventory of Continental European Paintings (NICE Paintings, http://www.vads.ac.uk/collections/NIRP/index.php), in 2007, funded by the UK’s Arts and Humanities Research Council (AHRC), the Getty Foundation, the Kress Foundation and Pilgrim Trust. This now includes over 9,000 old master paintings from over 200, principally smaller, regional collections across the UK. the Imperial War Museum, Tyne and Wear Museums, the Fitzwilliam Museum and the Victoria and Albert Museum. The long-term aim is still to complete the set of hard-copy volumes but this is dependent on local fundraising being raised for each volume. It soon became evident to the PCF that a parallel process of web publishing would save time and money and allow for regular updating. Following the PCF’s partnership with the BBC, the PCF’s data and images are now publically accessible and searchable through the Your Paintings website, designed, built and hosted by the BBC. Your Paintings (www.bbc.co.uk/arts/yourpaintings/) was launched alongside the Your Paintings Tagger, an initiative to invite the public to add vital metadata to the collection by tagging the paintings online via a crowdsourcing interface (www.tagger.thepcf.org.uk), on 23 June 2011. The PCF worked through locally based regional coordinators who identified and contacted institutions holding collections and arranged data collection and the photography of paintings. The programme of high quality digital photography, often undertaken in the most difficult of circumstances, has been one of most significant and worthwhile aspects of the PCF’s work. The overlap in interests between NIRP and the PCF has led the two projects to work increasingly closely together. Digital images created by the PCF are made available to NIRP and the two projects’ databases will eventually be merged so that the rich research data on NIRP’s National Inventory of Continental European Paintings (NICE Paintings) will be available to users of Your Paintings. Links are included on Your Paintings to the enriched data on NICE Paintings. NIRP staff Andrew Greg and Dr Jo Meacock have also been contributing to the development of the Your Paintings Tagger (henceforth YPT), the subject of this chapter. Metadata Requirements Metadata–or data about data –are of course essential if an objectis to be identified and found in a database or website. Users of an online database or other text expect to be able to find the objects or references they are interested in. Computers and the web enable this task to be done quickly and effectively – but only if accurate and consistent metadata, both in their structure and their content, are in place. Descriptive metadata for collections were pioneered in the library sector (Dewey Decimal System, 1876). Metadata standards for works of art have been the subject of intensive, overlapping and problematic implementation for decades. Spectrum, the UK museum collections management standard, which includes object cataloguing standards, is one example; Dublin Core is a low-level framework for describing any cultural product; the Van Eyck Project (Visual Arts Network for the Exchange of Cultural Knowledge) and the Getty’s Categories for the Description of Works of Art (CDWA) are two other relevant metadata standards. CDWA has been adopted by the PCF. Museum collection managers will be aware that, although metadata frameworks such as Spectrum have been increasingly adopted by museums and by collection cataloguing software companies, whose products are compliant with Spectrum, no sets of metadata terms have reached anywhere near universal implementation across UK museums, nor is it probably practical to attempt to create or implement such a dataset. Thus each project or collection creates and defines its own terms and terminology structure, more or less pragmatically, for its own purposes. The original data collection by the PCF for the Your Paintings books and website included limited metadata. The PCF was dealing with an extremely wide range of collections, from national and university museums to hospitals, fire stations and schools without art collection managers. With no resources to research and catalogue collections, PCF regional co-coordinators had to rely on the data they could collect locally from participating institutions. Understandably this was limited to the most basic descriptive data, and even then only when readily available. The metadata fields were restricted to: • artist; • title; • medium; • size; • production date; • acquisition method; • identity number. The Your Paintings project’s first work on metadata standards was to determine the most important additional metadata necessary to enable the website to be usefully searched by the BBC’s target users, a ‘mainstream audience’. The PCF and BBC worked with Glasgow and Oxford Universities to create and implement a more comprehensive set of descriptive metadata structures and terms to be used within Your Paintings. The design and implementation of this additional metadata was called the Data Enhancement Programme (DEP). The initial DEP work concentrated on subject matter, analysing and describing what is depicted in a painting, since that was recognised as being one of the principal ways in which users of Your Paintings would want to search the database, and one of the most complex. The initial scoping study was undertaken and the first terminology set was created by Dr Aimee Blackledge and Dr Gervase Rosser at the University of Oxford. One of the most useful outcomes of this work was the recognition of the need to include sets of more abstract terms and concepts that would both broaden the scope of the term set and reflect more contemporary approaches to art history. At the other end of the academic/populist spectrum, the BBC commissioned from the leading ‘user.centred design’ consultants Flow Interactive some fundamental research into the way potential users of the Your Paintings website would describe the subject matter of a representative range of paintings.2 Oxford’s role was then taken over by Andrew .Flow Interactive, ‘BBC Your Paintings’. See also http://www.foolproof.co.uk/ clients/bbc/ (accessed June 17, 2013). Greg and Dr Jo Meacock at Glasgow who worked with Flow Interactive to marry these two approaches, a task which involved compromise on all sides. The parameters which governed the choice, structure and detail of the descriptive metadata were several and overlapping: • Types of intended and expected users: the BBC aims its websites at a ‘mainstream audience’ including learners (which in practice means parents and children). • Expectations of users: the BBC is experienced at testing its websites with user groups and its audiences have high expectations regarding the quality of the user experience. • Educational potential: all parties wanted the website to be educationally sound, reliable, accurate and authoritative.3 • Established art historical practice: the PCF adapted the Getty’s CDWA as its metadata standard and the Union List of Artists’ Names (ULAN) for artists’ names. The project partners also wanted to use established art historical terms so that the website would be a useful resource for advanced students, teachers and researchers. • BBC policies: for example, the BBC is a strong advocate of ‘Linked Data’, which promotes the sharing and connecting of data, information and knowledge on the web. It uses DBpedia, a database of terms on Wikipedia, to link terms within its own websites and other digital content.4 It required Your Paintings data, especially personal and place names, to be similarly linked. • Search tools to be employed: the BBC was unable to find resources to develop search tools in advance of the Data Enhancement Programme, so the creation of the metadata structure had to anticipate potential future search mechanisms. • Economics: the DEP had to be implemented at minimal cost; some form of volunteer input seemed inevitable from the beginning. • Time-scale: originally a deadline of the 2012 Olympics was decided on for the completion of Your Paintings digitisation, which was completed in December 2012. Data enhancement was intended to take one or two years more. Based on these factors and the requirements for the website, the additional metadata requirements finally agreed were: 3.BBC, ‘The Your Paintings Vision’. Quoted with permission. 4.See http://www.w3.org/2001/sw/sweo/public/UseCases/BBC/ (accessed September 26, 2012); http://www.slideshare.net/metade/linked-data-on-the-bbc (accessed September 26, 2012). • Freely created social tags – ‘things and ideas’. The creation and use by the public of their own descriptive terms, so-called ‘social tags’, became an important feature of YPT at an early stage. However there was some concern in the academic team that aspects such as uneven tagging across the collection and tag quality would need to be addressed for Your Paintings, based on the experience of other pioneers of social tagging in museum contexts (see below). • Types, or ‘genres’, of painting. The concept of ‘genre’ is fundamental to the history of Western art, especially in the period of formal art education and training from the Renaissance to the late nineteenth century. However from the nineteenth century new genres of painting have developed, not least ‘abstract’ art, and the functions of art have changed. The meaning of particular types of paintings, such as ‘genre’ painting itself, history painting or allegories, may not be familiar to a wider public, but the concepts of landscape, still life and portrait remain valid and in common use. • Names of people, places and events depicted in the paintings. The names of people, places and events present sufficiently unique problems of identification, recording and authentication to be treated separately from ‘subject’. • Astructured, hierarchical set of subject keywords. The ‘subject’of a painting is often a contentious issue. While the objects depicted in a representational work will often be readily identifiable, the subject of a painting, in the sense of its deeper meaning, may be lost, incomprehensible to a modern audience or the subject of individual interpretation. Much work was done by the team in creating and debating two- and three-tier hierarchies of subject keywords that would cover as many subjects and themes of paintings in the Western tradition as possible. • Production dates. A substantial proportion of the paintings in Your Paintings have no information about when they were painted. It was considered essential that production date information, however approximate, be created for all paintings so they could be searched for and ordered by production date, time lines could be produced, etc. Keywords for ‘Periods’ in British history (Victorian, Elizabethan, etc.) could then be created by defining these in relation to these production dates. • Artistic styles and movement. It was recognised that users would expect to be able to search for familiar art historical terms such as Impressionist, Pop Art, the Pre-Raphaelites or Mannerism. We differentiated between the ‘style’ of a painting and the name of an artistic group or movement; some of these terms are applicable to an individual painting, some to an artist. The academic team made it clear that not every painting, indeed only a minority, could appropriately and usefully be tagged with a style, movement or group term. It produced a list of artistic styles and movements and associated artists, but noted the need to assess all those artists’ paintings as to whether they corresponded to the relevant style(s). • Artists’ groups. The team came up with a list of artists strongly associated with selected artists’ groups such as the Pre-Raphaelite Brotherhood, the Glasgow Boys and the St Ives School, so that a search for those groups would produce all paintings by all the associated artists, whether or not in an appropriate style or dating from their membership of the group. The Metadata Creation Methodology Of over-riding importance was to work out the way in which these tags for 200,000 paintings could be created. It was obvious that it would be prohibitively expensive to pay expert fine art cataloguers to describe all 200,000 paintings systematically and that some sort of voluntary system would have to be devised. Much work was spent devising scenarios of local volunteers, museum friends’ organisations and students working locally to describe their regional collections. As this became increasingly complicated the project team looked at Mechanical Turk and other crowdsourcing technologies. Crowdsourcing has been used by a number of museums to generate descriptive tags. Awell-known pioneer in this field is the US publically funded steve tagger (www.steve.museum/), which has been implemented at a number of sites including the Indianapolis Museum of Art and Brooklyn Museum.5 For museums, social tagging, such as that supported by the steve tagger, has the potential to engage both existing and new audiences, to increase accessibility and to enable them to reflect the varying perspectives of their diverse publics. There is an enormous literature on this subject.6 It has been shown how different are the ways in which curators and the public describe museum objects7 and that social tagging creates new terms not found in existing museum documentation. For example, 90 per cent of tags created in the steve.museum prototype tested in five museums in 2005– 06 were not found in the museums’ internal catalogue documentation published online8 and 88 per cent of contributed tags were determined by museum staff to be useful terms.9 Not only are different words and concepts used, the taggers’ terms are perhaps more likely to be used by other people of similar backgrounds when looking for pictures with those attributes. Thus social tagging provides a set of search terms designed by, and for, the widest set of potential users. 5.For example, http://www.brooklynmuseum.org/opencollection/objects/111974/ Krishna_and_Radha_Seated_on_a_Terrace (accessed June 17, 2013). 6.For an overview see Oomen and Aroyo, ‘Crowdsourcing in the Cultural Heritage Domain’. 7.For a good example see Trant and Wyman, ‘Investigating Social Tagging and Folksonomy’. 8.Trant, ‘Social Classification and Folksonomy in Art Museums’. 9.Trant, ‘Tagging, Folksonomy and Art Museums’. However for the Your Paintings Tagger project the principal problems with these examples were threefold: 1. The somewhat unrandomised nature of the tagging: when taggers can select what they want to tag, the most interesting and engaging objects are tagged first and more thoroughly. 2. Visibility of tags: taggers could be influenced by each other, and it was feared a ‘herd mentality’ would lead to confirmation of existing thinking rather than encouraging independence of thought and variety of interpretation. 3. Quality control: there is no automatic validation of tags. For the YPT team the most encouraging, indeed most inspiring, of existing crowdsourcing projects was in fact Galaxy Zoo, an astrophysics project devised by a consortium of astronomers and developers at the Universities of Oxford, Nottingham, Portsmouth, Johns Hopkins and Yale, and the Sloan Digital Sky Survey (www.galaxyzoo.org/). Galaxy Zoo 1, launched in July 2007 with one million galaxy images photographed by the robotic telescope of the Sloan Digital Sky Survey, set out to categorise these into elliptical or spiral galaxies, and, if spiral, to record the direction of the arms. Within 24 hours of launch, the site was receiving 70,000 classifications an hour and more than 50 million classifications were received by the project during its first year from over 150,000 people. For Galaxy Zoo 2, 250,000 of the best and brightest of the original sample of galaxies were subjected to more detailed questions. In its first 14 months users of Galaxy Zoo 2 made over 60 million classifications. The Galaxy Zoo methodology is based on the fact that the human brain is far better than current computer technology at pattern recognition. Its implementation relies on a few fundamental principles: • the tasks should require no specialist knowledge; • images are delivered randomly; • tags are not visible or shared; • the same object is classified by multiple participants; • results are subject to statistical analysis. By implementing these, it was proven that the classifications Galaxy Zoo provides are as good as those carried out by professional astronomers.10 Implementing the Methodology The YPT team worked with Dr Arfon Smith of Galaxy Zoo to apply these principles to the requirements of the YPT. When taggers arrive on the website, they are given 10 Galaxy Zoo, ‘The Story so Far’. the opportunity of an online tutorial and are then presented with random images which they tag with their own terms and with descriptive options presented to them. Taggers cannot select which paintings they tag and are never given the same paintings twice. Tags are selected for acceptance after having been independently selected by taggers a minimum number of times. The thresholds of acceptance were based on the results of a large-scale pilot and later amended as a result of the early results from the live YPT in July 2011. A painting is deemed to have been ‘fully tagged’ and is withdrawn from the system when it has been tagged by 15 people. The tag acceptance process is intended to be entirely automated, with tags that reach the thresholds before being ‘fully tagged’ being automatically adopted and those that do not being discarded. There are however a few carefully defined scenarios with ambiguous results (such as the identification of multiple sitters in a portrait) where tags are submitted to a ‘Supervisors Interface’ through which a panel of specially recruited art historians can arbitrate on ambiguities and other issues, and whose decision is final. Tagging quality is also regularly monitored by project staff. Tags are initially stored on the PCF’s database and accepted tags go live on Your Paintings after regular synchronisations of data between the PCF and the BBC. The use of minimum thresholds also eliminates the danger of rogue or deliberately vandalising tags. In fact, as other similar crowdsourcing projects have shown, vandalism is very rare. Modest thresholds may keep out rogue tags but they cannot entirely eliminate common misspellings or misconceptions about subjects. However correct spellings are bound to outweigh misspellings, and correct usages, incorrect ones, so we have been reassured that good tags are always more likely to meet the threshold than bad ones. In fact, since tagging is intended to create search terms, it is arguably advantageous that a common misspelling is accepted as a valid and functioning search term. A related debate was had about the use of singular and plural terms. In theory, if both singular and plural forms were used, a term might not reach the threshold. The use of the singular is requested on the website, the dictionary links provide only the singular, and in practice singular terms outnumbered plural forms even before the enhanced prompt so were preferentially accepted by the system. The project partners’ consensus was that it is preferable to have some erroneous tags accepted than to lose good tags by having higher thresholds. The use of a crowdsourcing technique for the tagging influenced the way some tasks could be carried out. An early and rather complex hierarchy of tagging tasks and associated groups of participants was simplified to two groups: public taggers, open to anyone, who would describe the pictorial content of paintings, and ‘expert taggers’, approved on the basis of submitted evidence usually taken at face value, who would deal with two other priority tasks (see below). Expert taggers amount to around 7 per cent of all taggers. The public tasks were those that could be carried out without any art historical knowledge and would accurately reflect the priorities and interests of the end users. This required some careful thought about public needs, from both the tagger and general audience perspectives. It was decided to combine a free text tagging opportunity with a more structured scheme. The public tagging tasks are the social tags for things and ideas (free text), ‘names’ (of places, people and events), ‘types’ and ‘subjects’. With the public tagging, taggers can use their own judgement as to what is the significant pictorial content of any given painting, whether objects or ideas. Terms are checked against a version of the Oxford English Dictionary to rationalise spellings, although users can over-ride the OED. Names are entered in a separate ‘workflow’and are checked against DBpedia, again to rationalise and disambiguate and eventually to enable links to be provided to further information on Wikipedia. Alongside the free text tagging two structured lists are used in the final public workflows: firstly, the traditional genres of paintings, which in YPT are called ‘types’ (portrait, still life, landscape, etc.) and, secondly, more objective but non-specialist concepts of subject matter. The latter became a structured subject vocabulary with two hierarchical levels. The relationship between ‘types’ and ‘subjects’ resulted in one of the most interesting internal debates. Finally, although the concept of ‘types’ was retained, the list was restricted to terms that did not overlap in a potentially confusing way with other kinds of subject matter or refer to types of painting the taggers were unlikely to be familiar with. The list of ‘types’ is therefore not intended to be comprehensive, but enables searching for familiar concepts that are not included in the subject hierarchy. Other kinds of information considered most valuable for end-users of Your Paintings included the dating of undated paintings, which make up a significant proportion of the PCF’s database, and the categorisation of paintings by ‘style’, and by artists’ ‘movements’ and ‘groups’. These tasks obviously require art history expertise, but again by manually selecting potential ‘expert taggers’ for knowledge and experience (through a simple online form) and subjecting the results to statistical tests, reliable results can be achieved. In the online tagging tasks, we ask these ‘expert taggers’ to provide earliest and latest production dates for undated paintings and to associate styles and movements with selected paintings. Tests showed that it requires 10 taggers to provide sets of dates to produce a reliable pair of earliest and latest dates. This is done by calculating the median of each of the two sets of dates. Although an assumption of some non-art historians in the project team had been that all paintings could be allocated to a ‘style’ or ‘movement’, it was agreed that Glasgow University staff would begin by creating a selective list of styles and movements and associating selected artists with one or more of these styles and movements. The tagging task was then to select whether a particular painting by one of the selected artists could be appropriately allocated to any of the styles and movements associated with that artist. (In addition a list of artist groups was created and specific artists were associated with each group. Since it is impractical and often meaningless to allocate specific paintings to an artists’group, it is the artists themselves – and thus all their works on the database – which become associated with the group for search purposes. This pragmatic, though academic, tagging exercise is thus outside the public tagging project.) Table 8.1 Summary of tagging tasks and properties Workflow  Public or expert  Threshold for acceptance of tags  Freely created social tags (‘things and ideas’)  Public workflow  2  Names of people, places and events depicted  Public workflow  2  Types, or ‘genres’, of painting  Public workflow  4  A structured, hierarchical set of subject keywords  Public workflow  2  Production dates, where missing  Expert workflow  10  Artistic styles and movement  Expert workflow  2  Artists’ groups  Automated  n/a  In the project’s first major pilot the sequence of tagging tasks presented on the Your Paintings Tagger was ‘type’, ‘subject’, ‘things and ideas’, ‘names’ (of places, people and events) and then, for the expert taggers, ‘styles and movements’, and finally ‘dates’. The team received significant levels of feedback from the pilot that, having selected several types and subjects before then looking more closely at the paintings while recording the things and ideas in them, users often wished to ‘go back’ and revise their selection of types and subjects. As this was technically too difficult to implement, the order of tagging was changed to: things and ideas, names, types, subjects, etc. Thus the first thing taggers are asked to do is to study the image and list things they see. They then go on to identify from our lists the type(s) or subjects that best describe the painting. Taggers: Demographics, Motivation and Community The importance of understanding the usage and impact of digitised scholarly and heritage resources has long been recognised.11 Such studies can be used to gain insights about actual versus expected audiences and their behaviours, to capture serendipitous usage and users, to understand the impact of such resources on scholarly behaviour and communication and to maximise the potential audiences and reach.12 In crowdsourced collections where the users are actively contributing to and enhancing the resource, such understanding is highly relevant and can be used to aid recruitment, retention and productivity. Data about the Your Paintings 11.See, for example, Meyer et al., ‘Final Report to JISC’; Marchionni, ‘Why Are Users So Useful?’; Warwick et al., ‘If You Build It Will They Come?’. 12.See Meyer, Splashes and Ripples. taggers has been gathered both by the Public Catalogue Foundation and the BBC, and by an AHRC-funded research project based at the Oxford Internet Institute, University of Oxford.13 Here we present some findings relating to demographics, motivation and community. About the Your Paintings Taggers A survey of taggers by the Public Catalogue Foundation in November 2012 painted a vibrant picture of this community. The 769 respondents, representing about 20 per cent of the active tagger population, shared demographic and rich reflective data. The survey showed that taggers are nearly 70 per cent female, a figure that is comparable with volunteers to the Transcribe Bentham project14 but which differs markedly to the Galaxy Zoo volunteers, who were predominantly male.15 It is worth noting these figures when considering the very different tasks at the heart of Your Paintings Tagger and Transcribe Bentham (see Chapter 3 of this volume) as compared to Galaxy Zoo. Further research in this area may wish to consider whether certain types of task and subject matter are more likely to engage male or female volunteers. The survey also revealed that 55 per cent of taggers are over 55 years old and that the majority (79 per cent) were educated to degree level and above, both of which findings correlate with studies of the Transcribe Bentham volunteers. Due to the nature of the task (transcribing handwritten manuscripts) the Transcribe Bentham team particularly targeted educated volunteers, having stated that they believed that the ‘academic and professional community would be the most receptive to our project’.16 In contrast, the Public Catalogue Foundation deliberately chose a different path, partnering with the Galaxy Zoo team and the Citizen Science Alliance in order to ‘encourage broad public involvement’, with the idea that the resulting tagged and searchable database would be designed for a ‘mainstream public audience’, as well as ‘museum professionals, academics and … learners of all types’.17 Further data from this survey reveal that the majority of taggers (80 per cent) are regular art gallery visitors, visiting at least every few months, and 28 per cent work or volunteer in the art world or museums. It seems, then, that YPT has succeeded in reaching an audience with a high level of education and relevant interest. This is an interesting finding, given that the net was cast deliberately widely. Perhaps more interesting, however, is the fact one-fifth of taggers reported never, or very 13.Dr Eccles gratefully acknowledges the support of the Arts and Humanities Research Council [grant number: AH/J003077/1]. 14.Causer and Wallace, ‘Building a Volunteer Community’. 15.Romeo and Blaser, ‘Bringing Citizen Scientists and Historians Together’. 16.Causer and Wallace, ‘Building a Volunteer Community’. 17.Public Catalogue Foundation, ‘Framework for Classifying Paintings Online’. Internal document, quoted with permission. rarely, visiting art galleries. Asked to comment on their visits, or lack of visits, to art galleries, half of those who responded reported that they would like to visit more often but were constrained by location, difficulty of travel, ill health or lack of funds. Several pointed out that the availability of digital collections had helped overcome these difficulties, with one tagger adding that ‘this is why Tagging is good’. The remainder represents a small body of taggers who are interested in viewing and interacting with art online, but do not choose to visit galleries. For this group, online art galleries such as Your Paintings, and crowdsourcing initiatives such as Your Paintings Tagger, have created a means of enjoying art that was not previously available to them. Motivation Crowdsourcing projects are frequently cited as having the potential to transform research, democratise access to research and enhance opportunities for community engagement, lifelong learning and knowledge exchange. Understanding the motivation of crowdsourcing volunteers is vital, therefore, if we are to capitalise on these possibilities. A number of studies have explored the motivations of crowdsourcing volunteers, and this analysis of YPT tagger motivations seeks to build on this work.18 It does so by enacting a direct comparison with the 12 principal motivations identified through surveys of and interviews with Galaxy Zoo’s users.19 The comparison with Galaxy Zoo is shown in Table 8.2. The evident differences in absolute figures are thought to be a product of differences in methodology: Galaxy Zoo volunteers seem to have been less strongly prompted to identify multiple motivations.20 However several motivations rate highly in both groups of contributors: • an interest in the subject (astronomy/paintings); • a desire to contribute to research; • amazement at the scale of the universe/national paintings collection. • The most significant differences are that YPT taggers are more motivated by: • helping a large project; • using a useful resource for teaching; • discovering unseen paintings. 18.Raddick, ‘Galaxy Zoo’; Romeo and Blaser, ‘Bringing Citizen Scientists and Historians Together’; Causer and Wallace, ‘Building a Volunteer Community’; Holley, ‘Crowdsourcing’; Dunn and Hedges, ‘Crowd-Sourcing in the Humanities’. 19.Raddick, ‘Galaxy Zoo’. 20.The average ratio of responses by YPTand Galaxy Zoo volunteers to motivations is 4.8 in favour of YPT. Table 8.2 Compared motivations of Galaxy Zoo and Your Paintings Tagger volunteers Galaxy Zoo description Motivation % Your Paintings Tagger Motivation % description I am interested in Astronomy 46 I am interested in Paintings 85.5 astronomy paintings I enjoy looking at the Beauty 16 n/a beautiful galaxy images I can meet other people Community 6 I like working with Community 12.3 with similar interests people with similar interests I am excited to Contribute 22 I am excited to Contribute 60.8 contribute to original be contributing to scientific research research into paintings I can look at galaxies Discovery 8 I can look at paintings Discovery 50.5 that few people have that few people have seen before seen before I had a lot of fun Fun 11 I have fun Fun 55 categorising the galaxies categorising the paintings I am happy to help Helping 7 I am happy to help Helping 76.3 with a national project like Your Paintings I find the site and forums Learning 10 I find Tagger helpful Learning 45.6 helpful in learning about in learning about astronomy paintings I am interested in science Science 4 n/a I find Galaxy Zoo to be Teaching 2 I find Tagger to be a Teaching 15.7 a useful resource for useful resource for teaching other people teaching other people I am amazed by the vast Vastness 24 I am impressed by Vastness 51.5 scale of the universe the wide range of the national collection of paintings I am interested in the Galaxy Zoo 8 n/a Galaxy Zoo project The desire to help, to achieve a large and challenging group goal and to ‘record, find or discover new things’were identified by Rose Holley as important motivations among crowdsourcing volunteers.21 The identification of the crowdsourcing project YPT as a useful educational tool is a powerful new motivation, and suggests that the public are gaining awareness about and confidence in exploiting crowdsourcing projects for their own ends. This is pertinent, as there has been a great deal of focus on how the public can be engaged in crowdsourcing projects to solve ‘big data’ problems, but less focus on what crowdsourcing can offer the public in return. The ways in which the YPT was used for teaching also varied, with some respondents stating that they were interested in learning about classification and cataloguing, some stating that they were using the YPT for language teaching and others using it to develop their own or their students’ art historical and critical skills. Many taggers reported that they were engaged in academic research and were using the YPT to identify potential resources for their work. An important 21.Holley, ‘Crowdsourcing’. part of the PCF’s mission in developing the digital collection Your Paintings was to offer the public, museum professionals and academics a dynamic resource for teaching and learning, and it is evident from these responses that the potential of Your Paintings, and also of YPT, has been recognised and provides a powerful incentive to volunteers. Respondents identified a number of other benefits of tagging, which motivated them to continue. Several reported that they found their powers of observation and focus were challenged and improved by the process of tagging, particularly by the random nature of the paintings presented for tagging and the close examination of the painting demanded. Others stated that they found the process relaxing, offering an opportunity for reflection and enrichment. One tagger reportedusing the project as a distraction to reduce their smoking habit. A number of respondents admitted that they were tagging in order to gain knowledge about how such projects work in order to apply this knowledge in their own professional lives, from museums and galleries professionals to digital asset management. These examples lend weight to the impression formed above that crowdsourcing audiences are becoming more engaged in making the project work for them as well as contributing. Overall, the relative strength or weakness of certain motivations compared to other projects can provide useful insights for projects, alerting them to successes and missed opportunities in recruiting participants, the design of project websites and communicating with volunteers. With the proliferation of crowdsourcing initiatives, this type of analysis will enable projects to focus their efforts in the right areas, and to reflect on their strengths in what is fast becoming a crowded market. Community The YPT taggers are currently more of a ‘crowd’ than a ‘community’, following Caroline Haythornthwhaite’s distinction between two intersecting types of engagement, ‘lightweight’ and ‘heavyweight’, in peer production.22 Contributions are relatively anonymous and straightforward, and recognition is limited to a league table of top taggers, both overall and of the previous month, and progression through a hierarchy of paintbrush colours, each colour awarded in line with their tagging progress. There are indications that the taggers would welcome the opportunity to interact more as a community, and to form connections with project team members and interested experts. Taggers were asked whether they thought providing the option for discussions with other taggers about the project (such as through blogs) would encourage participation, and the majority reported that they thought it would (Figure 8.3). Those who commented in response to this question stated that the main advantage of such contact would be to share best practices and mistakes, and to allow tag categories to develop through discussion. It should be 22.Haythornthwaite, ‘Crowds and Communities’. people to tag more paintings noted, however, that there is a proportion of taggers who value the solitary activity of tagging and who have no desire for great social activity. When asked whether there was anything else they would like to do on the YPT site that they could not currently do, 29 per cent of those surveyed asked for extra features, and 20 per cent of the requested features were for some sort of social functionality. The social features requested largely involved seeking feedback and interactions with project team members, experts and other taggers, and being able to see and reflect upon other taggers’work. Taken in conjunction with requests for non-social advanced functionality such as being able to zoom into a picture for close analysis, add specialist knowledge using free form text and to move more easily between the YPT and frequently used research tools, this suggests that there is a community of taggers who would like to move towards what Haythornthwaite defines as ‘heavyweight’ peer production, where feedback and a peer support system are instrumental. The survey findings support those of a focus group of active taggers held at the National Portrait Gallery in March 2013. Uptake of invitations to this event, where taggers were invited to meet the project team, hear more about the way in which their efforts were contributing to the project and offer constructive feedback about YPT was high and participants were extremely positive during and after the event about the opportunity to meet the team and other taggers. During the focus group, taggers reported that they felt that their knowledge and confidence grew as their exposure to the paintings increased, and this was the point at which they began to be frustrated by aspects of the functionality: they wanted to be able to do more, to add more and to be able to move more fluidly between tasks in order to be able to correct themselves. Several taggers reported going away from the YPT to research items in the pictures and wanted an easy way to add this knowledge. When prompted, around 50 per cent of the active taggers present were interested in a forum, or dynamic community space, in order to interact with other taggers. This potential disjuncture between the needs of the project – to add metadata to the paintings in the collection as quickly and simply as possible in order to make the database searchable – and the needs of the volunteers, who become invested in the task and want to perform to the best of their increasing abilities, and to form a community around these tasks, is worth noting. Crowdsourcing initiatives can be complex to design and expensive to create, leaving few resources for improvements and adjustments once the project is open to the public, yet it is frequently after they are launched that user behaviour and demands begin to emerge. Understanding these developments could allow projects to tailor their tasks in order to harness public support more fully. Your Paintings Tagger Productivity As of June 2013 there are 9,590 YPT taggers registered, of whom 31 are staff or administrators and 707 ‘expert’ taggers drawn from the public. Over 23,000 paintings have been tagged, and over four million individual tags have been created. However the productivity of taggers varies enormously (see Table 8.3 and Figure 8.4). In total 44 per cent of registered taggers have tagged fewer than five paintings since they registered and are thus essentially inactive; only 16 per cent have tagged more than 50 paintings and are thus contributing meaningfully to the project. The 65 taggers who have each tagged over 1,000 paintings can be defined as ‘super-taggers’, a phenomenon of many crowdsourcing projects.23 Super-taggers are those who complete a disproportionate amount of the work, such as in the Australian Newspapers Digitisation Program project, where Holley 23.For example see Springer et al., ‘For the Common Good’; Holley, ‘Tagging Full Text Searchable Articles’. found that 57 per cent of the tag pool was created by the top 10 ‘super-taggers’, and where super-taggers created ‘significantly higher number of tags than other users (usually thousands)’. Holley also noted that this accorded with the findings of the Library of Congress Flickr project ‘where 40% of the tags were added by a group of 10 super-taggers’ and the ‘top super-tagger entered more tags than all the anonymous users put together’.24 A similar phenomenon was noted by Brinkerink in the video tagging project Waisda? (see also Chapter 7), which was designed as a game in which users were invited to tag what they see and hear, and receive points for a tag if it matches a tag that their opponent has entered. Like Your Paintings Tagger, the assumption is that tags are most probably valid if there is mutual agreement. Over six months, 2,000 participants added over 340,000 tags to 600 videos. Brinkerink observed that ‘only a few players added more than a thousand tags (63, 2.7%), but together they were responsible for adding the largest number of all contributed tags’, indicating that projects ‘like Waisda? shouldn’t only aimfor a wide audience, but should also find a way to specifically target these “super taggers”’.25 The top four YPT ‘super-taggers’ in June 2013 had tagged 24,016, 23,830, 17,860 and 17,348 paintings respectively. Given that 15 taggers are required to ‘complete’ a painting, these four taggers alone are in effect responsible for completing some 5,500 paintings, about 20 per cent of the 23,000 paintings so far completed by all 9,590 registered taggers, or 5,419 active taggers. Table 8.3 Your Paintings Tagger achievement levels and productivity as of 17 June 2013 Level and number of pictures tagged  Count  %  No tags  2,487  25.93  Green Tagger (1–4 paintings tagged)  1,684  17.56  Yellow Tagger (5–49 paintings tagged)  3,888  40.54  Red Tagger (50–249 paintings tagged)  1,248  13.01  Blue Tagger (250–499 paintings tagged)  146  1.52  Gold Tagger (500–999 paintings tagged)  72  0.75  Master Tagger (1,000 and more paintings tagged)  65  0.68  Total  9,590  100  24.Holley, ‘Tagging Full Text Searchable Articles’. 25.Brinkerink, ‘Waisda? Video Labeling Game’. There are now (June 2013) about 100 taggers active every day producing about 5,000 tags per day, but only about 71 paintings are ‘completed’ per month. This is partly because there are 188,000 paintings currently in the system, all being randomly delivered to taggers, so the chance of any one painting being tagged 15 times is low. Acomparison of the figure of 71 per month with the total number of paintings remaining to be tagged indicates the extent of the challenge to complete the collection. In January 2013, following the survey of taggers, the YPT was adapted to allow taggers to select the collection(s) they tag, at individual or county level, and to select which of the public workflows, the free text ‘things’and ‘names’ workflows and/or the hierarchical menu-driven ‘subject’workflow, they wish to do. ‘Subjects’ now seem to attract only half the tagger activity of ‘things’ and ‘names’. There is the potential to reduce the threshold for the number of taggers required to ‘complete’ a painting. At launch it was 20 and has since been reduced to 15. A further reduction would not reduce the quality of tags but would proportionally reduce the quantity of accepted tags, since fewer tags would reach the individual workflow thresholds of two or four before being tagged by 15 people. At present each painting receives about 80 ‘things’ and ‘names’ tags, of which about 25 meet the thresholds and are accepted: these are the only tags so far implemented by the BBC on the Your Paintings site. Tags created by Your Paintings Tagger: 4th Royal Irish Dragoon Guards, People, Hat, Men, Woman, Sky, Cloud, Building, Dog, Women, Buildings, Horse, Horses, Soldier, War, Sword, Uniform, Helmet, Soldiers, Cavalry, Saddle, Shawl, Balcony, Street, Luggage, Departure, Cat, Dragoon, Crimea, Crimean War, Landscape. Conclusions The development of Your Paintings Tagger shows that it is possible to create a successful crowdsourcing architecture to allow the public to contribute high quality tags to an art collection of national importance. These tags form the metadata that underpin and augment this collection, an innovative and cost-effective answer to expensive, large-scale digitisation programmes. The evidence shows that this approach is working, in that it is supplying high quality data to the Your Paintings site, and is not attracting malicious content. Evidence suggests, however, that the rate of tagging is far below expectations, and that if tagging continues at this rate and in this fashion, it will be a long time before the collection is completed. Research on the YPT tagger community reveals that there is a core of dedicated taggers, who are committed to regular, high quality tagging. It also reveals that there are a number of self-reported incentives and rewards for tagging, and that these both reinforce and extend categories of motivation observed in other crowdsourcing projects. The phenomenon of the ‘super-tagger’ is particularly notable, since a dedicated core of taggers can have a transformative effect on a project. Further analysis of factors affecting this group’s particular motivations, habits and behaviour around tagging, and how these differ from other sub.categories of taggers, may suggest ways in which the amount and rate of tagging may be improved. It will also be interesting to observe the impact of greater flexibility in choosing paintings for tagging. There are a number of other options that YPT could consider to redouble the current volunteer effort, a number of which have been discussed above. Such interventions, if attempted, would be valuable in adding to our understandings of how and why volunteers contribute to crowdsourcing projects. The more we learn, the more we can do to ensure such initiatives reach their full potential, for the sake of the projects and for the volunteer communities that support them. Acknowledgements Some material in this chapter was originally published in Andrew Greg, ‘Your Paintings: Public Access and Public Tagging’, Journal of the Scottish Society for Art History 16 (2011–12): 48–52. Other parts are included in a more detailed discussion: Andrew Greg, ‘The Your Paintings Tagger – Theory and Practice’, in Display: Consume: Respond – Digital Engagement with Art, Proceedings of the 28th Annual Conference of Computers and the History of Art, edited by C. Bailey. London, Association of Art Historians, 2013. http://www.chart.ac.uk/chart2012/ papers/toc.html. References BBC. ‘The Your Paintings Vision’. Unpublished BBC/PCF document, [c. 2010]. Brinkerink, M. ‘Waisda? Video Labeling Game: Evaluation Report, Images for the Future’. Report, 2010. http://imagesforthefuture.com/en/research/waisda.video-labeling-game-evaluation-report (accessed June 17, 2013). Causer, T. and V. Wallace. ‘Building a Volunteer Community: Results and Findings from Transcribe Bentham’. Digital Humanities Quarterly 6, no. 2 (2012). http://www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html. Dunn, S. and M. Hedges. ‘Crowd-Sourcing in the Humanities: A Scoping Study’. Report to the Arts and Humanities Research Council, 2012. Flow Interactive. ‘BBC Your Paintings: User Experience Research and Design’. Internal report for the BBC, 2012. Galaxy Zoo. ‘The Story So Far’. http://www.galaxyzoo.org/#/story (accessed June 18, 2013). Greg, A. ‘Your Paintings: Public Access and Public Tagging’. Journal of the Scottish Society for Art History 16 (2011–12): 48–52. Greg, A. ‘The Your Paintings Tagger – Theory and Practice’. Paper presented at the CHArt – Computers and the History of Art conference, Display: Consume: Respond – Digital Engagement with Art, London, November 15–16, 2012. Haythornthwaite, C. ‘Crowds and Communities: Light and Heavyweight Models of Peer Production’. Proceedings of the 42nd Hawaiian Conference on System Sciences, 1–10. Waikola, Hawaii: IEEE Computer Society, 2009. Holley, R. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3/4 (2010). http://www.dlib.org/dlib/march10/ holley/03holley.html. Holley, R. ‘Tagging Full Text Searchable Articles: An Overview of Social Tagging Activity in Historic Australian Newspapers August 2008–August 2009’. D-Lib Magazine 16, no. 1/2 (2010). http://dlib.org/dlib/january10/holley/01holley. print.html (accessed June 17, 2013). Marchionni, P. ‘Why Are Users So Useful? User Engagement and the Experience of the JISC Digitisation Programme’. Ariadne 61 (October 2009). http://www. ariadne.ac.uk/issue61/marchionni. Meyer, E.T. Splashes and Ripples: Synthesizing the Evidence on the Impacts of Digital Resources. Report for JISC. Oxford: Oxford Internet Institute, University of Oxford, 2011. Meyer, E.T., K. Eccles, M. Thelwall and C. Madsen. ‘Final Report to JISC on the Usage and Impact Study of JISC-Funded Phase 1 Digitisation Projects & the Toolkit for the Impact of Digitised Scholarly Resources (TIDSR)’. Report for JISC, 2009. http://microsites.oii.ox.ac.uk/tidsr/system/files/TIDSR_ FinalReport_20July2009.pdf (accessed June 18, 2013). Oomen, J. and L. Aroyo. ‘Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges’. Proceedings of the 5th International Conference on Communities and Technologies, 138–49. New York, 2011. http://www. cs.vu.nl/~marieke/OomenAroyoCT2011.pdf (accessed June 17, 2013). Public Catalogue Foundation. ‘Framework for Classifying Paintings Online – Is It Fit for purpose?’. Internal document. Raddick, M.J. ‘Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9, no. 1 (2010). http://arxiv.org/ abs/0909.2925 (accessed June 17, 2013). Romeo, F. and L. Blaser. ‘Bringing Citizen Scientists and Historians Together’. In Museums and the Web 2011: Proceedings, edited by J. Trant and D. Bearman. Toronto, 2012. http://www.museumsandtheweb.com/mw2011/papers/ bringing_citizen_scientists_and_historians_tog (accessed June 20, 2013). Springer, M., B. Dulabahn, P. Michel, B. Natanson, D. Reser, D. Woodward and H. Zinkham. ‘For the Common Good: The Library of Congress Flickr Pilot Project’. Report, 2008. http://www.loc.gov/rr/print/flickr_report_final.pdf (accessed June 18, 2013). Trant, J. ‘Social Classification and Folksonomy in Art Museums: Early Data from the Steve.Museum Tagger Prototype’. Advances in Classification Research Online, October 17, 2006. https://journals.lib.washington.edu/index.php/acro/ article/view/12495 (accessed June 18, 2013). Trant, J. ‘Tagging, Folksonomy and Art Museums: Results of Steve.Museum’s Research’. Report, 2009. http://www.museumsandtheweb.com/files/ trantSteveResearchReport2008.pdf (accessed June 18, 2013). Trant, J. and B. Wyman. ‘Investigating Social Tagging and Folksonomy in Art Museums with Steve.Museum’. Paper presented at the Web Tagging Workshop at WWW2006, Edinburgh, May 23–26, 2006. http://www.ra.ethz.ch/cdstore/ www2006/www.rawsugar.com/www2006/4.pdf (accessed June 18, 2013). Warwick, C., M. Terras, P. Huntington and N. Pappa. ‘If You Build It Will They Come? The LAIRAH Study: Quantifying the Use of Online Resources in the Arts and Humanities through Statistical Analysis of User Log Data’. Literary and Linguistic Computing 23, no. 1 (2008): 85–102. PART II Challenges and Opportunities of Cultural Heritage Crowdsourcing This page has been left blank intentionally Chapter 9 Crowding Out the Archivist? Locating Crowdsourcing within the Broader Landscape of Participatory Archives Alexandra Eveleigh A working understanding of crowdsourcing has evolved amongst archives professionals from a combination of practical experimentation with participatory web tools and platforms,1 and rather more theoretical speculation about the transformative, democratising potential of such technologies.2 The term crowdsourcing may then be loosely, and is often retrospectively, applied to almost any initiative in the field which seeks to engage users to contribute to archives or to comment upon archival practice through the medium of the Internet. For example, crowdsourcing has been used by archivists to describe a public consultation exercise regarding archives policy,3 collaborative appraisal and collection development work,4 and a volunteer scanning programme.5 However, the word has perhaps come to be particularly associated with user involvement in archival description, transcription and metadata enhancement. In the field of archives, as in related information and cultural heritage domains, there has been a specific experimental focus upon the potential for users’ contributions to be employed to help address acknowledged problems in the description and representation of collections.6 These connections are the particular focus of this chapter on crowdsourcing in the archives domain. Access to, and use of, archival source materials depends to a large extent upon the availability of appropriate and effective access routes. Traditionally, these access systems have taken the form of textual descriptions, typically created by a single professional archivist. Commentators have noted that, while archivists 1.For case studies and examples, see Theimer, A Different Kind of Web; also Theimer, Web 2.0 Tools. 2.For instance, Flinn, ‘“An Attack on Professionalism and Scholarship”?’; Yakel, ‘Who Represents the Past?’. 3.Theimer, ‘NARA Crowdsourcing Classification Reform’. 4.Pennock, ‘Twittervane’. 5.Theimer, ‘Fantastic Volunteer Scanning Project’. 6.For example, for this issue discussed in a museums context see Karp and Lavine, Exhibiting Cultures. agree that archival description is important, there is considerable disagreement over what description is for (broadly dividing into those who focus on the record, and those who focus on users), the best descriptive method to implement and the point(s) in time at which description should take place.7 Thus understanding of the professionalised process known as ‘arrangement and description’is seen to be a ‘fraught terrain’,8 containing the tension inherent between a custodial instinct to control context and authenticity, and a desire to share access and promote usage. This fault line is deepened by the prospect of user participation in the descriptive process, since involving others in description seems inevitably to weaken the archivist’s control over the process but at the same time seeks to magnify the accessibility of the descriptive product. However, given that most archive organisations struggle with significant cataloguing backlogs, the idea that users might create, or supplement descriptions, has a clear, practical appeal. Currently, descriptive ‘finding aids’or catalogues to archives are often incomplete, or realised at an insufficient level of detail to satisfy the information-seeker’s needs, or in some cases are simply non-existent. Even where finding aids are available, they are rarely updated to reflect new information, interests or perspectives. As such, catalogues may as easily prove a barrier as an enabler of access. This is a consequence both of practical collections management issues – cataloguing failing to keep up with the pace of new accessions9 – but also of a professional compromise; a: resolution that [the products of description], in order to be useful for every kind of research, had to serve none in particular. Thus, all documents had to be described in equal depth, independently of their .importance. for one kind of research or another, and the descriptions had to emphasize context and function rather than content.10 Crowdsourced description promises a solution to these dilemmas, enabling description – even transcription – of content to take place at a detailed level of granularity across a broad range of subjects and collections. The term crowdsourcing makes an early appearance in the archival professional literature in 2008 from Isto Huvila who explicitly conceives of his concept of a radical participatory archive as something which goes beyond crowdsourcing: ‘Even though a participatory archive is about crowdsourcing, it focuses on deeper involvement and more complex semantics rather than on larger crowds 7.Yeo, ‘Debates about Description’; Hurley, ‘Parallel Provenance’; Duff and Harris, ‘Stories and Names’; Duranti, ‘Origin and Development of the Concept of Archival Description’. 8.Duff and Harris, ‘Stories and Names’. 9.Greene and Meissner, ‘More Product, Less Process’. 10.Duranti, ‘Origin and Development of the Concept of Archival Description’, 52. and simple annotations’.11 This enhanced sense of user participation in archives being necessarily associated with ‘a comprehensive shift in archival thinking and practice’12 remains a strong current in the archival literature, in contrast to a shallower engagement in crowdsourcing comprehended as ‘letting some others to (sic) play with (some of) my toys in my sandbox’.13 The rhetoric here is strongly informed by constructivist philosophies, in particular perhaps by a relatively late flourishing of postmodernist critique as a creative influence upon the development of archival theory.14 Participation in this vein is promoted as a means to address troubling issues of marginalisation and representation, professional passivity and power. Web 2.0 technology is then harnessed to this argument as a facilitating mechanism for achieving the vision of the archive as a community in a continual process of becoming, celebrating a multiplicity of different perspectives, meanings and contexts, and castigating the singularity of the authoritative, professional voice: Web 2.0 is about connection, collaboration, community. For archives, Web 2.0 connects communities with collections or, maybe even more conceptually, communities with their history and identity. What is more, it invites collaboration about that history: what it means, how it should be presented, and what we know. Shared authority and distributed curation are the point. Yet Web 2.0 technologies can be implemented and the community can be integrated in many different ways. This raises questions about how much authority we as archivists are willing to share and how to manage the voices of all those distributed curators.15 A growing number of essays by archivists take up this line and extol the transformative potential of the social web for ‘opening up’both archival content and the archive profession ‘for the people’.16 But the actual word ‘crowdsourcing’17 still appears only infrequently in the formally published archival literature, although it has gained greater traction in the professional blogosphere as a convenient shorthand. Instead, a host of alternative terms have been coined to describe internet-based user involvement and participation projects in the archives domain, including the Participatory Archive, Archives 2.0, Citizen Archivists, the Archival Commons and Citizen-Led Sourcing. Most archival commentators too have preferred a notion of community (translated online) over ‘the crowd’ as the conceptual model for online user participation – supporting a greater 11.Huvila, ‘Participatory Archive’, 27. 12.Theimer, ‘What Is the Meaning of Archives 2.0?’, 58. 13.Huvila, ‘What Is a Participatory Archive?’, emphases in original. 14.Kaplan, ‘“Many Paths to Partial Truths”’. 15.Yakel, ‘Who Represents the Past?’, 258. 16.Anderson and Allen, ‘Envisioning the Archival Commons’; Evans, ‘Archives of the People’. 17.Or ‘crowd sourcing’or ‘crowd-sourcing’ – the editorial confusion perhaps bears witness to archivists’ uneasiness about the use of this term. degree of self-regulation and project ownership amongst participants, who, it is acknowledged, may well be experts in their own particular field.18 And responding to the disappointment of some early experiments in online participation in archives which succeeded in attracting only nominal levels of engagement, the tendency has been to chide archivists for clinging to an archive-centred worldview; for their reluctance to share control and build equitable partnerships with these user communities – pushing further still at this agenda of anticipated professional revolution assisted by Internet technology.19 Crowdsourcing in contrast has been characterised as merely a technologically enhanced version of volunteering, limited in its scope and anticipated impact upon professional practice.20 Trevor Owens (Chapter 12) chooses to retain the crowdsourcing buzzword, but also acknowledges crowdsourcing’s debt, in libraries, archives and museums, to long-standing traditions of volunteering. He contends that most crowdsourcing projects in cultural heritage contexts ‘have not involved massive crowds and they have very little to do with outsourcing labour’(p. 269). But rather than paint the small numbers of engaged participants as indicative of a professional failure to embrace change and cede control to the community, he suggests instead that the key to success lies in ‘inviting participation from engaged members of the public’ (p. 269), passionate amateurs who already identify with a particular professional mind-set. Owens’ conception of crowdsourcing then, like Huvila’s participatory archive, offers an opportunity for citizens to engage deeply with cultural collections and to contribute in meaningful ways to the ‘public memory’. But this engagement is instead portrayed as a recognition and reinforcement of established identities through the incorporation of additional user knowledge into the existing professional domain of practice, rather than necessarily a source of innovation and creativity via an encountered heterogeneity of external opinion.21 Yet the hope of tapping into diverse new audiences, on a scale unimaginable without the Internet, making niche areas of history and research accessible and capturing the interest of a wider public – these remain the inspiration, if not perhaps always the reality, for many crowdsourcing-type ventures in archives and heritage practice contexts.22 Existing models of crowdsourcing in archives (and the humanities more generally) that posit close-knit communities continuing the volunteering tradition online seem inadequately flexible to incorporate this larger-scale ambition, or those instances where the participants’online interactions may only be fleeting or serendipitous (for example, comments added to digital images or catalogue entries encountered during research or in browsing the web). 18.Flinn, ‘“An Attack on Professionalism and Scholarship”?’; Palmer, ‘Archives 2.0’. 19.Yakel, ‘Who Represents the Past?’. 20.For example,the Archivist of the United States, David S. Ferriero, ‘Crowdsourcing and Citizen Archivist Program’, speaks of ‘crowdsourcing or microvolunteering’. 21.Owens, ‘Digital Cultural Heritage and the Crowd’. 22.Dunning, ‘Innovative Use of Crowdsourcing Technology’. Furthermore, crowdsourcing in archives at least is sometimes viewed as a wholly pragmatic solution to a lack of financial and labour resource: ‘crowdsourcing may help institutions faced with dwindling budgets address resource constraints by involving interested participants in the process of contributing metadata. […] If the experience engages participants and they value it, the “labor” involved in the exchange can be considered a voluntary, in-kind contribution’.23 Again, this circumstance is poorly served by existing conceptualisations of crowdsourcing and online participation, since it involves neither innovation in, nor in-depth engagement with, existing professional practice, but rather the reallocation of some of the more tedious or repetitive parts of the descriptive process to unpaid human-computational effort. Mapping the Participatory Landscape Despite the growing popularity of crowdsourcing and participatory practice in archives, there have been few attempts systematically to map and evaluate this landscape. The results of practical initiatives in the sector have been mixed, in spite of what one commentator calls the ‘triumphal rhetoric’24 of participatory archives culture. Whilst some projects report apparently runaway success (the Old Weather project, discussed in Chapter 2, in which participants transcribe meteorological observations from historic ships’logs, reported 685,000 log pages transcribed in one year, for example), others have struggled to attract the anticipated rich seams of user knowledge, and several have quietly closed or transferred their content onto ‘read only’ websites.25 The neologism ‘crowdsourcing’ also promotes an impression of transience, a passing fad; of participatory practice as merely an exercise in wanting to be seen as working at the cutting edge. This in itself may be limiting the potential of some participatory projects, contributing to an organisational reluctance to support and sustain their development beyond the pilot testing, experimental stage, and later, to a tendency to move on quickly to the next technological platform without a proper review of what worked or what did not. Crowdsourcing initiatives in archives, as in related professional fields, are also haunted and constrained by the fear that a contributor might be wrong, or that descriptive data might be pulled out of archival context, and that researchers using collaboratively authored resources might somehow swallow all of this without question or substantiation, in what has been described as a ‘fundamental change in 23.Flanagan and Carini, ‘How Games Can Help Us Access and Understand Archival Images’, 536. 24.Palmer, ‘Archives 2.0’. 25 For instance, the planned closure of the National Archives’ (UK) Wiki site, Your Archives, went ahead during 2012. http://webarchive.nationalarchives.gov.uk/ 20121030162709/http://yourarchives.nationalarchives.gov.uk/index.php?title=home_page. the relationship between researchers and archivists [and] between the records and researchers that leaves out archivists’.26 The ideological impetus which fuels much of the theoretical debate about participation in archives, both off- and online, can be equally unhelpful when it comes to evaluating practical initiatives and planning future ventures in this area. If crowdsourcing is an opportunity to democratise professional archival practice and to promote the active participation of the general public in co-creating historical meaning, how should a project be judged which fails to attract large numbers of new users, or where the interaction is brief or ephemeral? How and when should participants’ contributions be integrated with professionally authored resources such as the archive catalogue, particularly when the contributions received are emotive or illustrative in contrast to the traditions of archival description? And what role does the professional archivist have to play in this democratised archive, and who is responsible for supporting or verifying the accuracy and reliability of contributed information? A User Participation Matrix This chapter seeks to analyse the variety of online participation practice in archives through four frames. These four frames, or quadrants, come together to form a proposed matrix of user participation (Figure 9.1). The borders between the frames are fluid, but together the four quadrants provide a conceptual map to help make sense of the ambiguities and contradictions, ideological inclinations and diversity of configurations observed in contemporary crowdsourcing and other online participation initiatives in archives. The aim of the matrix then is not to provide a definition of crowdsourcing in the archives domain, but rather to set out a framework through which existing practical initiatives can be assessed, particularly in terms of their influence on archival professionalism. Should success in archival crowdsourcing be gauged in the same terms as online outreach or volunteering? Or if crowd initiatives can be distinguished within a broader concept of participatory practice, how might this insight affect the design of future ventures which seek to reach out to the widest range of participants? Nor is the matrix proposed as a strict classification of current programmes, for any single project may exhibit characteristics from more than one frame simultaneously. For instance, many transcription projects combine a directed primary task structure (the Transcription Machine) with a participant-managed forum to boost intrinsic engagement, and to facilitate learning amongst the members of the participant group (a Collaborative Community). But, since ultimately, achieving the objective of any participatory project is entirely dependent upon the response the project receives from participants, matching the intended outcomes to contributors’ likely motivations and exhibited patterns of behaviour is vital 26.Yakel, ‘Balancing Archival Authority’, 77. for the success of any project. The matrix then provides a tool through which to examine these various points of interaction between the project organisation (the upper and lower halves of the framework) and the participants (the left and right hand sides of the matrix). The upper and lower halves of the framework represent contrasting approaches to the structure and management of online participation projects, adopting Burns and Stalker’s classic distinction between ‘mechanistic’ and ‘organic’ styles of organisation.27 Here, this spectrum pertains to the structural coordination of online participation, whether focused upon specific goals and objectives (mechanistic) or taking a more flexible and open-ended approach (organic). It is important to note that whilst a mechanistic structure might be assumed to represent directed management by a formal institution, there are examples in participatory practice where a mechanistic approach has been designed and implemented by the participants themselves. Many programmes with a genealogical focus, for example, are entirely volunteer-initiated and run, but many of these are coordinated under a formalised command and control structure (for instance, the long-running FreeBMD project uses a syndicate system, with appointed local coordinators and separate data teams for checking accuracy). Other 27.Burns and Stalker, The Management of Innovation. projects operate as consortia led by external professionals and subject specialists, but have similar tiers of responsibility for separate parts of the processing of data. In the Old Weather project, for example, a specialist in naval history coordinates the extraction of chronological ships’ histories from the log data transcribed. The role of the custodial institution in such partnerships varies greatly: in some instances, the organisation is a formal project partner, and archivists and other heritage professionals are involved in the design or testing of the participation interface or in supplying expert contextual knowledge on archival sources; in other projects, the archives organisation may be merely a supplier of source material or digitised content. Clearly the impact of crowdsourcing upon professional practice will vary according to the specifics of such partnership arrangements, and also with the individual employer’s tolerance for professional autonomy. A risk-averse organisation may act as a restraint or deflection upon the potential for any transformation in the role of the professional archivist, perhaps unintentionally endorsing an overly cautious approach, in order to maintain the organisation’s reputation or traditional position of authority. The left and right sides of the participation matrix are created from Caroline Haythornthwaite’s ‘crowds and communities’ peer-production spectrum, representing the motivations and behaviours of online participants.28 ‘Crowd’ in this model does not necessarily refer to large numbers of people, but instead relates to the relative strength of the social ties between participants, being strong in a community, but becoming weaker as the continuum line moves towards the crowd. This conception of a crowd retains the all-embracing sense of scale which underpins the ambition to reach out to infinite new audiences, but also allows for comments and encounters of a more serendipitous nature from participants with no previous connection to a particular organisation or set of archive documents, or indeed to each other. The Archival Commons The Archival Commons metaphor characterises user participation with a strong ideological bent. This is an understanding shaped significantly by the popular rhetoric which promises a relentlessly positive social transformation on a dispersed, global scale through engagement with Web 2.0 technologies. Consequently, it is sometimes dismissed as a utopian and romantic vision.29 Nevertheless, it is a vision that continues to be a dominant influence in shaping and understanding crowdsourcing in archives, and within the cultural heritage sector more generally.30 28.Haythornthwaite, ‘Crowds and Communities’. 29.Schafer, Bastard Culture!. 30.For example, Smith-Yoshimura and Shein, Social Metadata for Libraries, Archives, and Museums; Zarro and Allen, ‘User-Contributed Descriptive Metadata for Libraries and Cultural Institutions’. A specifically archival vision of the Commons idea has been put forward in some detail in an eponymous 2009 article in the American Archivist. In essence, the concept could be summarised as an all-encompassing, postmodern, archival ecology. The article’s authors envisage ‘a decentralized market-based approach to archival representation’(elsewhere referred to as a ‘democratic culture’). They anticipate a shift away from a professionally regulated, ‘singular arrangement’of archives towards a malleable, continually evolving descriptive practice reflecting the ‘constantly changing views and meanings’ of archives. This is a vision of archives for a global, interactive society; an emergent, organic orientation representing a ‘sea change in how users engage’ with archives online.31 As its ‘crowd’position in the user participation matrix indicates, this ‘distributed but integrated’mode of production is envisaged to operate at a cross-repository, cross-domain, ‘web of connectivity’32 magnitude, reaching ‘thousands of potential volunteers’.33 This sense of infinite scale is seen in the promotional texts used for archives’participation initiatives: an inclusive, welcoming vocabulary – explore, share, collaborate, contribute; an ambition to reach as many people as possible, particularly the elusive ‘new users’; and an awareness of archives’wider contexts. Recent developments with Linked Data (structured data that can more easily be linked to other data sets) are also beginning to put in place a plausible technological underpinning to the Commons concept, providing the elasticity required to serve the varied and unpredictable demands of a technologically astute Interactive User Community34 and extending the shareable, extensible, flexible principles of the Commons into the realm of open data reuse. But for the most part, whilst the Archival Commons remains a source of inspiration, it seems current practice is more constrained. The Commons concept relies substantially upon users being willing and able to participate and on archivists being prepared to accept their contributions. This would appear to imply that the envisaged contributors to the Commons (researchers, historical society members and students are specifically mentioned) have, like Owens’ crowdsourcing volunteers, some prior experience of the archival domain, and feel motivated to impart their knowledge in a public space. Unfortunately, the dispersed, global nature of the Commons could militate against both of these characteristics by increasing the likelihood of incidental participation from individuals who have encountered archives by chance online, who may not contribute what archives organisations expect to hear or express themselves in ways in which archivists want to hear it. Archivists become caught between the conflicting trajectories of an imagined radical professional transformation and the defence of their employer’s reputation. This is not merely a matter of inappropriate language or the pedalling of historical myths and falsehoods, as much as a misalignment between a professional 31.Anderson and Allen, ‘Envisioning the Archival Commons’, 384–90. 32.Ibid., 389. 33.Evans, ‘Archives of the People, by the People, for the People’, 395. 34.Anderson, ‘Necessary but Not Sufficient’. understanding of description and users’more often emotive and personal response to archives. It appears too that the very openness of the Commons – the weakness of social connections amongst the crowd and the consequent fragility of trust between them – may act as a barrier to participation by precisely those more expert users whom archivists had most hoped to attract. Furthermore, for all the avowed intent to create an open, inclusive space, the Archival Commons concept still reserves a particular centrality for the archives repository as a memory institution. Certainly practical experience of crowdsourcing initiatives inspired by the Commons ideal is already suggesting that ‘if we build it’, we cannot assume that ‘they’will come.35 Outreach and Engagement As an alternative then to building bespoke platforms, some archivists have turned to external social media services, such as Flickr and HistoryPin, to provide a space for user participation, and to furnish access to communities beyond individual archives’ immediate, local audience boundaries. Elizabeth Yakel notes how engaging with these third-party services shows ‘the initiation [of archives] into and understanding of social norms in these peer production systems’.36 Such ventures are rationalised as an exercise in taking archive material out to a place where an interested audience already exists, and have much in common with traditional audience engagement and marketing initiatives, extended in reach and ambition by means of the internet. Sometimes, in-person outreach events may be incorporated into online projects as a means of building community around the archival content. Yet for all their avowed intent of dipping into spaces inhabited by users, these outreach-type projects take a strong strategic steer from the archives organisation, with participation taking place according to a planned timetable and organisationally defined remit. This mechanistic notion of project planning and management also necessitates clearly defined objectives (rather than outcomes emerging according to the participants’interests), and the impact of the participation is bounded in terms of time-scales, carefully selected archive content and target user communities. Approaching a tightly knit community of interest with such a mechanistic approach to project coordination can lead to some structural friction. Community engagement strategies may aspire to a bilateral exchange between professionals and participants, but the boundaries between ‘us and them’ remain substantially intact. Contributions are treated as supplemental rather than fundamental, since crowdsourcing in this vein seems to require a bedrock structure of professional description onto which participants are invited to add embellishments. And since participation in this outreach mode is generally staged in spaces apart from the main archive website, the results of such crowdsourcing projects are often poorly integrated with finding aids and other organisational web resources. This restricts 35.Palmer, ‘Archives 2.0’. 36.Yakel, ‘Balancing Archival Authority’, 86. the impact that the interaction with new users might otherwise have had upon either professional practice, or upon established researchers who have no reason to encounter the contributed information in the course of their habitual work routines. Nevertheless, success in this style of participatory practice is still contingent upon professional sensitivity to the user environment, in order to be able to identify, and negotiate relationships of trust with suitably motivated participant communities. Participant ‘energy’ is sought to promote the sustainability of the archival enterprise by widening the pool of advocates for the activities of archives. A professional renewal then, if not a professional re-birth, this outreach form of participatory practice is a natural response to turbulence and complexity in the external operating environment, to economic pressures and to public policy and legislative shifts which challenge archivists to show strong leadership in shaping the future of the profession towards greater openness and flexibility.37 The role of the archivist here seems to be that of intermediary between organisation and target community, or a boundary gatekeeper maintaining the archivist’s position of authority.38 Editorial control in these outreach enterprises is usually reserved to a staff moderator, and contributions may even be rejected according to professional judgements about suitability and pertinence. But this role can only be performed successfully if archivists also operate within a new degree of empathy for the participants’points of view, and accept responsibility for a deeper involvement in interpreting and presenting the archival record. Yakel argues that the authority claimedhere is a kind of cognitive influence –the archivist and archives institution acting in concert as a proxy for personal knowledge of the accuracy of archival finding aids and the authenticity of the records described therein.39 It is authority which many users indeed may be willing to recognise in the archivist, since it implies no ‘right to command’, and also lessens the filtering and verification burden on research user. But it is also vulnerableto allegations of censorship, even where the archivist operates a relaxed moderation policy. Collaborative Communities More rarely, a more thoroughgoing remodelling of archival practice is sought which aims to break down, or at least redraw, the boundaries between archivists and participants. To achieve this shift, both archives organisations and the archival profession actively seek to embrace uncertainty in the environment, as a catalyst towards new ways of working, scanning the horizon for newly emergent 37.Morgan, Images of Organization. A useful summary of recent developments towards greater openness and accountability relating to the UK archives sector can be found in Dacre, Review of the 30 Year Rule. 38.Duff et al., ‘Finding and Using Archival Resources’; Hedstrom, ‘Archives, Memory, and Interfaces with the Past’. 39.Yakel, ‘Balancing Archival Authority’. opportunities. This frame sees archival practice in a state of evolution and flux, indicating a greater depth of change than the professional renewal brought about through outreach and engagement projects. In particular, before archivists can participate equitably in a collaborative community with participants, they must first address any cultural issues over sharing knowledge within their own domain. The literature is critical of the high visibility of archivists, rather than users, on some public participation sites.40 But an alternative reading might see this as an important staging post in the transformation of archival practice, in learning to share archival knowledge more openly, and in new and more adaptable ways. In this way, archivists can begin to identify areas where current professional processes and services fail to meet user needs. The next step is to entrust specific user communities to help resolve these issues, by reorienting participation opportunities around the intrinsic interests of the diverse communities of practice which already surround the archival record (for instance, family historians, geographers, economic historians might use the same source, but in different ways).41 When user communities are invited to input into the processes of participation in this way, as well as to contribute content, the results are no longer restricted by the established structures of acceptable professional archival practice. The professional role is reoriented away from a mechanistic focus on strong archival leadership and hierarchically determined goals towards a new emphasis upon facilitation, dispersed community coordination and emergent design. This may demand unaccustomed levels of professional humility: recognising that archivists too make errors, and welcoming dispute and debate around the contents of a catalogue in ‘always beta’.42 Yet it is by handing over some responsibility for the maintenance of community norms and standards, and for the direction and sustainability of the site of participation, that archivists seek to resolve the tension of cognitive authority encountered in outreach initiatives.43 Participation can then begin to move beyond a channelled exchange of supplementary descriptive information towards a deeper understanding of historical sources as genuinely new knowledge and unanticipated discoveries emerge from the network of (redundant) connections. The greater freedom granted to participants within a collaborative community can also lead to the creation of new descriptive services, such as visual finding aids or ‘mash-ups’ using archival data.44 40.Ibid.; Palmer, ‘Archives 2.0’. 41.Wenger, Communities of Practice. 42.Yeo, ‘Debates about Description’, 102. 43.Yakel, ‘Balancing Archival Authority’; Wasko and Teigland, ‘Public Goods or Virtual Commons?’. 44.See, for example, the Guardian’s visualisation of data from the Old Weather project: http://www.guardian.co.uk/news/datablog/interactive/2012/oct/01/first-world-war.royal-navy-ships-mapped. Transcription Machines If outreach-style participation is to defend the professional boundary, Collaborative Communities seek to redraw it, and the Archival Commons to dissolve it, a fourth option is to reinforce it. Rules and structure are imposed from above to ensure consistent, standardised input (and output). Quality control becomes a matter of consecutive processing up through a hierarchical chain of command, combined with double (or sometimes triple) entry, statistical sampling and automated error detection. The emphasis in a Transcription Machine is on bureaucratic or administrative control over user input, which is characterised by the reductive nature of both the participative task and of contributors’ commonly fleeting commitment to that task and to each other. Participants may shun opportunities to contribute beyond the basic data input task (for instance, only a small proportion of registered members of projects like Old Weather are regular contributors to the project forum). The issues of attracting participants, and of motivating and rewarding performance may even be implemented as a competitive game.45 This mechanical image of user participation can even be extended, metaphorically and also literally, into the ways in which archival metadata can be released for use through the structured delivery mechanisms of APIs (application programming interfaces) and Linked Data. The impact upon the professional role here is not transformation so much as extension or translation of function. The enforced consistency can be viewed as a continuation of the international standardisation of archival descriptive practice, extending control over input standards down to a micro-level which enables this type of crowdsourcing to operate across consortia of different organisations and subject interests. Meanwhile, responsibility for the actual process of data input transfers to the users, the archivist taking on more of a coordinating role, released from the drudgery of the routine and freed up to concentrate on tasks demanding a greater degree of professional skill. But this shift might also harbour a hidden threat to professionalism. The bureaucratic nature of the authority wielded here is not relative to a particular sphere of interest or expertise,46 and hence does not of necessity have to be exercised by professional archivists, and the reductive nature of the transcription task is easily dismissed as beneath the professional dignity of an archivist. Archives organisations have outsourced responsibility for many such projects to a range of external delivery partners, ranging from the entirely volunteer-led and managed, to subject specialist consortia, to commercial enterprises such as Ancestry. So whereas one benefit of the more community-focused forms of user participation is the advocacy role that such projects can play in raising the profile of professionalised archive services, participants in outsourced transcription machines may be disinterested or 45.For example, Flanagan and Carini, ‘How Games Can Help Us Access and Understand Archival Images’. 46.Wilson, Second-Hand Knowledge. simply unaware of any link to a formal repository or of any input of expertise made to the project by professional archivists. Furthermore, restrictive licensing deals or simply a lack of foresight over data rights can also lead to a loss of archival control over the extensive quantities of descriptive metadata generated by such projects. This is a particular issue in the context of the increasing prominence of open data and the potential for ‘big data’computational analysis to transform historical research using archives. As the role of the archivist shifts away from the sole authorship of description, a new opportunity or imperative opens up in respect of descriptive information retrieval: linking together the multiple representations and contexts of each archival asset, and devising new tools for filtering, searching and understanding the historical world: ‘Gatekeeping of information resources shifts from contribution to retrieval. When “anyone” can post to the web, the value is in being retrieved’.47 Conclusion Writing about the relationship between museum computing practice and the emergent theory of digital heritage, Ross Parry has observed that ‘commentators …have too easily adopted the posture of either advocate or sceptic’. He identifies a need to find ‘more nuanced ways of thinking and communicating’which resolves this polarity between the advocate’s enthusiasm for professional revolution on the one hand, and the sceptic’s fear of professional demise on the other.48 This chapter has proposed a framework to support such an analysis of crowdsourcing in the archives domain, given that crowdsourcing initiatives sit within a broader landscape of participatory practice similarly moulded by the intersection of theory and practical experimentation. Using the matrix to reflect on the strengths and weaknesses of current practice, it becomes evident that existing community-focused theories of crowdsourcing as a deeply engaging, collaborative participant experience, are not sufficiently elastic to accommodate the task-driven, individual involvement of the majority of contributors to a Transcription Machine, for instance, just as innovation in professional practice has not inevitably resulted from the targeting of specific expert communities in outreach and engagement programmes. Yet the encounter between profession and participants might still be productive even where it is not transformative. Susanne Justesen distinguishes between learning, which she defines as ‘more knowledge about an existing domain’, and innovation, which ‘is about the exploration and creation of new domains’.49 Hence an outreach and engagement project might prompt the diffusion of existing, but perhaps latent or particularly specialist, knowledge, whilst a basic Transcription 47.Haythornthwaite, ‘Crowds and Communities’, 8, emphasis in original. 48.Parry, ‘Digital Heritage and the Rise of Theory in Museum Computing’, 343. 49.Justesen, ‘Innoversity in Communities of Practice’, 84, citing Katz and Lazer, ‘Building Effective Intra-Organizational Networks’. Machine is designed to augment professional knowledge with additional layers of detail which cannot (currently at least) be extracted from manuscript source material algorithmically by computer.50 These are both examples of learning, but may leave professional practice itself relatively unscathed, since participation is channelled towards a pre-determined output or an outcome designed to complement rather than change established professional work methods. In the context of citizen science, Bonney et al. have termed such initiatives contributory projects, as contrasted with collaborative or co-created projects, where participants have more input into the design of the work they undertake.51 In some instances, particularly in Transcription Machine projects, outsourcing a task to the crowd may even substitute for paid labour, but this is generally justified as freeing up professional time to focus on more expert phases of the overall process. In the Transcribe Bentham project, for instance, participants carry out the initial transcription work which might previously have been undertaken by an editorial assistant, enabling staff to concentrate on the more detailed work of preparing the transcriptions for formal publication. Such contributory projects are often conceived and understood as enriching a pre-existing barebones informational structure about a particular set of historical sources, whereby supplementary knowledge is ‘pulled’ from the participant and embedded within the professional domain of practice. Yet the transfer of knowledge described here as learning may equally operate in the opposite direction, and relate to participants’initiation into professional norms of practice –such as learning the expert language used to describe a medieval charter, perhaps, or the terminology of the nineteenth-century Poor Law. In other instances, the professional role might lie specifically in providing learning materials, for example supplying expert collections knowledge to external consortia. Justesen additionally connects the complexity of the knowledge exchange which takes place to the strength of the ties between participants. The homogeneity of a tightly bonded community is said to facilitate more complex learning or innovation52 – in the case of crowdsourcing in cultural heritage, perhaps around sensitive topics or issues of some historical or technical intricacy – whereas the diversity of weakly connected individuals who make up the crowd are well placed to diffuse less specialised knowledge or more widely appealing content into a range of different external contexts or in ways novel to the archival and related professions. An example might be the PhotosNormandie Flickr project which deliberately replicated (described as ‘liberated’) out-of-copyright images 50.Quinn and Bederson, ‘Human Computation’, would site the task of transcribing handwritten data in the intersection between crowdsourcing and human computation, allowing for some work to be undertaken by a single person in isolation as well as in the context of a group. 51.Bonney et al., Public Participation in Scientific Research. 52.Justesen, ‘Innoversity in Communities of Practice’, citing Katz and Lazer, ‘Building Effective Intra-Organizational Networks’ and Hansen, ‘The Search-Transfer Problem’. outside of the professional custodial context in order to boost access and interaction with the photographs.53 In all cases of learning, the host sphere of practice (‘competence regime’) remains strong, absorbing new knowledge into the established domain rather than being challenged by it.54 In contrast, more innovative outcomes are achieved when the participants’ outsider perspective is able to influence an established specialist or professional domain to shift in new directions – although learning is also a pre-requisite for innovation, and vice versa, so this process is cyclical and iterative. One example of this might be the committed community moderators and super-contributors in a Transcription Machine who make suggestions for project enhancements and adaptations. Other participants may wish to use the transcribed information in their own research, or to explore and experiment with new ways of presenting and understanding the data. Many of these innovations will be in themselves just small-scale redefinitions of practice on the boundary between professional and participant communities, but together they can contribute towards some broader shifts of perspective – Justesen labels this process ‘incremental innovation’.55 Included here might be the growing appreciation amongst cultural heritage professionals of the sustained effort required to motivate and support contributors, moving well beyond a simple understanding of participatory practice as simply ‘a means of designing a better and more user-friendly finding aid or of crowd-sourcing metadata in an era of diminishing resources’.56 More radical innovations, which occur when a completely new knowledge domain is created, are often the aspiration of participatory projects established with an adaptive, organic orientation, particularly of the Archival Commons’ vision for the global networked environment ‘where archives are not singular destinations for research and inquiry, but are integrated into the daily fabric of activities’.57 Innovation on this scale is hard to pinpoint whilst in progress, but might it be detected in the convergence of traditional cultural heritage disciplines (archives, museums, special collections and so forth) and the emergence of the digital humanities as a creative influence upon the future development of the professional role within this much broader landscape? Crowdsourcing in cultural heritage is ultimately all about making connections – in its different guises these may be connections between traces of the past or between people in the present. Perhaps its enduring legacy will be in fostering the participants’ perspective of the digitised cultural heritage realm, encouraging professionals in these fields too to transcend their own view of the world – to focus then not inwardly on narrowly defined disciplinary goals, but to look outwards, embracing complexity and uncertainty, but also opportunity. 53.Peccatte, ‘Liberating Archival Images’. 54.Justesen, ‘Innoversity in Communities of Practice’, 83–4, 89. 55.Ibid. 56.Yakel, ‘Who Represents the Past?’, 258. 57.Anderson and Allen, ‘Envisioning the Archival Commons’, 400. References Anderson, I.G. ‘Necessary but Not Sufficient’. D-Lib Magazine 14, no. 1/2 (2008). doi:10.1045/january2008-anderson. Anderson, S.R. and R.B Allen. ‘Envisioning the Archival Commons’. American Archivist 72, no. 2 (2009): 383–400. Bonney, R., H. Ballard, R. Jordan, E. McCallie, T. Phillips, J. Shirk and C.C. Wilderman. Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Information Science Education. A CAISE Inquiry Group Report. Washington, DC: Center for Advancement of Informal Science Education (CAISE), 2009. http://caise.insci.org/uploads/docs/PPSR%20 report%20FINAL.pdf. Burns, Tom and G.M. Stalker. The Management of Innovation. London: Tavistock, 1961. Dacre, Paul. Review of the 30 Year Rule. London: The Stationery Office, January 2009. http://www2.nationalarchives.gov.uk/30yrr/30-year-rule-report.pdf. Duff, W.M., B. Craig and J. Cherry. ‘Finding and Using Archival Resources: A Cross-Canada Survey of Historians Studying Canadian History’. Archivaria 58 (2004): 51–80. Duff, W.M. and V. Harris. ‘Stories and Names: Archival Description as Narrating Records and Constructing Meanings’. Archival Science 2, no. 3 (2002): 263– 85. Dunning, Alastair. ‘Innovative Use of Crowdsourcing Technology Presents Novel Prospects for Research to Interact with Much Larger Audiences, and Much More Effectively Than Ever Before’. LSE Impact of Social Sciences blog, August 25, 2011. http://blogs.lse.ac.uk/impactofsocialsciences/2011/08/25/ innovative-use-of-crowdsourcing/. Duranti, L. ‘Origin and Development of the Concept of Archival Description’. Archivaria 35 (1993): 47–54. Evans, M.J. ‘Archives of the People, by the People, for the People’. American Archivist 70, no. 2 (2007): 387–400. Ferriero, David S. ‘Crowdsourcing and Citizen Archivist Program’. AOTUS: Collector in Chief blog, May 19, 2011. http://blogs.archives.gov/aotus/?p=2938. Flanagan, Mary and Peter Carini. ‘How Games Can Help Us Access and Understand Archival Images’. American Archivist 75, no. 2 (2012): 514–37. Flinn, Andrew. ‘“An Attack on Professionalism and Scholarship”? Democratising Archives and the Production of Knowledge’. Ariadne 62, January 30, 2010. http://www.ariadne.ac.uk/issue62/flinn/. Greene, M.A. and D. Meissner. ‘More Product, Less Process: Revamping Traditional Archival Processing’. American Archivist 68, no. 2 (2005): 208–63. Hansen, Morten T. ‘The Search-Transfer Problem: The Role of Weak Ties in Sharing Knowledge across Organization Subunits’. Administrative Science Quarterly 44, no. 1 (1999): 82–111. Haythornthwaite, Caroline. ‘Crowds and Communities: Light and Heavyweight Models of Peer Production’. In Proceedings of the 42nd Hawaii International Conference on System Sciences, 1–10. Los Alamitos, CA: IEEE Computer Society, 2009. Hedstrom, M. ‘Archives, Memory, and Interfaces with the Past’. Archival Science 2, no. 1 (2002): 21–43. Hurley, C. ‘Parallel Provenance: (1) What, If Anything, Is Archival Description?’. Archives and Manuscripts: The Journal of the Archives Section, the Library Association of Australia 33, no. 1 (2005): 110–45. Huvila, I. ‘Participatory Archive: Towards Decentralised Curation, Radical User Orientation, and Broader Contextualisation of Records Management’. Archival Science 8, no. 1 (2008): 15–36. Huvila, I. ‘What Is a Participatory Archive?’, August 24, 2010. http://istohuvila. eu/what-participatory-archive. Justesen, Susanne. ‘Innoversity in Communities of Practice’. In Knowledge Networks: Innovation through Communities of Practice, by P.M. Hildreth and C. Kimble, 79–95, Hershey, PA: Idea Group, 2004. Kaplan, E. ‘“Many Paths to Partial Truths”: Archives, Anthropology, and the Power of Representation’. Archival Science 2, no. 3 (2002): 209–20. Karp, Ivan and Steven Lavine, eds. Exhibiting Cultures: The Poetics and Politics of Museum Display. Washington, DC: Smithsonian Institution Press, 1991. Katz, Nancy and David Lazer. ‘Building Effective Intra-Organizational Networks: The Role of Teams’. Working Paper, Kennedy School of Business, Harvard University, 2003. Morgan, Gareth. Images of Organization. Updated Edn. London: SAGE, 2006. Owens, Trevor. ‘Digital Cultural Heritage and the Crowd’. Curator: The Museum Journal 56, no. 1 (2013): 121–30. Palmer, Joy. ‘Archives 2.0: If We Build It, Will They Come?’. Ariadne 60 (2009). http://www.ariadne.ac.uk/issue60/palmer. Parry, R. ‘Digital Heritage and the Rise of Theory in Museum Computing’. Museum Management and Curatorship 20, no. 4 (2005): 333–48. Peccatte, Patrick. ‘Liberating Archival Images: The PhotosNormandie Project on Flickr’. In A Different Kind of Web: New Connections between Archives and Our Users, edited by K. Theimer, 148–57. Translated by Lynne Thomas. Chicago: Society of American Archivists, 2011. Pennock, M. ‘Twittervane: Crowdsourcing Selection’. UK Web Archive blog, December 2, 2011. http://britishlibrary.typepad.co.uk/webarchive/2011/12/ twittervane.html. Quinn, A.J. and B.B. Bederson. ‘Human Computation: A Survey and Taxonomy of a Growing Field’. In Proceedings of the 2011 Annual Conference on Human Factors in Computing Systems, 1403–12. New York: Association of Computing Machinery, 2011. Schafer, Mirko Tobias. Bastard Culture! How User Participation Transforms Cultural Production.Amsterdam: Amsterdam University Press, 2011. Smith-Yoshimura, K. and C. Shein. Social Metadata for Libraries, Archives, and Museums. Part 1: Site Reviews. Dublin, OH: OCLC Research, 2011. http://www. oclc.org/research/publications/library/2012/2012-01r.html?urlm=161049. Theimer, K. Web 2.0 Tools and Strategies for Archives and Local History Collections. London: Facet, 2010. Theimer, K. ‘Fantastic Volunteer Scanning Project with NationalArchives – Great Example of Crowdsourcing (and Interesting Remarks from Mr. Ferriero)’. Archivesnext blog, February 11, 2010. http://www.archivesnext.com/?p=1018. Theimer, K. ‘NARA Crowdsourcing Classification Reform’. Archivesnext blog, March 18, 2011. http://www.archivesnext.com/?p=1823. Theimer, K. ‘What Is the Meaning of Archives 2.0?’. American Archivist 74, no. 1 (2011): 58–68. Theimer, K., ed. A Different Kind of Web – New Connections between Archives and Our Users. Chicago: Society of American Archivists, 2011. Wasko, M.M.L. and R. Teigland. ‘Public Goods or Virtual Commons? Applying Theories of Public Goods, Social Dilemmas, and Collective Action to Electronic Networks of Practice’. Journal of Information Technology Theory and Application (JITTA) 6, no. 1 (2004). http://aisel.aisnet.org/jitta/vol6/iss1/4. Wenger, Etienne. Communities of Practice: Learning, Meaning, and Identity. Learning in Doing. Cambridge: Cambridge University Press, 1998. Wilson, P. Second-Hand Knowledge: An Inquiry into Cognitive Authority. Westport, CT: Greenwood Press, 1983. Yakel, E. ‘Balancing Archival Authority with Encouraging Authentic Voices to Engage with Records’. In A Different Kind of Web: New Connections between Archives and Our Users, edited by Kate Theimer, 75–101. Chicago: Society of American Archivists, 2011. Yakel, E. ‘Who Represents the Past? Archives, Records, and the Social Web’. In Controlling the Past: Documenting Society and Institutions (Essays in Honor of Helen Willa Samuels), edited by Terry Cook, 257–78. Chicago: Society of American Archivists, 2011. Yeo, G. ‘Debates about Description’. In Currents of Archival Thinking, edited by T. Eastwood and H. MacNeil, 89–114. Westport, CT: Libraries Unlimited, 2010. Zarro, M. and R. Allen. ‘User-Contributed Descriptive Metadata for Libraries and Cultural Institutions’. In Research and Advanced Technology for Digital Libraries Lecture Notes in Computer Science 6273, 46–54. Presented at the 14th European Conference ECDL Glasgow, UK, September. Berlin and Heidelberg: Springer, 2010. This page has been left blank intentionally Chapter 10 How the Crowd Can Surprise Us: Humanities Crowdsourcing and the Creation of Knowledge Stuart Dunn and Mark Hedges Introduction The term crowdsourcing is frequently used as a convenient label for a diverse range of activities and projects involving the public doing something to, or with, content. This chapter seeks to build on our 2012 Scoping Study1 for the United Kingdom’s Arts and Humanities Research Council (AHRC) to identify how academic uses of crowdsourcing might be framed, especially with regard to the creation of complex content and the circulation of knowledge, rather than simply as the distributed accomplishment of manual or mechanical tasks. The latter derives from a conception of crowdsourcing based on business and production, rather than creating knowledge or enabling understanding. The chapter will also examine how the imperatives and research questions of the humanities are driving more complex processes of knowledge generation by crowdsourcing using more complex tasks and processes, and considers next steps for this emergent field. The early history and association of the term ‘crowdsourcing’ is itself one of business, being coined in a 2006 Wired article2 as a parallel with the then-emerging trend of outsourcing. Howe wrote of the conceptual similarity between businesses farming out labour to cheaper markets in the developing world, and utilising ‘the productive potential of millions of plugged-in enthusiasts’, with similar reduction in labour costs. For application to an academic or cultural heritage context, in the creation or processing of scholarly content, this creates problems. For example it conjures notions of exploitation, manipulation and amateurisation: terms at odds with the aims of academia. Brabham speaks of crowdsourcing as ‘[a]n emerging, successful, alternative business … substantially different from open-source production – and superior in many ways. I also argue that crowdsourcing is a legitimate, complex problem-solving model, more than merely a new format for holding contests and awarding 1.Dunn and Hedges, ‘Crowd-Sourcing Study’. 2.Howe, ‘The Rise of Crowdsourcing’. prizes’.3 Even in the current early stages of cultural and academic crowdsourcing, this concept inevitably needs to undergo a certain amount of re-configuration in becoming a method (or rather a set of methods) for academics to use to further their research aims. To function as a means of creating and/or enhancing academic knowledge, rather than as a business model, it requires an epistemic model which enables the identification of replicable and meaningful ways of creating (or somehow improving or enhancing) scholarly knowledge. Academic Crowdsourcing Academic applications of crowdsourcing generally start from models of management and inception similar to those described by Howe or Brabham, in which distributed groups of participants contribute time and effort, usually to create resources or to add metadata.4 The startling and well-publicised success of some early efforts in this direction in so-called ‘citizen science’ embedded this concept, and the impact of such efforts has been felt not least because of their sheer scale: in the well-known case of Galaxy Zoo, some 200,000 volunteers made 100 million classifications of images of galaxies between July 2007 and April 2009.5 Meanwhile, researchers in the humanities have begun to investigate the potential of crowdsourcing for the development and processing of resources, and for the development and application of metadata, through such projects as Transcribe Bentham6 and Old Weather,7 a project to transcribe meteorological observations from historical ships’ logs and thus provide valuable information on climate change, attracted some 12,000 volunteer transcribers (see also Chapters 2 and 3 of this volume). This includes large numbers of contributors who contribute relatively little, but the overall volume of transcriptions is extremely high.8 Such forms of crowdsourcing, in both the humanities and the sciences, share certain important characteristics. They are primarily concerned with the creation of large-scale structured digital resources from raw data, using means that could not be accomplished by automation. The barriers to participation are relatively low: the core tasks are straightforward, self-contained and relatively easily learned. Undertaking them requires enthusiasm, motivation and time, and in some cases a willingness to learn new or more advanced skills, rather than specific expert knowledge or qualifications – although engaging with content via such tasks means that the participant has opportunities to become familiar with the content and, over time, even expert in it. This, as shall be seen, is important. 3.Brabham, ‘Crowdsourcing as a Model for Problem Solving’, 76. 4.Dunn and Hedges, ‘Crowd-Sourcing Study’. 5.Raddick et al., ‘Galaxy Zoo’. 6.Causer et al., ‘Transcription Maximized’. 7.Brohan et al., ‘Marine Observations of Old Weather’. 8.Ibid. Most discussions treat crowdsourcing as being distinct from the concept of collective decision-making. If we are to frame crowdsourcing in epistemic terms, as a means of creating and exploring knowledge, this surely makes sense: if the production of such knowledge is a pursuit conditioned by academic discipline and method, then the ‘Wisdom of Crowds’9 is by definition undisciplined. The ‘Wisdom of Crowds’ thesis argues that large-scale collective decision-making can be superior to that of individuals, even experts. It lacks the elements of shared endeavour, of collaboration around targeted activities conceived and directed for a common purpose that characterise crowdsourcing as commonly understood. Furthermore, an academic judgement arrived at by these means is likely to lack the traceability, and thus the contestability that make academic judgements valid (or not). It should be noted that while ‘traceability’can be achieved by using Wikipedia-style tracking pages, this does not necessarily allow trust to be accorded to the changes tracked. Academic crowdsourcing can be about contributing to academic decisions, although yet another distinction between a business model and an epistemic model is that the judgements involved in the latter are rarely as neatly packageable as those implied in business, where the ‘good’ or ‘bad’ nature of a decision can be evaluated on the basis of profitability.10 Defining Crowdsourcing No doubt as a result of the emergence of crowdsourcing as a means of creating academic resources, the academic literature has seen several attempts to define it in recent years. However, our Scoping Study found that the often very idiosyncratic nature of the data involved – and, even more so, of what humanists want to do with it – makes developing such a definition extremely difficult. The interests and motivations that drive people to contribute are similarly disparate; the citizen science projects cited above have found similar disparity in their contributor communities. For example, maps and mapping hold wide popular appeal as a subject area, but people who are capable of annotating or referencing a map of a particular area with local knowledge will, by definition, be a small subset of that larger interested group, with a very personal set of unique motivations. Estellés-Arolas and González-Ladrón-de-Guevara11 adopt a highly structured definition of crowdsourcing in their study, in which they identify eight characteristics derived from 32 distinct definitions identified by a review of the literature: the crowd; the task at hand; the recompense obtained; the crowdsourcer or initiator of the crowdsourcing activity; what is obtained by them following the crowdsourcing process; the type of process; the call to participate; and the Surowiecki, The Wisdom of Crowds. 10.Brabham, ‘Crowdsourcing as a Model for Problem Solving’. 11.Estellés-Arolas and González-Ladrón-de-Guevara, ‘Towards an Integrated Crowdsourcing Definition’. medium. Aprocessual definition of this kind largely disregards diversity in the type of material being crowdsourced, and also assumes a straightforward relationship with the outcome. Furthermore, for the humanities, the ‘type of process’ is both more significant and more problematic, given the great diversity of processes in the creation of humanities research material, and in the difficulty of articulating post hoc definitions of humanities research methods (as opposed, say to those of the sciences or social sciences). Amore task-oriented approach is that of Wiggins and Crowston,12 who focus their typology on the structural and organisational aspects of ‘citizen science’ activities. The use of the word ‘science’ (at least in the usual Anglophone sense) confines the activities reviewed to a particular epistemic bracket, which inevitably excludes some aspects of humanities research. Wiggins and Crowston identify five areas of application: Action; Conservation; Investigation; Virtual; and Education. The factors that lead to an activity being assigned to a category are various and the identification of the categories is based on whether or not there is an occurrence in a category, rather than frequency of those occurrences. The coverage is therefore extremely broad; ‘Action’, for example, covers self-organising citizen groups that use web technologies to achieve a common purpose, often to do with campaigns on local issues. Bonney et al. adopt a more managerial perspective in their report for Center for the Advancement of Informal Science Education (CAISE), Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education.13 Adopting a similar organisational approachto that of Wiggins and Crowston, they divide the field into three broad categories: contributory projects, in which members of the public, via an open call, contribute along lines that are tightly defined and directed by scientists; collaborative projects, which have a central design but to which members of the public contribute data, and may also help to refine project design, analyse data or disseminate findings; and finally co-created projects, which are designed by scientists and members of the public working together and for which at least some of the public participants are actively involved in most or all steps of the scientific process. Most academic citizen science projects, and humanities crowdsourcing activities that derive directly from citizen science approaches, fall into the category of ‘collaborative projects’. They are conceived and designed by research teams, the methodology is decided by that team and the material to be crowdsourced – which is driven by the team’s research interests – is provided by the team. It is often claimed that crowdsourcing empowers the crowd and democratises information, but this assumption must be seen in the context of how a project is designed as well as how it is executed: should academicor cultural crowdsourcing reflect the interests of the academy, or of the crowd itself? 12.Wiggins and Crowston, ‘From Conservation to Crowdsourcing’. 13.Bonney et al., Public Participation in Scientific Research. Crowdsourcing Cultural Heritage Our Scoping Study identified galleries, libraries, archives and museums (hereafter GLAM) as a separate category within the emergent field of humanities crowdsourcing. GLAM institutions are inherently public facing, and many have long traditions of volunteerism and public engagement. Most museums, especially smaller ones, must attract and engage audiences to justify their funding; and memory institutions such as national libraries and museums have formal duties to maintain access to their collections, both for scholars and the public. Against this background, approaches such as that of Copeland have emphasised the importance of ‘constructivist’ approaches, where the public is encouraged to engage with the interpretation of collections, rather than ‘positivist’ approaches where they are passive recipients of knowledge organised by curators.14 While such processes of interaction are valid in terms of engagement, they do not fit the usual definitions of crowdsourcing or citizen science, nor could they be seen as extensions to the Brahbam/Howe view of crowdsourcing as a business model. On the other hand however they involve no specific research aim, apart from greater appreciation and/or reception of the collection in question. While such engagement might not lead to the production of new academic knowledge, or new or enhanced academic resources, as such, it can lead to the development of a wider understanding of museum collections, and of the curatorial narratives of their collections. One typology for crowdsourcing with a special focus on simple crowdsourcing games in the GLAM sector has been suggested by Mia Ridge.15 In this typology, the proposed categories are: Tagging; Debunking (i.e. correcting/reviewing content); Recording a personal story; Linking; Stating preferences; Categorising; and Creative responses. Again, these categories imply a processual approach, and are, at least potentially, extensible across different types of online and physical-world content and collections. They are concerned with the type of activity that the crowd is being requested to carry out. Our Scoping Study proposed this as one of four means of classification that might underpin an epistemic model of humanities crowdsourcing (see below). A further typology for crowdsourcing in the GLAM domain was developed by Oomen and Aroyo.16 Their categories include Correction and Transcription, defined as inviting users to correct and/or transcribe digitised outputs (a category that Ridge’s ‘Debunking’ partially, but not entirely, covers); Contextualisation, or adding contextual knowledge to objects, by constructing narratives or creating user-generated content (UGC) with contextual data; Complementing Collections, which is the active pursuit of additional objects to be included in a collection; Classification, defined as the gathering of descriptive metadata related to objects in a collection (Ridge’s ‘tagging’ would be a sub-set of this); Co-curation, which is 14.Copeland, ‘Presenting Archaeology to the Public’. 15.Ridge, ‘Playing with Difficult Objects’. 16.Oomen and Aroyo, ‘Crowdsourcing in the Cultural Heritage Domain’. using the inspiration/expertise of non-professional curators to create (web) exhibits (somewhat analogous to Bonney et al.’s co-created projects category, but oriented more towards content than organisation or management); and Crowdfunding, or the collective cooperation of people who pool their money and other resources together to support efforts initiated by others.17 Enterprises such as the Flag Fen project undoubtedly give the public a channel to engage with the project, and to take a tangible stake in it. Flag Fen, which was supported by DigVentures. com, allowed selected members of the public who had contributed financially to participate for all or part of the excavation period, with training and supervision provided by the professional archaeological team. This is proactive engagement with a shared purpose that goes well beyond simply becoming a member of a museum or donating to a heritage organisation or charity. In the context of the present discussion, the category of ‘co-curation’ is especially interesting. This implies the bringing of new knowledge or perspectives, rather than simply energy or enthusiasm, to a project or collection. Epistemic Models of Humanities Crowdsourcing The case of co-curation highlights that citizen science projects which have adopted and employed methods derived from crowdsourcing business models principally draw on mechanical and self-contained tasks: transcribing small and consistent fragments of text; or making high-level classifications using the naked eye. Their workflow does not deal directly with the creation of new knowledge. The previous discussion, expanded from our Scoping Study, makes clear that there are many potential reasons for humanists, and those outside the academy involved in cultural heritage, to source ‘the crowd’ for knowledge, interpretation and understanding, as well as for accumulated micro-units of effort. This is not to say that humanities crowdsourcing cannot usefully engage such effort, or indeed that many kinds of contributors would prefer to continue contributing in this way – not everyone who helps to transcribe historical manuscripts wants to become a historian (our Scoping Study identified direct evidence of this by interviewing current contributors). However, there undoubtedly exists a numerically small sub-group of contributors with the capacity and motivation to graduate from the mechanical to more constructive contributions to knowledge, where the participant or audience engages with the process of interpretation, rather than passively consuming a given curatorial narrative. It is epistemic models of crowdsourcing, not business models, that will accommodate this in the future. The remainder of this chapter reflects how the four-facet typology described in the Scoping Study might encourage this process. 17.For example,Palmer, ‘Flag Fen Hosts “Crowdsourced” Bronze Age Archaeology Dig’. Processes as Scholarly Primitives Our Scoping Study proposed that crowdsourcing in the humanities can be understood in terms of the four facets of asset, process, task and output. It assumes that all crowdsourcing activity does something to some item, or set of items, and those items are the assets. Assets can be grouped by their affordances and the structure of their information. A process is a sequence of tasks, through which an output is produced by operating on an asset. It is conditioned by the kind of asset involved, and by the questions that are of interest to project stakeholders (both organisers and volunteers) and can be answered, or at least addressed, using information contained in the asset. In the Scoping Study, we identify assets as geospatial; text, numerical or statistical information; sound; image; video and ephemera (defined here very broadly as material arising from some aspect of daily life that is at risk of loss because of its transitory nature, such as personal photographs); and intangible cultural heritage (e.g. oral history). The last of these is a case apart: its very lack of the physicality that characterises the other six is itself its justification for inclusion. Moreover, given the importance of intangible cultural heritage, including its recognised protected status in UN conventions,18 it is included here. Tasks, which correspond to the actions required to undertake a process, can be categorised as follows: Mechanical; Configurational; Editorial; Synthetic; Investigative; and Creative.19 Mechanical tasks involve the processing of small or individual amounts of information, such as digitising birth certificates, or transcribing small units of text; most of the ‘business model’ types of commercial crowdsourcing, and of citizen science, fall into this category. Mechanically processed scholarly resources are typically subjected to subsequent interpretation, analysis, manipulation, etc., most often by people undertaking professional academic research in the area.20 Configurational crowdsourcing is slightly different, in that it involves identifying meaningful patterns within data collections, without necessarily processing the units themselves. Editorial crowdsourcing implies applying judgement in the presentation or revision of an asset (usually a text), for example on a platform such as Wikisource. Synthetic tasks involve the gathering of information from different sources and merging them in some meaningful way. Investigative tasks require participants to explore data sets looking for particular information. Finally, creative crowdsourcing is where participants create new material. All of these tasks require enthusiasm, commitment and time to perform; and the ability and inclination to devote such resources to such tasks will change from participant to participant.21 However, it is relatively easy to discern a progression from the mechanical end, where little judgement, prior expertise or skills are 18.Kurin, ‘Safeguarding Intangible Cultural Heritage’. 19.Dunn and Hedges, ‘Crowd-Sourcing Study’. 20.Terras, ‘Digital Curiosities’. 21.Dunn and Hedges, ‘Crowd-Sourcing Study’. needed, to the creative end, where a very particular set of skills, as well as time and enthusiasm, are needed to meet the objective or objectives at hand. Therefore, whereas the assets on which tasks operate can be grouped by affordance or data type, the tasks, we argue, are best grouped in terms of the demands placed upon the user, whether they require just time and enthusiasm, or more besides. One observation of our review was that some projects, such as Old Weather, are structured in such a way that a participant can easily start at the mechanical end, and, should they wish, proceed to tasks of editing, synthesis and investigation. In terms of epistemic models of humanities crowdsourcing, the concrete output is perhaps less important, since the overarching output should be taken to be some form of new or enhanced knowledge. However, the concrete result of the crowdsourcing activity should also be considered, and its nature depends on why the project has been designed, by whom, and by what kind of process (co-created, collaboratively designed centrally designed etc.). The categories here are Original text; Transcribed text; Corrected text; Enhanced text; Transcribed music; Metadata (data about the asset, which can be used either for preservation or discovery); Structured data; Knowledge/awareness; Funding; Synthesis; and Composite digital collections. This category is organised much along the same affordances/ structural lines as the asset category, and indeed in many ways mirrors it. The three aspects of asset, task and output are unified by the processes involved. If an epistemology of humanities crowdsourcing can be constructed, then it is around these. Crowdsourcing Processes This section reviews the kinds of processes identified in the Scoping Study, and considers how they can form a set of ‘scholarly primitives’ for humanities crowdsourcing. Firstly, collaborative tagging typically requires some form of cognitive judgement and/or prior knowledge. Tags can be based on existing controlled vocabularies (if the project is so designed), but are more usually derived from free text supplied by the users themselves. Such ‘folksonomies’ are distinguished from deliberately designed knowledge organisation systems by the fact that they are self-organising, evolving and growing as the crowd adds new terms. Research has also been carried out into extracting formal data structures from folksonomies.22 Collaborative tagging can give rise to the configurational task of identifying patterns in the tag collections created. Collaborative tagging can result in a corpus of information assets searchable using keywords applied by contributors, or it can highlight assets that have particular significance, as evidenced by the number of repeat tags they are accorded by the pool. Generally, collaborative tagging results in relatively stable 22.Lin and Davis, ‘Computational and Crowdsourcing Methods’. Table 10.1 Categories of humanities crowdsourcing processes Collaborative tagging Linking Correcting/modifying content Transcribing and marking up Recording and creating content Commenting, critical responses and stating preferences Categorising Cataloguing Contextualisation Mapping Georeferencing Translating tag structures, at least if significant numbers of people contribute.23 Minority opinions can thus be preserved alongside more highly replicated, and therefore mainstream, concentrations of tags. Other research has shown that user-assigned tags in museums may be quite different from vocabulary terms assigned by curators. 24 While this helps address the ‘semantic gap’between the language used by museums and their audiences, it also renders it potentially problematic as a means of establishing authoritative descriptions of museum collections. In any case, such approaches to knowledge organisation are likely to play a significant part in the organisation of cultural heritage data in the future. A good example of this is the BBC’s Your Paintings project (see Chapter 8 in this volume), developed in collaboration with the Public Catalogue Foundation, which has amassed a collection of photographs of all paintings in public ownership in the United Kingdom. The public is invited to apply tags to these, which makes them searchable by keyword. Linking can take the form of identifying and documenting links of a specified type between individual assets, or, far more commonly, of linking via semantic tags (where in this case the tags describe binary relationships). Correcting and modifying content responds to the fact that many means of creating digital information are imperfect, and require human corrective purposes for quality assurance. In this process, mechanical, configurational and editorial tasks are combined. For example, Optical Character Recognition (OCR) and speech recognition are generally error-prone, and factoring in quality control 23.Golder and Huberman, ‘Usage Patterns of Collaborative Tagging Systems’. 24.Trant et al., ‘The Eye of the Beholder’. and error correction is essential for any such enterprise. Reviewing the outputs for errors can be costly in terms of human input. The Trove project, a large-scale digitisation effort at the Australian National Archives, is an excellent example of this.25 In this case, the volume of digitised material would have been too great for the library to undertake the corrections using its own staff, and if only page images were produced then there would have been little or no capability for searching the text, significantly reducing the benefits of having the material in digital form at all.26 Transcribing is one of the most common and familiar forms of process currently in use in humanities crowdsourcing. It can be undertaken in parallel with correction and modification, or rather correcting and modifying processes can form parts of a broader transcription process. Transcription addresses directly one of the most fundamental problems with automated digitisation: that handwriting, especially complex and/or difficult to read scripts, cannot be automatically rendered into machine-readable form using current technology. It can only be transcribed manually with the human eye and, in many cases, with human interpretation. Transcription is interesting as an example of the spread of task types. It can be extremely mechanical; yet it can also require great levels of editorial judgement and discretion. It also brings into play the key issue raised at the start of this chapter: that of academic judgement and credibility. If a transcription process leads to a published edition, then the authority of that edition will depend on the process being transparent. For a transcribed edition to be usable (and citable) in the academic literature, it will need to be published under a scholarly imprimatur, with authoritative editorial judgement certified in some way. This means that the organisational and structural aspects of the project discussed by Bonney et al.27 will need to include an element of scholarly validation. Recording and creating content is a product of creative tasks, but will likely include aspects of investigation, which could be broadened to include more general forms of observation. The Cornell Ornithology Lab, for example, has employed crowdsourced observations from people’s gardens to study changes in bird population over time,28 and the Florida Fish and Wildlife Conservation Commission’s Nesting Beach Survey project applied similar methodologies to tracking the nesting habits of giant sea turtles.29 In the humanities however, processes in this category are usually concerned with recording ephemera and intangible culture, such as oral history or reminiscence. These frequently take the form of a GLAM or other cultural institution soliciting memories from the communities it serves, for example the Tenbury Wells Regal Cinema’s Memory 25.Holley, ‘Crowdsourcing’. 26.Holley, Many Hands Make Light Work. 27.Bonney et al., Public Participation in Scientific Research. 28.Brossard et al., ‘Scientific Knowledge and Attitude Change’. 29.Bradford and Israel, ‘Evaluating Volunteer Motivation for Sea Turtle Conservation in Florida’. Reel project.30 Such processes can incorporate a form of editorial control or post hoc digital curation, and their outputs can be edited into more formal publications, or analysed/explored using methods such as sentiment analysis which attaches quantitative weights to the positivity or negativity of units of text, such as tweets or commentaries, and allows collective analysis of these. Commenting, critical responses and stating preferences were included in our Scoping Study because of the emergence in the last three years of several academic projects that seek both to study social media, and to engage wider public audiences using social media. One example is the Shakespeare’s Global Communities project,31 which aimed to capture audience response to the 2012 World Shakespeare Festival. Akey question addressed by this project was ‘How do new social networking technologies reshape the ways in which diverse global communities connect with one another around a figure such as Shakespeare?’.32 The question itself provides a focus for the activity, and although in and of itself it does not produce a verifiable academic output, it provides a dataset that is highly relevant to the reception of Shakespeare’s work in modern times. In some cases, appropriately presented blogging software can provide a platform for focused scholarly interaction, where there is a shared goal involving a humanities research question or issue. For example, a review by Sonia Massai from King’s College London of King Lear on the Year of Shakespeare site attracted controversial responses, leading to an exchange about critical methods as well as content.33 This is scholarly interaction without a ‘traditional’ framework of scholarly communication. Instead it relies on the content creation process being contextualised, and thus guided, by content that is already there. The project thus provides a tangible link between the crowd and the subject. There is also much potential in mining corpora of comments and critical responses, using techniques such as sentiment analysis. Categorising involves the placingof assets into predefined categories; it differs from collaborative tagging in that the latter is unconstrained, whereas categorising depends on having predefined vocabularies. Cataloguing – or, more expansively, the creation of structured, descriptive metadata (e.g. for cultural objects) – is a more open-ended process than categorising, but it is nevertheless constrained to follow accepted metadata standards and approaches. In many curatorial settings, the two approaches are combined. Cataloguing frequently includes categorising as a sub-activity; for example, according to Library of Congress subject headings. Cataloguing is a costly process for GLAM institutions, and crowdsourcing this activity has been explored as a cost-effective means of accomplishing it. One 30.http://www.regaltenbury.org.uk/memory-reel (accessed July 31, 2013). 31.http://www.yearofshakespeare.com (accessed July 31, 2013). 32.http://crowds.cerch.kcl.ac.uk/wp-uploads/2012/09/workshop_report1.pdf (accessed July 31, 2013). 33 See http://bloggingshakespeare.com/year-of-shakespeare-king-lear-at-the-almeida (accessed July 31, 2013). example of a cataloguing process is provided by the British Library’s Georeferencer project,34 which has successfully used crowdsourcing to enrich a collection of over 700 digitised maps with accurate geographical metadata. Such a cataloguing process type is linked to contextualisation: such georeferencing allows the maps to be navigated, compared and contextualised as a whole collection, in a way that simple page scans could not be. Contextualisation activities are typically less specific in their aims than cataloguing or tagging; however adding context enriches an asset by adding to it or associating with it other relevant information or content. Because it can be shown to add value in this way, contextualisation may be considered as a fundamental process in its own right. Mapping is another process. In the sense used here, it refers to the process of creating a spatial or conceptual representation of some information asset(s). This could involve the creation of map data from scratch, as in the OpenStreetMap initiative,35 but it could also be applied to the visual or spatial mapping of concepts (as in a ‘mind map’). The precise sense will be highly dependent on the asset type to which mapping is being applied, but as a process it remains consistent. Mapping is not georeferencing, which is about applying external frameworks such as latitude and longitude to assets that have already been mapped. It refers to the process of establishing the location of un-referenced geographical information in terms of a modern real-world coordinate system (such as latitude and longitude). Georeferencing can be used to enrich significantly geographical datasets that do not include such information, but which could or should do, and there has been significant activity in this area in terms of crowdsourcing and user engagement. Translation is a process that covers the translation of content from one language to another. In many cases, a crowdsourced translation will require a strongly collaborative element if it is to be successful, given the semantic interconnections and interdependencies that can occur between different parts of a text. Also, a text translated by multiple translators is likely to contain inconsistencies, especially when the text is being translated from languages that do not have a close linguistic relationship. However, in cases where a large text can be broken up naturally into smaller pieces, a more independent mode of work may be possible. An example of this is the Suda On-Line project,36 which is (among other things) translating the entries in a tenth-century Byzantine lexicon/ encyclopaedia. A more modern, although non-academic, example is provided by the phenomenon of ‘fansubbing’, where enthusiasts provide subtitles for television shows and other audiovisual material.37 34.http://www.bl.uk/maps/georefabout.html. 35.http://www.openstreetmap.org (accessed July 31, 2013). 36.http://www.stoa.org/sol/ (accessed July 31, 2013). 37.Cintas and Sanchez, ‘Fansubs’. Conclusion As noted in the introduction, the heritage of crowdsourcing as a means of distributing profit-making activity to wide and self-selecting groups of people means that it is a business-oriented concept, underpinned by a generic business model – just as outsourcing is. The typology proposed in our Scoping Study sought to provide a consistent framework in which to conceptualise crowdsourcing in the humanities, and to start considering how crowdsourcing can be considered as an epistemic model rather than a business model. To do this, we propose a conceptualisation of a ‘methodological commons’ for humanities crowdsourcing. In developing a methodological commons for the digital humanities, Willard McCarty describes ‘computational techniques shared among the disciplines of the humanities and closely related social sciences, e.g., database design, text analysis, numerical analysis, imaging, music information retrieval, communications’, with the humanities disciplines ranged above, and areas of interdisciplinary overlap below.38 While humanities crowdsourcing does rely on technology for harnessing distributed effort, the approach taken in the Scoping Study is rather to identify a set of processes that may be considered as ‘scholarly primitives’ (as noted by John Unsworth)39 that constitute a basis of crowdsourcing as a knowledge-generating, or knowledge-changing field. Each of these processes can lead, depending on the assets, tasks and outputs in play, to the creation of new knowledge of some kind. This is the critical distinction between crowdsourcing as a cost-effective means of creating or enhancing digital content and crowdsourcing as an academic activity. There is no doubt that many of these processes can lead to the production or modification of digital content, but knowledge is also created as a result of that process. We did little work on mapping the processes we identified to academic humanities disciplines, and this is undoubtedly the next stage in establishing a ‘methodological commons’ for humanities crowdsourcing. However, we can observe, unsurprisingly perhaps, that many of the activities surveyed in our Scoping Study 2012 were oriented towards the historical and literary areas. The experience of the Year of Shakespeare project, and our identification of ‘Commenting, critical responses and stating preferences’ as a fundamental process, suggests that the creative and performing arts have interesting potential as an application area. The British Library Georeferencer project, which falls within the purview of history and yet has a clear and specific relevance to another discipline (geography), indicates that processes can work in cross-disciplinary settings. However, our review of contributor motivations suggested that subject interest was one of the major factors that drove people to contribute.40 While it may sound plausible that 38.McCarty, ‘Humanities Computing’, 1224. 39 http://people.lis.illinois.edu/~unsworth/Kings.5–00/primitives.html (accessed July 31, 2013). 40.Dunn and Hedges, ‘Crowd-Sourcing Study’. the subjects that receive the greatest level of user contributions are those with the broadest public appeal, this is probably not the case. Given the fact that a small percentage of contributors do a large percentage of the work, successful uptake of contributor effort in humanities crowdsourcing will be dependent on finding pockets of enthusiasm and expertise for specific areas. The broad appeal that drives citizen science projects may apply to some projects in the humanities, but it does not necessarily have to. References Bonney, R., H. Ballard, R. Jordan, E. McCallie, T. Phillips, J. Shirk and C.C. Wilderman. Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education. Washington, DC: Center for the Advancement of Informal Science Education, 2009. http://caise. insci.org/uploads/docs/PPSR%20report%20FINAL.pdf (last accessed July 31, 2013). Brabham, D. ‘Crowdsourcing as a Model for Problem Solving: An Introduction and Cases’. Convergence: The International Journal of Research into New Media Technologies 14, no. 1 (2008): 75–90. Bradford, B.M. and G.D. Israel. ‘Evaluating Volunteer Motivation for Sea Turtle Conservation in Florida’. Agricultural Education 372 (2004): 1–9. Brohan, P., P.R. Allan, J.E. Freeman, A.M. Waple, D. Wheeler, C. Wilkinson and S. Woodruff. ‘Marine Observations of Old Weather’. Bulletin of the American Meteorological Society 90, no. 2 (2009): 219–30. Brossard, D., B. Lewenstein and R. Bonney. ‘Scientific Knowledge and Attitude Change: The Impact of a Citizen Science Project’. International Journal of Science Education 27, no. 9 (2005): 1029–121. Causer, T., J. Tonra and V. Wallace. ‘Transcription Maximized; Expense Minimized? Crowdsourcing and Editing The Collected Works of Jeremy Bentham’. Literary and Linguistic Computing 27, no. 2 (2012): 1–19. Cintas, J.D. and P.M. Sanchez. ‘Fansubs: Audiovisual Translation in an Amateur Environment’. Journal of Specialised Translation 6 (2006): 37–52. Copeland, T. ‘Presenting Archaeology to the Public: Constructing Insights On-Site’. In Public Archaeology, edited by N. Merriman, 132–44. London: Routledge, 2004. Dunn, S. and M. Hedges. ‘Crowd-Sourcing Study: Engaging the Crowd with Humanities Research’. AHRC Connected Communities Programme, 2012. http://crowds.cerch.kcl.ac.uk. Estellés-Arolas, E. and F. González-Ladrón-de-Guevara. ‘Towards an Integrated Crowdsourcing Definition’. Journal of Information Science 38, no. 2 (2012): 189–200. Golder, S.A. and B.A. Huberman. ‘Usage Patterns of Collaborative Tagging Systems’. Journal of Information Science 32, no. 2 (2006): 198–208. Holley, R. Many Hands Make Light Work: Public Collaborative OCR Text Correction in Australian Historic Newspapers. Canbera: National Library of Australia, 2009. http://www.nla.gov.au/ndp/project_details/documents/ ANDP_ManyHands.pdf (accessed July 31, 2013). Holley, R. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3/4 (2010). http://www.dlib.org/dlib/march10/ holley/03holley.html (accessed July 31, 2013). Howe, J. ‘The Rise of Crowdsourcing’. Wired 14.06 (June 2006). http://www. wired.com/wired/archive/14.06/crowds.html (accessed July 31, 2013). Kurin, R. ‘Safeguarding Intangible Cultural Heritage in the 2003 UNESCO Convention: A Critical Appraisal’. Museum International 56, no. 1–2 (2004): 66–77. Lin, H. and J. Davis. ‘Computational and Crowdsourcing Methods for Extracting Ontological Structure from Folksonomy’. Lecture Notes in Computer Science 6089 (2010): 472–7. McCarty, W. ‘Humanities Computing’. In Encyclopedia of Library and Information Science, edited by M.A. Drake, 1224–35. New York: Marcel Dekker, 2003. Oomen, J. and L. Aroyo. ‘Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges’. Proceedings of the 5th International Conference on Communities and Technologies, 2011, 138–49. http://www. cs.vu.nl/~marieke/OomenAroyoCT2011.pdf (accessed July 31, 2013). Palmer, J. ‘Flag Fen Hosts “Crowdsourced” Bronze Age Archaeology Dig’. BBC, August 13, 2012. http://www.bbc.co.uk/news/science-environment-19192220 (accessed July 31, 2013). Raddick, J.M., G. Bracey, P.L. Gay, C.J. Lintott, P. Murray, K. Schawinski, A.S. Szalay and J. Vandenberg. ‘Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9 (2010). http://portico.org/ stable?au=pgg3ztfdp8z (last accessed July 31, 2013). Ridge, M. ‘Playing with Difficult Objects: Game Designs to Improve Museum Collections’. In Museums and the Web 2011: Proceedings, edited by J. Trant and D. Bearman. Toronto: Archives & Museum Informatics, 2011. http:// conference.archimuse.com/mw2011/papers/playing_with_difficult_objects_ game_designs_improve_museum_collections (accessed April 15, 2014). Surowiecki, J. The Wisdom of Crowds: Why the Many are Smarter than the Few. New York: Doubleday, 2004. Terras, M. ‘Digital Curiosities: Resource Creation via Amateur Digitisation’. Literary and Linguistic Computing 25, no. 4 (2010): 425–38. Trant, J., D. Bearman and S. Chun. ‘The Eye of the Beholder: Steve.Museum and Social Tagging of Museum Collections’. In Proceedings of the International Cultural Heritage Informatics Meeting (ICHIM07), edited by J. Trant and D. Bearman. Toronto: Archives & Museum Informatics, 2007. http://www. archimuse.com/ichim07/papers/trant/trant.html. Wiggins, A. and K. Crowston. ‘From Conservation to Crowdsourcing: A Typology of Citizen Science’. System Sciences (HICSS), 2011 44th Hawaii International Conference, 2011. http://ieeexplore.ieee.org/xpl/articleDetails. jsp?arnumber=5718708 (last accessed July 31, 2013). Chapter 11 The Role of Open Authority in a Collaborative Web Lori Byrd Phillips Introduction We have all this pent-up knowledge in museums, all this pent-up expertise, and all these collections designed to inspire and bring people together. I think the museum community has an ethical responsibility to unleash it. (Jane McGonigal)1 This statement neatly encapsulates the challenge presently facing museums: to balance institutional expertise with the potential of collaborative online communities. But this goes beyond just unleashing content, as museums must consider being both open as in free access and open to co-creating knowledge. Museums today must reconcile the need to maintain appropriate models of authority with the question of how to best incorporate visitor contributions. This chapter frames authority and openness through parallel metaphors within the museum and technology fields, and offers the Reggio Emilia educational approach as inspiration for collaborative knowledge-sharing between museums and communities. Expanding on the metaphors of the museum as ‘the Temple and the Forum’and the web as ‘the Cathedral and the Bazaar’, this chapter argues that a new model of ‘open authority’ is required to combine effectively community contributions and museum expertise in interpreting our shared heritage. Society is increasingly empowered by a social web that enables collaboration and connectivity – often in the form of crowdsourcing. In this new landscape, organisations are expected to be transparent and open – both in sharing research content and in community dialogue. Expectations of openness and transparency may seem to challenge traditional understandings of museum authority2 but in fact, this flood of user-generated content has produced a renewed need for authoritative expertise in museums. Within our fast-paced, digital world, institutional authority should be leveraged to facilitate and validate user-generated content on digital platforms. Ultimately, I expect that the successful museums of the future will be 1.McGonigal, ‘Gaming the Future of Museums’. 2.Simon, The Participatory Museum, 274; Merritt, ‘The Next Frontier of Museum Ethics’; Oomen and Aroyo, ‘Crowdsourcing in the Cultural Heritage Domain’. those that work alongside and engage with online and on-site communities in spite of the perceived challenges that this interaction poses. Museums can embrace the open web – a set of philosophies that include transparency, decentralisation, open-source code, open technical standards and two-way communication3 – as a model to reconcile traditional notions of authority with the expectations of the digital era, using institutional expertise to facilitate and validate this new, user-generated content. This model, which I call ‘open authority’, is the coming together of institutional expertise with the experiences and insights of our communities, both online and on-site.4 The concept of open authority is an effort to demystify and reconcile the seemingly contradictory ideas of open collaboration and museum expertise, making it clear that openness and authority are not mutually exclusive. It is not about giving up anything – it is about fostering an open dialogue so that institutional expertise can be made even better, together. Open authority is framed by two parallel metaphors. Duncan F. Cameron’s groundbreaking article, ‘The Museum, a Temple or the Forum’, addresses the museum’s dual mission of ‘excellence and equity’.5 Rather than giving up authority, Cameron argues that museums become both respected temples and forums for dialogue. In the open-source software movement, Eric S. Raymond’s ‘The Cathedral and the Bazaar’ is a landmark essay that has influenced the development and conceptualisation of open web communities, Wikipedia being a primary example. Raymond compares the open-source community to a collaborative bazaar, in contrast to an inaccessible cathedral. Comparisons can be drawn between the metaphors of the ‘Temple and Forum’and ‘Cathedral and Bazaar’ which strengthen the open authority model and further illustrate the implications for cultural heritage in the digital sphere. I believe that the future of museum authority lies in bringing the metaphors of ‘Temple and Forum’and ‘Cathedral and Bazaar’together, combining the need to engage new perspectives with the opportunities provided by the connected, open web. While some cultural professionals see the participatory web as a challenge to authority, museums should instead embrace it. When cultural professionals adopt models of open authority, we are able to use the digital platforms at our disposal to work within communities and collaboratively improve the interpretation of our cultural heritage. Temple and Forum Meets Cathedral and Bazaar Though written nearly 30 years apart and in two entirely different fields, Eric S. Raymond’s 1997 essay ‘The Cathedral and the Bazaar’ engages with similar fundamental issues to Duncan F. Cameron’s 1971 article, ‘The Museum: ATemple 3.Neuberg, ‘What Is the Open Web and Why Is It Important?’. 4.Phillips, ‘Defining Open Authority in Museums’; Phillips, ‘Open Authority & the Future of Museum Ethics’. 5.Hirzy, Excellence and Equity. or the Forum’, both in its imagery and in its underlying themes. Both argue that traditional assertions of power and authority stifle the potential of their respective communities. Cameron explains that while museums should maintain their role as temples, ‘there must be concurrent creation of forums for confrontation, experimentation, and debate’.6 Although Cameron’s article is over 40 years old, the museum field is far from upholding his ideal that museums need not be a temple or a forum, but should be both. Cameron’s ‘temple’ is based on the premise that information presented in museums embodies a ‘standard of excellence … and a statementof truth’.7 In this way, the temple metaphor represents the idea of a museum’s perceived authority. While few question Cameron’s assertion of the museum as an authoritative temple, the museum field has been slow to embrace wholly the concept of the forum. Numerous scholars have expanded upon the idea of a space for dialogue existing within a museum, but the related ideas of authority, voice and the democratisation of culture are still points of conflict in museological debate today.8 The dominant message remains intact: museums should play a more central role in society while also remaining steadfast in their excellence as stewards of culture. Just as the museum field has contended with both the promise and conflict of the forum, so too have information technology theorists. Running parallel to the museological theory of ‘the temple or the forum’, the open-source software community has advocated for the shift from a closed ‘cathedral’ to an open ‘bazaar’of software development. In ‘The Cathedral and the Bazaar’, Raymond, a software programmer and open-source advocate, compares the top–down models of proprietary software development, exemplified by Microsoft, with the coalescence of the open-source movement through the development of the Linux operating system. The principles discussed in ‘The Cathedral and the Bazaar’have greatly influenced the development of online communities. Raymond implies that the ‘Cathedral’, the proprietary model of software development ‘by wizards and mages in splendid isolation’, will not be able to compete with the ‘Bazaar’ of volunteers who ‘put orders of magnitude more skilled time into a problem’.9 He paints a picture of the development of Linux software as ‘a great babbling bazaar of differing agendas and approaches’.10 The Linux community’s experiment with open methods of collaboration, as described 6.Cameron, ‘The Museum, a Temple or the Forum’, 68. 7.Ibid., 66. 8.Simon, The Participatory Museum, ii; Hirzey, Excellence and Equity, 19; Merritt, ‘The Next Frontier of Museum Ethics’; Hooper-Greenhill, The Educational Role of the Museum, 23; Weil, ‘From Being about Something to Being for Somebody’; Falk, Thriving in the Knowledge Age; Gurian, ‘A Savings Bank for the Soul’; Stein, ‘Chiming in on Museums and Participatory Culture’. 9.Raymond, ‘The Cathedral and the Bazaar’, 54. 10.Ibid., 21. by Raymond, is now the basis for the open-source movement, a community that encourages the production of software that is free to reuse and modify. An important implication of openness is that, ‘given enough eyeballs, all bugs are shallow’.11 In other words, the more people involved in developing software, the more likely it is that mistakes will be found and fixed quickly. Raymond concludes that, ‘The future of open-source software will increasingly belong to people … who leave behind the cathedral and embrace the bazaar.’12 This ‘babbling bazaar’ of participation could serve as a new approach to the museum’s forum, one which is more in line with the needs and expectations of the museum’s current socially and digitally connected community. The open-source community has already established a solid foundation upon which to build collaborative spaces. Evolving out of the concept of Raymond’s ‘bazaar’is the idea of ‘barn raising’within online communities. The online usage of barn raising is derived directly from the literal action of a community coming together to build a barn, something that cannot be done alone but can be completed efficiently as a collective action. An important component of barn raising is the celebratory social aspect – a community completes an action and then shares in that accomplishment.13 Online barn raising is a type of crowdsourcing, but perhaps maintains an element of community that some crowdsourced projects lack. Wikipedia is one such collaborative space that frequently embraces the premise of barn raising. Wikipedia is the most influential application of the open-source software movement within the cultural sphere, and is an important and widely read source of information. The editable encyclopaedia is often cited as the prototypical open, collaborative community. With the goal of amassing ‘the sum of all human knowledge’ freely, openly and in every language, Wikipedia has attracted a deeply committed community that spans the globe.14 Open Authority: Reconciling the Temple and the Bazaar The foundational elements of the open-source movement inspired me to consider how these lessons apply to issues of authority in museums. By combining the metaphors of ‘temple and forum’and ‘cathedral and bazaar’, we can take advantage of the strengths of the collaborative web and move beyond the museum’s traditional notion of forum. Instead, the museum should be both the ‘temple’, or authority, and the ‘bazaar’, an open, collaborative community. This is open authority. 11.Ibid., 19. 12.Ibid., 23. 13.See ‘BarnRaising’, Meatball Wiki, 2009. http://meatballwiki.org/wiki/BarnRaising. 14.Benkler, Wealth of Networks; Rosenzweig, ‘Can History be Open-Source?’; Tapscott and Williams, Wikinomics; Wyatt, The Academic Lineage of Wikipedia; Lih, The Wikipedia Revolution. At its most basic, open authority combines a museum’s established expertise with the contributions of broad audiences through collaborative platforms. Mirroring Raymond’s idea that code will improve with more participation, cultural interpretation can only become more multifaceted with increased diversity and inclusiveness. I believe that the future of ‘authority’does not need to be an ‘either/ or’scenario, but should be a ‘both/and’. Just as Cameron declared that museums should be temples and forums, museums should now maintain their authority and become more open. Open authority is a specific term for a nebulous concept that the museum field has been grappling with for some time. In recent years museum theorists have been more deliberate in their discussion of technology’s impact on perceptions of institutional authority.15 Museums are eager to utilise digital technologies to remain relevant as the internet becomes the centre of our cultural crossroads, with curatorial authority seeming to hang in the balance. Rob Stein, Deputy Director of the Dallas Museum of Art, clearly articulated the current state of authority in museums when he made the distinction between ‘authoritarian’and ‘authoritative’ approaches. He pointed out that museums should remain authoritative in their expertise, but avoid being authoritarian, moving away from traditional perceptions of museums as omniscient purveyors of truth, thus placing the expertise of curators and other domain experts at the centre of an open discussion with the public.16 Open Authority and the Democratisation of Knowledge The merging of museum expertise and community is a form of democratisation that will be foundational to the future success and relevance of museums. I believe that the museum’s role as a forum or bazaar has great potential as a system of checks and balances in aid of the democratisation of the museum. The concept of openness in museums has been addressed from a variety of perspectives, from a focus on ‘equality of cultural opportunity’17 to the museum as an ‘instrument for social change’.18 To remain relevant amid trends in digital collaboration, a democratised museum must allow for a diversity of voices to participate in order to reassess cultural narratives critically. These writers all point to the fact that cultural narrative is inherently biased and, in an increasingly transparent world, an omniscient voice in exhibit interpretation is no longer feasible unless attributed to a specific author.19 Instead, museums should embrace more inclusive processes; the collaborative web is a powerful means to pursue this effort. 15.Chung, ‘Museums and Society 2034’; Adair et al., Letting Go?; Poole, ‘The Rise and Fall of the Curator’. 16.Stein, ‘Chiming in on Museums and Participatory Culture’, 225. 17.Cameron, ‘The Museum, a Temple or the Forum’, 67. 18.Weil, ‘From Being about Something to Being for Somebody’, 233. 19.Cameron and Robinson, ‘Digital Knowledgescapes’. Educational models have long inspired museum theorists as they address how to connect better with communities. In the 1992 report Excellence and Equity, the American Association of Museums placed Cameron’s model of ‘temple and forum’ squarely within an educational context.20 Museum theorists urge museums to abandon top–down approaches to interpretation and instead encourage equity through the inclusion of diverse perspectives via debate and dialogue.21 Many museums, however, have been slow to embrace actively this mandate. Educational philosopher Paolo Freire framed the issue of authoritative voice through the lens of the imbalance of power between teacher and student, drawing connections between didactic teaching methods and oppression. His solution to this imbalance is to re-humanise the oppressed through personal empowerment via community dialogue and the enhancement of critical thinking skills. While his revolutionary work focuses on the educational system, many correlations can be drawn to the museum field and issues relating to representations of cultures in exhibit narratives. At its core, Freire’s perspective reminds us to respect the learner and to not speak for them, but to let them learn for themselves while providing guidance along the way. Museums should more fully implement this deeper form of constructivist learning – wherein knowledge is constructed through the learner’s unique ideas and prior knowledge – and in so doing become true forums for community dialogue. The purpose of the forum is to allow others to have a voice and provide a means for reflection and critical dialogue, which Freire considers to be necessary for empowered learning.22 In ‘The Temple or the Forum’, Cameron defines the ‘democratization of culture’ as the ‘equality of cultural opportunity’.23 More recently, this has been described as a potential ‘culture gap’that corresponds with the increasing ‘technology gap’ in society. Stein has called upon museums to fill this gap in order to embrace participatory culture, stating: Much of the beauty and power of museums lies in their ability to level the playing fields of society and to offer a place for discourse and exchange with diverse audiences. Sadly, much of this potential beauty is latent and waiting to be activated by museums with a vision to change the status quo. 24 In order to accomplish this equality of cultural opportunity, non-hierarchical models in which the teacher–student or curator–visitor are mutual equals in the learning environment should be integrated into the core of interpretation of cultural heritage. A museum’s community should be part of the process, not just a passive audience. 20.Hirzy, Excellence and Equity. 21.Anderson, ‘Prescriptions for Art Museums in the Decade Ahead’; Falk, Thriving in the Knowledge Age, 24; Adair, Letting Go?, 11. 22.Freire, Pedagogy of the Oppressed, 69. 23.Cameron, ‘The Museum, a Temple or the Forum’, 67. 24.Stein, ‘Chiming in on Museums and Participatory Culture’, 225. From Closed to Open Participation The Open Web As the web has become increasingly interconnected, much has been written about the implications this has had for society as a whole. Many call this focus on user-generated content the ‘new web’, and even describe it as ‘revolutionary’, because of the immensely social, interconnected and collaborative environment that increases the efficiency of knowledge-sharing in unprecedented ways.25 The web’s use of interlinking information challenges traditional hierarchies of information, resulting in ‘non-linear, non-sequential, horizontal ways of thinking and connecting knowledge’.26 The internet is in fact a ‘distributed network’, made relevant by the individual’s ability to contextualise information through personalised exploration of links.27 Some theorists say the future is flat, connected, horizontal, decentralised; meanwhile, hierarchical systems, or those that encourage disconnected collections of information, will become obsolete.28 In his book Wealth of Networks, open-source theorist Yochai Benkler details the societal shift brought on by a new information environment in which the public is able to share proactively, reuse and remix content. Benkler states that our current information environment has ‘practical promise as a platform for democratic participation and as a means to be more critical and self-reflexive’as a society.29 Benkler points out that this new form of cooperative knowledge-sharing threatens those who formerly controlled information in the industrial economy. This phrasing is similar to that of museum theorist John Falk, who encourages museum leadersto move from Industrial Age models and ‘the paternalistic attitude that the museum can and should be the arbitrator of quality and knowledge’ and instead become places ‘devoted to people rather than things’.30 Peer production, the large-scale collaboration of individuals and firms to produce information, knowledge and culture openly, is at the heart of the web’s potential to aggregate information on a global scale. The potential of peer production was first illustrated by open-source software development and has now expanded into the information and cultural spheres in the form of crowdsourcing. The ability to take collective action on such a grand scale has led to the public’s ability to complete tasks of greater duration, scope and complexity than any one individual could complete alone. 25.Benkler, Wealth of Networks, 3; Tapscott and Williams, Wikinomics, 19; Weinberger, Everything Is Miscellaneous, 146; Shirky, Here Comes Everybody, 21. 26.Weil, ‘From Being about Something to Being for Somebody’, 243. 27.Proctor, ‘The Museum as Distributed Network’. 28.Friedman, The World Is Flat. 29.Benkler, Wealth of Networks, 15. 30.Falk, Thriving in the Knowledge Age, 24. The Open Museum Like digital theorists, museum professionals have also contemplated the implications of open, mass collaboration. Museum professionals are beginning to look to the ‘distributed network’ as a model for digital museum experiences, in contrast to the more frequently adopted ‘multi-platform’ approach to sharing museum content. In the more prevalent multi-platform model, institutions tend to push existing content out to various online communities. In contrast, a distributed network connects communities of interest with relevant content in ways that are engaging, conversational and open-ended. The distributed network is more relevant for the non-hierarchical expectations of online communities today, and provides a means for the museum experience to be ‘enhanced, not diluted, by multiple voices and authors’.31 A number of museum theorists and researchers have specifically looked to collections as an area ripe for community input and the collective improvement of information, and encouraged museums to promote dialogue around the interpretation of objects.32 The international Reciprocal Research Network (RRN) is a promising example of online collaboration between community members and museum professionals. Focusing on the indigenous heritage of the north-west coast of the United States and British Columbia, Canada, the RRN combines collections, objects and metadata from 19 institutions in order to allow ‘geographically dispersed users and institutions to share their own perspectives and knowledge’.33 The online platform brings together academics and museum experts with indigenous communities and organisations to provide additional context to collections objects and carry out research that spans institutional and geographic boundaries. The Reciprocal Research Network is a digital outgrowth of an authoritative shift that has already been occurring in the interpretation of indigenous cultures physically within museums. Over the last decade, particularly following the Native American Graves Protection and Repatriation Act of 1990 (NAGPRA), collaboration between curators and indigenous communities has become more prevalent in exhibit interpretation. By bringing together the curatorial expertise of the museum professional with the cultural expertise of those who live in those cultural contexts, a new ‘representational strategy’was formed that is a precursor to open authority. This shift in curatorial practice was not without controversy then, as now, with curatorial authority clearly under challenge. Nonetheless, 31.Proctor, ‘Museum as Distributed Network’. 32.Hooper-Greenhill, The Educational Role of the Museum, 23; Cameron and Robinson, ‘Digital Knowledgescapes’, 178; Gurian, ‘What Is the Object of this Exercise?’; Dodd and Sandell, ‘Collections Management and Inclusion’, 83; Srinivasan et al., ‘Blobgects’, 678; Eschenfelder and Caswell, ‘Digital Cultural Collections in an Age of Reuse and Remixes’. 33.Wallace, ‘Reciprocal Research Network’. initiatives such as the Reciprocal Research Network illustrate that new models for effective collaboration between museums and diverse communities continue to be developed.34 Participatory digital projects like the Reciprocal Research Network will continue to be replicated in coming years due to the expectations produced by the prevalence of the social and mobile web. It is increasingly expected that collections are made openly and freely accessible in order to garner community cooperation. Like the RRN, this can be done on a digital platform that contextualises expert contributions with community input. The practice of engaging community perspectives around collections objects is a first step towards the museum serving as a digital forum, or bazaar, of open authority. The fact that museums are drawing inspiration from the technology sphere and are beginning to see visitors as ‘users’ and ‘participants’rather than ‘the audience’is illustrative of the trend for museums to become more comfortable in opening up authority. 35 The New Web and the New Museum Just as museums grapple with issues of inherent bias in exhibit interpretation, the participatory web is one cause of society more generally accepting the notion that individual perspectives are fallible. The reliability of public-produced content on the web is often called into question without considering that this public-produced content is more accurate when there are higher levels of participation. As philosopher Pierre Lévy states, ‘No one knows everything, everyone knows something.’36 In other words, when context is negotiated through various perspectives, an individual’s authority is less significant. This new mode of knowledge-creation brought on by the open web has necessitated a shift in the way information is shaped and distributed. Authority is now built through the context in which information is provided, such as web links or other forms of citation. Information technology theorist David Weinberger states, ‘Links are a visible manifestation of the author giving up any claim to completeness or even sufficiency; links invite the reader to browse the network in which the work is enmeshed, an acknowledgement that thinking is something that we do together.’37 In this more contextualised version of authority, success has tended to follow those platforms that encourage connected, open and transparent communities. Those who build closed-off websites, who innovate internally and guard knowledge, risk becoming irrelevant while companies with 34.Ames, ‘Are Changing Representations of First Peoples in Canadian Museums and Galleries Challenging the Curatorial Prerogative?’, 73. 35.Richardson, ‘The Audience Is Dead’; Center for the Future of Museums, TrendsWatch 2012. 36.Lévy, Collective Intelligence, xi. 37.Weinberger, Everything Is Miscellaneous, 113. the foresight to utilise open, public and integrated platforms are seen to thrive.38 Increasingly, openness is expected and restricting access is seen as a weakness. A new mantra has emerged: ‘It’s not what you know … it’s how much knowledge you give away.’39 The interconnectedness of the new web has caused transparency to be seen as the rule, not the exception. But as some museums begin to embrace openness, fears have emerged over the erosion of traditional expertise and authority. Museums have previously seen themselves as purveyors of ‘timeless truths’, but that sense of authority has been challenged in recent years. Museums now must reconcile the ‘timeless’with being ‘timely’and relevant in the eyes of their visitors.40 Many museum theorists, technologists and futurists agree: the museum’s authority is no less important, it just needs to take a different form. This reconceptualisation can be described as moving from a focus on ‘absolute authority’ to a new form of ‘contextual authority’. In past decades, the museum was considered authoritative, encyclopaedic; it asserted hierarchical, curatorial control.41 Only recently have forward-thinking museum professionals described the current reality: museums maintain authority within specific contexts while users will increasingly expect to be in control of their own, personalised experiences. Ownership of the object and the context is not one and the same, and the context surrounding the object is no longer considered to be proprietary to the museum. In spite of this perception, many museums still wish to control the information that contextualises collections. In fact, relevance is no longer directly correlated with the possession of an object. Museums should instead consider more creative stewardship of the context, and look to new ways to be open and transparent in the presentation of content that is authored by themselves and others. Opening Up to Crowd and Community Some museums have experimented with openly sharing resources and have established models with elements of open authority. Examples include freely releasing multimedia to open online content aggregators, increasing access and transparency through behind-the-scenes museum information, encouraging visitor participation in museum exhibits and enabling the social tagging of collections objects through mobile applications. By embracing these technologies, museums are relaxing absolute control, which is seen by some museum professionals as threatening the foundation of authority on which they stand. Museums are now left to address these fears, and to figure out where institutional expertise can best be utilised in the future. While cultural professionals may argue that the role of 38.Tapscott and Williams, Wikinomics, 39. 39.Weinberger, Everything Is Miscellaneous, 230. 40.Gurian, ‘Timeliness: A Discussion for Museums’, 59. 41.Chan, ‘Culture + Heritage + Digital at Web Directions South 2011’. the curator is that of connoisseur, not moderator, the proactive approach must combine the two. Crowdsourced content and collaborative communities are not flawless. Some critics point to the ‘stupidity of the crowd’, and consider the web an echo chamber of self-perpetuating views.42 There is also concern about the reliability of massive amounts of user-generated content resulting from ubiquitous social technology. If generalised notions regarding the anonymity of ‘crowdsourcing’incite reluctance among cultural organisations, a more recent term, ‘community-sourcing’, may be effective in describing a more relevant approach for museums. Community-sourcing is different from crowdsourcing in that the organisation has an existing relationship with that community segment, and consequently can make bigger demands in calls to action and expect higher levels of engagement.43 While the ‘crowd’ in crowdsourcing can be distant and sometimes unassociated with the institution, the museum’s community is loyal and already engaged, be it online or on-site. In crowdsourcing, the participants would need to be convinced of the value of the project, whereas in community-sourced initiatives the existing community is already convinced of that value. While the concept of community-sourcing has not yet been formally applied to museum projects, the principles behind it are a natural fit for museums looking to share authority with those visitors who care most about the institution. Whether ‘crowdsourced’or ‘community-sourced’, museum professionals may feel that they are the last defence for credible information on the web. It is, in fact, this overabundance of information online that makes the expertise found in cultural institutions all the more valuable. The nature of the web requires museums to serve as champions of ‘high standards of quality, originality, and authenticity’.44 The New Media Consortium’s Horizon Report notes this need for an authoritative voice amidst increasing digital content, stating: ‘as authoritative sources lose their importance, there is need for more curation and other forms of validation to generate meaning in information and media’.45 The New Curator Stemming from this need for additional curation, the traditional role of the curator as a content provider should be augmented with that of a platform provider, gathering and dispersing information in addition to creating it.46 Just as the public does not want museum collections to be hidden in vaults, I would argue that curators should not hide behind institutional walls, but should be visible and helpful participants 42.Carr, ‘Is Google Making Us Stupid?’. 43.Sample Ward, ‘Crowdsourcing vs Community-Sourcing’. 44.Tapscott and Williams, Wikinomics, 271. 45.New Media Consortium, ‘A Communiqué from the Horizon Project Retreat’. 46.Simon, The Participatory Museum, 121. within the community dialogue. As one example, Smithsonian Institution’s Web and New Media Strategy calls for the creation of a commons of information that will allow the institution to be a ‘helpful agent and partner to makers, doers, and learners’.47 Like the Smithsonian, more museums should embrace the trust that the public has in their expertise to navigate, filter and validate content. This is a core area where institutional expertise will become increasingly important. In the digital age, the concept of the physical forum has shifted to that of a virtual platform for sharing knowledge and fostering conversations. In fact, forms of open authority such as crowdsourced projects, oral history initiatives, or community curation are each made possible through the facilitation of a platform by museum experts.48 Our world now works across multiple platforms and across space and time; museum curators should be connecting with users through such platforms. What makes online collaboration so valuable is the ability to accumulate ideas and information through the expertise of distributed participants. By combining the shared knowledge of cultural experts and online communities, content can be continually improved. Rather than giving away control, the platform can serve as a framework for engaging with hobbyists and amateur experts and facilitating conversations among and between communities. Establishing and mediating these boundaries requires more work on the part of museums, from sifting through content, responding to volunteers and offering timely insights on topics, but these are all valuable and productive endeavours. Curators can truly ‘curate’this new, user-generated content, filtering it to make sense of the information while connecting more frequently and deeply with the community providing it. In fact, participants in online projects are more likely to be interested in sharing content when they are in dialogue with others, particularly experts.49 Museum professionals should be using their expertise and ability to connect concepts, facts and narratives as they promote dialogue, both online and within exhibits. The Reggio Emilia Approach as Open Authority It is not curators alone who should be facilitating dialogue and connecting content; museum educators can also implement open authority in museums, particularly within the physical space. While museum educational theory has already been heavily influenced by notions of democratisation, the Reggio Emilia approach is the educational model which most directly links to the principles imbued in open authority. Reggio Emilia is an approach to early childhood education that is dependent on learning being led by the interest of the child, rather than the design of the teacher, in order to produce relevant, authentic learning experiences 47.Smithsonian Institution, ‘Smithsonian Institution Web and New Media Strategy’; Edson, ‘The Commons’, 19. 48.Adair et al., Letting Go?, 13. 49.Srinivasan et al., ‘Blobgects’, 677. in the form of ongoing, collaborative projects. It is a progressive constructivist philosophy borne out of the city of Reggio Emilia, Italy in 1945 that has since spread throughout Europe and the United States. In Reggio Emilia the learning context is shaped by the choices of the children, from infants to five-year-olds, and their consequent exploration, interest and curiosity. The role of the educator is to support this exploration through respectful interactions, making connections to prior experience while creating opportunities for community learning and collaboration.50 Characteristics core to the Reggio Emilia philosophy align naturally with the museum’s efforts to increase democratisation and openness within its communities. These characteristics include a focus on the interests of the child, the notion of respect and cyclical learning between teacher and student and the importance of the community in coming together around the needs of the children. Community and Community-Sourcing What makes Reggio Emilia unique among educational models is its focus on community as a support system to maintain respect and ongoing dialogue. This characteristic of the Reggio Emilia approach originated in post-Second World War Italy, when in the face of adversity, parents and teachers worked together to build a new system of education for their pre-school children. The Reggio Emilia model evolved to be based on adult–child interactions that created a respectful community of learners where children navigate at their own pace through observation, exploration and community discourse.51 This foundation places the Reggio Emilia philosophy squarely within a social constructivist approach to learning, one in which reciprocal communication leads participants to become co-constructors of knowledge. The respectful dialogue of the Reggio Emilia teacher– student relationship allows the children to be empowered, a factor that is integral to building a strong learning community. The physical representation of the Reggio Emilia community is the atelier, or studio, which exists in every school. The atelier was developed as a way to extend learning freely outside of the classroom by providing an open-ended art space – along with an art educator, or atelierista – with materials available for creative expression.52 The atelier exemplifies the possibility of free exchange between experiences, people, materials and disciplines. This is not dissimilar from the museum envisioned as forum or bazaar, where community members share, create and dialogue equally and freely with the museum. In encouraging their communities to come together to solve problems and respectfully support 50.Fawcett and Hay, ‘5x5x5=Creativity in the Early Years’; Firlik, ‘Promoting Development through Constructing Appropriate Environments’. 51.Paris and Hapgood, ‘Children Learning with Objects in Informal Learning Environments’. 52.Gandini, ‘The Essential Voices of the Teachers’. one another in the learning process, as in an atelier, museums are ultimately establishing a new form of community-sourcing. Child interest-led and visitor interest-led Just as in the Reggio Emilia philosophy the classroom is shaped around student interests, the open authority museum experience is shaped by visitor interests. The concept of learner-led learning can be incorporated into museum event and exhibition programming by requiring museum staff to focus on the interest of not just the child, but adult visitors as well. Learning can be more effective if programme leaders have the ability to be flexible and spontaneous in their interpretation, shifting to take advantage of the teachable moment – that spark of interest expressed by the learner that is expanded on to create a more personalised, memorable experience than a predefined programme could provide.53 While not all museum galleries are fully staffed, some exhibits effectively adapt to visitor interests by providing opportunities to explore topics more deeply through various materials and activities. It is that one-on-one interaction, however, that allows for the most personalised learning to occur. Adapting to the teachable moment is only the start. Applied to adults, Reggio Emilia could be a model for formally incorporating visitor voices into an exhibit. Truly adopting visitor interest-led interpretation requires community involvement in the early stages of exhibit development, providing a means for their insights to be incorporated into the core of exhibit design. Currently, this occurs in an ad hoc manner, only when deemed necessary by the topic – or, often, the political ramifications, as with the implementation of NAGPRA, when members of Native American communities became involved out of necessity. When open authority is fully applied to the museum, the community is involved before, during and after the interpretation of a topic, rather than as an afterthought. This requires that not only museum educators, but curators also take part in involving the community in the exhibit process, by moving beyond the core mission of preservation and research and embracing more open models of interpretation. Respecting the learner The Reggio Emilia approach has at its heart the belief that children are competent, curious and creative learners. By first respecting their ideas and then harnessing them through observation, documentation and discussion, educators can build the learner’s knowledge as well as their self-esteem.54 In the Reggio Emilia environment, the child’s engagement is enhanced because their contributions, based on their personal identities and cultural backgrounds, are valued.55 Through positive support and collaboration, a confident community of learners is created where all views are respected and contributors are on equal ground, no matter their age. 53.Piscitelli and Weier, ‘Learning with, through, and about Art’. 54.Fawcett and Hay, ‘5x5x5 = Creativity in the Early Years’. 55.Van Kraayenord and Paris, ‘Reading Objects’. The Reggio Emilia approach can be applied to the premise of open authority’s balance-of-power in the curator–visitor relationship. When a curator approaches the interpretation of an exhibit as if they will be on an equal footing with the visitor, it is analogous to the pre-school teacher stooping to the level of the student so that she is not towering over them authoritatively. Instead, they are eye-to-eye with the student, speaking not in a condescending tone, but in a normal cadence as they would with any adult. Open authority is not the equivalent of putting the visitor on stilts to increase their power; it is the curator approaching visitors as equal authorities so that we are all on a level playing field. Implementing the Reggio Emilia principle of respecting the learner requires museum staff to embrace the experience-generated authority of their communities. For example, a single community can include a number of amateur experts on a particular topic, ranging from a life long fossil collector to a four-year-old who knows an impressive amount about dinosaurs. A community member’s experience and passion for a topic is just as valuable as a curator’s studies of the same topic; they are both forms of authority and should be equally valued. In order to implement community contributions more fully, exhibit interpretation – be it the selection of objects, choice of subject areas or the content of labels – needs to be viewed as an ever-evolving, nimble process rather than a finished end product. Interpreters, educators and curators, within the exhibit and online, should continually seek input, not just in the form of feedback but also in the potential reinterpretation of a topic. From the perspective of Reggio Emilia, this process requires observation, dialogue and the immediate, public documentation of results. Taking this further, the results would lead to a speedy and deliberate shift in the direction of the project or gallery. In this way, museum staff are not just evaluating the results of a project, but working with the community to reinterpret and formally update the exhibit to reflect these changes. This elusively nimble process may be impractical within current exhibit practices, but it is nonetheless something to work towards. Some museums have implemented on-site visitor feedback through the use of sticky notes, which is a simple solution for incorporatingcommunity input quickly and efficiently. Online, this takes the form of comments on blogs or streams of conversation on social media platforms such as Twitter. This is a good first step, but in an exhibit space that is implementing Reggio Emilia principles, this feedback should be formally incorporated into the labels within the exhibits. Taken in this light, sticky notes could in fact perpetuate new forms of hierarchy by being seen as distinct from the curatorial voice, and even as an afterthought.56 In the future, this means of soliciting feedback might even be seen as disrespectful – but we are not there yet. Until more realistic design methods or technologies are developed that allow for more efficient updates within exhibits, these basic forms of dialogue between museum staff and community should still be used and improved upon. 56 Price, ‘Oh Snap!’. Reggio Emilia as open authority If the museum’s visitor–curator relationship is likened to the Reggio Emilia’s child–educator relationship, the open authority model becomes apparent within the Reggio approach. The Reggio classroom is in fact a daily illustration of open authority – the joining of children’s insights with the teacher’s expertise. Teachers follow the interests of the child and take advantage of the teachable moment. While open authority draws its influences from the open-source software movement, the ‘open’ in open authority refers to an open attitude rather than simply a content licensing scheme. The Reggio Emilia approach is open in the sense that educators do not begin with a firm lesson plan, but instead maintain an attitude in which they are free to adapt the day’s learning towards the students’ interests. Similarly to open authority, Reggio Emilia is essentially an open mode of knowledge production, and one that draws on a community of learners to do so. The Portland Children’s Museum includes Opal School and the Center for Learning, and is one example of an organisation that has combined the Reggio Emilia educational approach with the museum environment. The Opal School is made up of a private pre-school and a public charter elementary school serving ages three through 11. Located physically within the museum and drawing on art spaces, outdoor areas and exhibits, the mission of Opal School is to ‘provoke fresh ideas concerning environments where creativity, imagination, and the wonder of learning thrive’. Susan Mackay, Director of the Museum Center for Learning, illustrated the importance of establishing a respectful community of learners: ‘We have to participate as a community, together, in order to get a full perspective. At Opal School the children learn this at a young age; we’re building empathy from the beginning.’57 To support this focus on developing empathy, in 2013 Opal School was designated a Changemaker School in the Ashoka network’s Start Empathy initiative.58 The application of the Reggio Emilia approach in museums is a first step towards a more organic democratisation of knowledge and interpretation in exhibits and on the web – it is open authority in action. Adopting open models in exhibit interpretation and in digital communities means that museums do not require that a visitor ascribe to a particular view, but invites them to contribute to a conversation. Currently, however, when the Reggio Emilia model is adopted in museums it is always in child-oriented exhibits and institutions, and within the physical space of the museum. Museums should extend this notion beyond children alone, and onto digital platforms as well. If adults can respectfully work with children as young as two years old to understand their insights and perspectives, then this should easily be applicable to people of all ages. In order to adopt open authority wholly, I encourage museums to apply the principles of Reggio Emilia in a way that not only builds interpretation around visitor interests, but also empowers visitors to have a voice within that interpretation. 57 Susan Mackay, Personal Interview, April 17, 2013. 58 Ashoka, ‘Introducing the Newest Changemaker Schools’. Conclusion Museum professionals need to reconsider the definition of authority in order to remain connected to our communities, both on-site and online. Embracing the concept of open authority is a crucial first step that will allow museum professionals to become more comfortable with the museum’s place in an ever-expanding, digital world. By adopting the models of the open-source movement and Reggio Emilia and refocusing on the museum’s role as forum bazaar, and facilitator of platforms, we can begin to encourage more purposefully the open and collaborative interpretation of cultural heritage. We are still in the early days of this paradigm shift, which is part of a broader cultural change brought on by the connected and participatory web. More qualitative work needs to be done to discern where our current practices fall within a still-undefined spectrum of open authority. New models can and should be developed that further hone the museum field’s true potential as authoritative leaders and facilitators of platforms. We have not yet discussed how open authority might differ between museums of varying types and sizes. Most importantly, we have not considered how we can use this concept to incorporate more seriously new voices into the core of interpretation, rather than in supplemental programmes. Museums are beginning to recognise the need to re-envision their place as authorities within this quickly shifting digital landscape, if only to remain relevant and connected to society. More importantly, museums have a social obligation to embrace openness, increase accessibility and contribute to broader conversations.59 This reconceptualisation of authority does not mean that the museum field needs to disregard ethics, mission or the inherent purpose of stewardship. On the contrary, it is an opportunity to deepen the museum’s purpose in this regard. I believe that open authority is a new way to approach the mission of museums, in a way that can uphold the promise and potential of both the temple and the bazaar. Acknowledgements A version of this chapter was originally published in Lori Byrd Phillips, ‘The Temple and the Bazaar: Wikipedia as a Platform for Open Authority in Museums’, Curator: The Museum Journal 56, no. 2 (2013): 219–35. References Adair, Bill, Benjamin Filene and Laura Koloski, eds. Letting Go? Sharing Historical Authority in a User-Generated World. Philadelphia, PA: The Pew Center for Arts and Heritage, 2011. 59.McGonigal, ‘Gaming the Future of Museums’. Ames, Michael M. ‘Are Changing Representations of First Peoples in Canadian Museums and Galleries Challenging the Curatorial Prerogative?’. In The Changing Presentation of the American Indian, National Museum of the American Indian, 73–88. Seattle, WA: University of Washington Press, 2000. Anderson, Maxwell L. ‘Prescriptions for Art Museums in the Decade Ahead’. Curator 50, no. 1 (2007): 9–17. Ashoka. ‘Introducing the Newest Changemaker Schools’. Start Empathy blog, June 17, 2013. http://startempathy.org/blog/2013/06/introducing-newest.changemaker-schools. Benkler, Yochai. Wealth of Networks. New Haven, CT: Yale University Press, 2006. http://www.benkler.org/wonchapters.html. Cameron, Duncan F. ‘The Museum, a Temple or the Forum’. Curator 14, no. 1 (1971): 61–79. Cameron, Fiona and Helena Robinson. ‘Digital Knowledgescapes: Cultural, Theoretical, Practical and Usage Issues Facing Museum Collection Databases in a Digital Epoch’. In Theorizing Digital Cultural Heritage: A Critical Discourse, edited by Fiona Cameron and Sarah Kenderdine, 165–92. Cambridge, MA: MIT Press, 2007. Carr, Nicholas. ‘Is Google Making Us Stupid?’. The Atlantic, August 2008. http://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/6868/. Center for the Future of Museums. TrendsWatch 2012: Museums and the Pulse of the Future. Washington, DC: American Association of Museums, 2012. http:// futureofmuseums.org/reading/publications/upload/TrendsWatch2012.pdf. Chan, Sebastian. ‘Culture + Heritage + Digital at Web Directions South 2011’. Fresh + New(er), October 18, 2011. http://www.powerhousemuseum.com/ dmsblog/index.php/2011/10/18/culture-heritage-digital-at-web-directions.south-2011/. Chung, James. ‘Museums and Society 2034: Trends and Potential Future’. Trends paper, Center for the Future of Museums, American Association of Museums, Washington, DC, December 2008. Dodd, J. and R Sandell, eds. ‘Collections Management and Inclusion’. In Including Museums: Perspectives on Museums, Galleries and Social Inclusion, 80–3. Leicester: University of Leicester, Department of Museum Studies, 2001. Edson, Michael. ‘The Commons: Helping People Get Stuff Done’. In All Together Now: Museums and Online Collaborative Learning, edited by William Crow, 15–27. Washington, DC: AAM Press, 2011. Eschenfelder, Kristin R. and Michelle Caswell. ‘Digital Cultural Collections in an Age of Reuse and Remixes’. First Monday 15 (November 1, 2010). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/ view/3060%60/2640. Falk, John. Thriving in the Knowledge Age: New Business Models for Museums and Other Cultural Institutions. Lanham, MD: Altamira Press, 2006. Fawcett, Mary and Penny Hay. ‘5x5x5 = Creativity in the Early Years’. International Journal of Art and Design Education 23, no. 3 (2004): 234–45. Firlik, Russell. ‘Promoting Development through Constructing Appropriate Environments’. Daycare and Early Education 22 (1994): 12–20. Freire, Paolo. Pedagogy of the Oppressed. New York: Continuum, 2000. Friedman, Thomas L. The World Is Flat: A Brief History of the Twenty-First Century. New York: Farrar, Straus and Giroux, 2006. Gandini, Lella. ‘The Essential Voices of the Teachers’. In In the Spirit of the Studio: Learning from the Atelier of Reggio Emilia, edited by Lella Gandini, Lynn T. Hill, Louise Boyd Cadwelland Charles Schwall. New York: Teachers College Press, 2005. Gurian, Elaine Humann. ‘ASavings Bank for the Soul’. In Civilizing the Museum: The Collected Writings of Elaine Heumann Gurian, 88–96. London: Routledge, 2006. Gurian, Elaine Humann. ‘What Is the Object of This Exercise?’. In Civilizing the Museum: The Collected Writings of Elaine Heumann Gurian, 33–47. London: Routledge, 2006. Gurian, Elaine Humann. ‘Timeliness: A Discussion for Museums’. In Civilizing the Museum: The Collected Writings of Elaine Heumann Gurian, 57–64. London: Routledge, 2006. Hirzy, Ellen. Excellence and Equity: Education and the Public Dimension of Museums. Washington, DC: American Association of Museums, June 1992. http://www.aam-us.org/sp/exc-eq.cfm. Hooper-Greenhill, Eilean. The Educational Role of the Museum. 2nd Edn. New York: Routledge, 1999. Lévy, Pierre. Collective Intelligence: Mankind’s Emerging World in Cyberspace. Translated by Robert Bononno. Cambridge: Perseus Books, 1997. Lih, Andrew. The Wikipedia Revolution. New York: Hyperion, 2009. McGonigal, Jane. ‘Gaming the Future of Museums’. Presentation at the Center for the Future of Museums, Washington DC, December 2, 2008. http://www. futureofmuseums.org/events/lecture/mcgonigal.cfm. Merritt, Elizabeth. ‘The Next Frontier of Museum Ethics’. Center for the Future of Museums, December 6, 2011. http://futureofmuseums.blogspot.com/2011/12/ next-frontier-of-museum-ethics.html. Neuberg, Brad. ‘What Is the Open Web and Why Is It Important?’. Coding in Paradise, April 1, 2008. http://codinginparadise.org/weblog/2008/04/whats.open-web-and-why-is-it-important.html. New Media Consortium. ‘A Communiqué from the Horizon Project Retreat’. New Media Consortium Horizon Project, 2012. http://www.nmc.org/news/ download-communique-horizon-project-retreat. Oomen, Johan and Lora Aroyo. ‘Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges’. Agora, March 18, 2011. http://www.cs.vu. nl/~marieke/OomenAroyoCT2011.pdf. Paris, Scott G. and Susanna E. Hapgood. ‘Children Learning with Objects in Informal Learning Environments’. In Perspectives on Object-Centered Learning in Museums, edited by Scott G. Paris, 37–54. London: Lawrence Erlbaum Associates, 2002. Phillips, Lori Byrd. ‘Defining Open Authority in Museums’. MIDEA, New Media Consortium, January 13, 2012. http://midea.nmc.org/2012/01/defining-open-authority-in-museums/. Phillips, Lori Byrd. ‘Open Authority & the Future of Museum Ethics’. Center for the Future of Museums, February 21, 2012. http://futureofmuseums.blogspot. com/2012/02/open-authority-future-of-museum-ethics.html. Piscitelli, Barbara and Katrina Weier. ‘Learning with, through, and about Art: The Role of Social Interactions’. In Perspectives on Object-Centered Learning in Museums, edited by Scott G. Paris, 121–51. London: Lawrence Erlbaum Associates, 2002. Poole, Nick. ‘The Rise and Fall of the Curator’. OpenCulture, Collections Trust, December 14, 2011. http://openculture.collectionstrustblogs.org. uk/2011/12/14/the-rise-and-fall-of-the-curator/. Price, Jonathan. ‘Oh Snap! AMuseum Gets Us to Share Our Images’. MuseumZero, 2013. http://museumzero.blogspot.co.uk/2013/04/oh-snap-museum-gets-us-to-share-our.html. Proctor, Nancy. ‘The Museum as Distributed Network’. MuseumID, 2010. http:// www.museum-id.com/idea-detail.asp?id=337. Raymond, Eric S. ‘The Cathedral and the Bazaar’, 2000. http://www.catb.org/~esr/ writings/homesteading/cathedral-bazaar/. Richardson, Jim. ‘The Audience Is Dead – Let’s Talk Participants Instead’. MuseumNext, 2011. http://www.museumnext.org/2010/blog/museum_ audience_development. Rosenzweig, Roy. ‘Can History Be Open-Source? Wikipedia and the Future of the Past’. Journal of American History 93, no. 1 (2006): 117–46. Sample Ward, Amy. ‘Crowdsourcing vs Community-Sourcing: What’s the Difference and the Opportunity?’. Amy Sample Ward’s Version of NPTech, 2011. http://amysampleward.org/2011/05/18/crowdsourcing-vs-community.sourcing-whats-the-difference-and-the-opportunity/. Shirky, Clay. Here Comes Everybody: The Power of Organizing Without Organizations. New York: Penguin Books, 2009. Simon, Nina. The Participatory Museum. Santa Cruz, CA: Museum 2.0, 2010. http://www.participatorymuseum.org/. Smithsonian Institution. ‘Smithsonian Institution Web and New Media Strategy’, 2009. http://www.si.edu/About/Policies. Srinivasan, Ramesh, Robin Boast, Katherine M. Becvar and Jonathan Furner. ‘Blobgects: Digital Museum Catalogs and Diverse User Communities’. Journal of the American Society for Information Science and Technology 60, no. 4 (2009): 666–78. Stein, Robert. ‘Chiming in on Museums and Participatory Culture’. Curator 55, no. 2 (2012): 215–26. Tapscott, Don and Anthony D. Williams. Wikinomics: How Mass Collaboration Changes Everything. London: Portfolio, 2007. Van Kraayenord, Christina E. and Scott G. Paris. ‘Reading Objects’. In Perspectives on Object-Centered Learning in Museums, edited by Scott G. Paris, 215–34. London: Lawrence Erlbaum Associates, 2002. Wallace, Ryan. ‘Reciprocal Research Network’. Museums and the Web, San Diego, 2012. http://www.museumsandtheweb.com/mw2012/best/research_ online_collection/reciprocal_research_n. Weil, Stephen E. ‘From Being about Something to Being for Somebody: The Ongoing Transformation of the American Museum’. Daedalus 128 (1999): 229–58. Weinberger, David. Everything Is Miscellaneous: The Power of the New Digital Disorder. 1st Edn. New York: Holt, 2008. Wyatt, Liam. The Academic Lineage of Wikipedia: Connections and Disconnections in the Theory and Practice of History. Sydney: University of New South Wales, 2008. This page has been left blank intentionally Chapter 12 Making Crowdsourcing Compatible with the Missions and Values of Cultural Heritage Organisations Trevor Owens To what extent are the values and missions of cultural heritage organisations compatible with the idea of crowdsourcing? While libraries, archives and museums have a long history of collaboration with members of the public, the idea of outsourcing work to the crowds should give cultural heritage professionals pause. It is critical for the cultural heritage community to reflect on the values and ethics at stake in inviting members of the public, often referred to as ‘the crowd’, to tag and classify, transcribe, organise and otherwise add value to digital cultural heritage collection content.1 In this chapter I offer a philosophical, psychological and technical framework for approaching the components of crowdsourcing I see as compatible with the mission and values of cultural heritage organisations. In the process, I characterise a series of distinct components of crowdsourcing projects and suggest how, when thoughtfully deployed, these components offer some of the most meaningful ways that cultural heritage organisations can serve their missions on the web. The Two Problems with Crowdsourcing: Crowd and Sourcing There are two primary problems with bringing the idea of crowdsourcing into cultural heritage organisations. Both the idea of the crowd and the notion of sourcing are problematic terms. The most successful crowdsourcing projects in libraries, archives and museums have not involved massive crowds and they have very little to do with outsourcing labour. The term ‘crowd’ is somewhat misleading, since most successful crowdsourcing projects do not rely on large, anonymous masses of people. These projects succeed by inviting participation from engaged members of the public. The success is built .For projects focused on classification, see Trant, ‘Curating Collections Knowledge’. For a discussion of crowdsourcing in cultural heritage, see Holley, ‘Crowdsourcing’; Smith-Yoshimura, Social Metadata for Libraries, Archives, and Museums. upon a long-standing tradition of volunteerism and involvement of citizens in the creation and development of public good. For example, the New York Public Library’s (NYPL) menu transcription project, What’s on the menu? (see Chapter 5), invites members of the public to help transcribe the names and costs of menu items from digitised copies of menus from New York restaurants. Any interested person is free to visit the project website and start transcribing menus. But in practice, a dedicated community of foodies, New York history buffs, chefs and self-motivated individuals are the ones who are excited about offering their time and energy to help volunteer and contribute, improving the public library’s resource for others to use.2 (Years ago, NYPL volunteers sat at a desk in the reading room cataloguing the original collection of menus).3 The technology enabling this participation may be relatively new. However, the menu project is a continuation of a long-standing tradition of inviting members of the public to help refine, enhance and support resources. In short, ‘crowdsourcing’the menu transcription is not about crowds at all. It is about using digital tools to invite in volunteers in much the same way that members of the public have volunteered to help organise and add value to the NYPL collection in the past. The problem with the term ‘sourcing’ is its association with labour. Wikipedia’s definition of crowdsourcing helps further clarify this relationship: ‘Crowdsourcing is a process that involves outsourcing tasks to a distributed group of people.’4 The keyword in that definition is ‘outsourcing’. Crowdsourcing is a concept that was invented and defined in the business world and it is important that we reconsider the things that change when we bring the term into cultural heritage. At this point, we need to think for a moment about what we mean by terms like work and labour. While it might be acceptable for commercial entities to coax individuals to provide free labour, the ethical implications of such methods should give pause to cultural heritage organisations. It is critical here to unpack some of the different meanings we ascribe to ‘work’. When we use the term ‘a day’s work’ we are directly referring to labour – to the kinds of work that one engages in as a financial transaction for pay. In contrast, when we use the term ‘work’ to refer to someone’s ‘life’s work’, we are referring to something that is significantly different. The former is about acquiring the resources one needs to survive. The latter is about the activities that we engage in that give our lives meaning. The values and missions of cultural heritage organisations offer this kind of opportunity for meaningful contribution. However, when we invite the public to contribute, we should not treat them as a crowd, and we should not attempt to source labour from them. When we invite the public to participate in our work we should do so under more ennobling terms. 2.Vershbow, ‘Bringing in the Crowd’. Vershbow slides are online at http://www. digitalpreservation.gov/meetings/documents/ndiipp11/vershbow.pdf. 3.See Taranto, ‘Crowdsourcing Metadata’. Video available online at http://vimeo. com/38196574. 4.Crowdsourcing, http://en.wikipedia.org/wiki/Crowdsourcing. Citizen Scientists, Archivists and the Meaning of Amateur Some of the projects that fit under the heading of crowdsourcing have chosen very different terms to describe themselves. The names of these projects highlight the extent to which they invite participation from members of the public who identify with particular professional occupations and their characteristic ways of thinking. For example, the Galaxy Zoo project, which invites users to identify different types of galaxies in images collected from the Sloan Digital Sky Survey, refers to its users as citizen scientists.5 Similarly, the United States National Archives and Records Administration recently launched a crowdsourcing project, the Citizen Archivists Dashboard; its terminology invites ‘citizens’ – not some anonymous ‘crowd’ – to participate.6 While these citizen archivists and scientists are not professional in the literal sense (they are unpaid for their expertise), they connect with something a bit different than volunteerism. They are amateurs in the truest and best possible sense of the term. Amateurs have a long and vibrant history as contributors to the public good. Coming to English from French, the term amateur means a ‘lover of’. The primarily negative connotations we place on the term are a relatively recent development. In other eras, the term ‘amateur’ simply meant that someone was not a professional, that is, they were not paid for these particular labours of love. Charles Darwin, Gregor Mendel and many others who made significant contributions to the sciences did so as amateurs. As a continuation of this line of thinking, the various Galaxy Zoo projects see the amateurs who participate as peers, in many cases listing them as co-authors of academic papers published as a result of their work. I suggest that we think of crowdsourcing not as extracting labour from a crowd, but as a way for us to invite the participation of amateurs (in the non-derogatory sense of the word) – those with every bit of the potential to be a Darwin or Mendel – in the creation, development and further refinement of public good. Towards a Better, More Nuanced, Notion of Crowdsourcing Fighting against the common usage of a word, however, is rarely a successful project. From here on, I will continue to use and refine a definition for crowdsourcing that works for the cultural heritage sector. I will explain what I think are the four key components of this ethical crowdsourcing, which invites members of the public to participate as amateurs in the production, development and refinement of public goods. These fall into the following four considerations: human computation; the wisdom of crowds; thinking of tools and software as scaffolding; and the psychology of participant motivation. Each of these phrases suggests a series of questions to ask of any cultural heritage crowdsourcing project. 5 Galaxy Zoo, http://www.galaxyzoo.org/. 6.Citizen Archivist Dashboard, http://www.archives.gov/citizen-archivist/. I believe these four concepts provide us with the descriptive language to understand what makes crowdsourcing such a powerful tool – not only for improving and enhancing data related to cultural heritage collections, but also as a way for deep engagement with the public. Human Computation ‘Human computation’ is grounded in the fact that human beings are able to process particular kinds of information and make judgments in ways that computers cannot. To this end, a range of projects described as ‘crowdsourcing’ are anchored in the idea of treating people as processors. The best way to explain the concept is through a few examples of the role that human computation plays in crowdsourcing. ReCaptcha is a great instance of how the processing power of humans can be harnessed to improve cultural heritage collection data.7 Most readers will be familiar with the little ReCaptcha boxes we fill out when we need to prove that we are in fact a person and not an automated system attempting to log into a website. Our ability to read the strange and distorted text presented by the software proves that we are people. However, in the case of ReCaptcha it also helps us correct the relatively poor quality of the automated text of digitised New York Times and Google Books. The same capability that allows people to differentiate themselves from machines is what allows us to help improve the full text search of the digitised New York Times and Google Books collections. The principles of human computation are similarly on display in the Google Image Labeller game. From 2006 to 2011 the Google Image Labeller game invited members of the public to describe and classify the content of images in a head-to-head game. For example, imagine that a player is viewing a red car. Somewhere else in the world, another player is also viewing that image. Each player is invited to type in labelsfor the image, with a series of ‘off-limits’words, which have already been associated with the image (perhaps ‘red’ and ‘car’). Each label I can enter which matches a label entered by the other player results in game points (perhaps entering ‘road’, identifying the road underneath the car in the picture). The game has inspired an open-source version specifically designed for use at cultural heritage organisations.8 This interaction has been designed such that, in most cases, it results in generating increasingly high quality and detailed descriptions of images. 7.See Von Ahn et al., ‘ReCaptcha’. For some further context on both ReCaptcha and Google Image Labeller, see Von Ahn, ‘Human Computation’. 8.For an example of a particular implementation of this kind of tagging game for a cultural heritage organisation see http://www.brooklynmuseum.org/opencollection/tag_ game/start.php and for an example of the generic utility created to make it easy to create such projects see Metadata Games, http://www.tiltfactor.org/metadata-games. Making Crowdsourcing Compatible with the Missions and Values 273 Both the Image Labeller and ReCaptcha are fundamentally about tapping into the capabilities of people to process information. I had earlier suggested that the kind of crowdsourcing I want us to be thinking about is not about labour, yet these kinds of human computation projects are often fundamentally about labour. This is most clearly visible in Amazon’s Mechanical Turk, a site that allows anyone to program and post rudimentary tasks (transcribing text, write short reviews, identifying if images are obscene) and pay people (often pennies per individual task) to complete them. Mechanical Turk’s tagline is that it ‘gives businesses and developers access to an on-demand, scalable workforce’where ‘workers select from thousands of tasks and work whenever it’s convenient’.9 The labour focus of this site should give pause to those in the cultural heritage sector, particularly those working for public institutions. There are legitimate concerns that this kind of labour could be serving as a kind of ‘digital sweatshop’.10 While there are reasons to worry about the potentially exploitive properties of projects like Mechanical Turk, it is important to realise that many of the same human computation activities which one could run through Mechanical Turk, like describing images or transcribing text, are not really the same kind of labour when they are situated as projects of ‘citizen science’. For example, Galaxy Zoo invites individuals to identify galaxies. The activity is fundamentally similar to the Google Image Labeller game: users are presented with an image of a galaxy and invited to classify it based on a simple set of taxonomic information. While the interaction is more or less the same, the change in context is essential. In Galaxy Zoo, participants are invited as peers into the process of scientific investigation. While the image identification task here is more or less the same as the others previously discussed, this site – at least in the early stages of the project – often gave these amateur astronomers their first-ever opportunity to see these stellar objects.11 These images were all captured by a robotic telescope so the first Galaxy Zoo participants who looked at these images were actually the first people ever to see each of these stellar objects. In this case, the amateurs who catalogue these galaxies do so predominantly because they want to contribute to science.12 Beyond engaging in this classification activity, the Galaxy Zoo project also invites members to discuss the galaxies in a forum. This discussion forum ends up representing a very different kind of crowdsourcing, one based not so much on the idea of human computation but instead on a notion which I refer to here as the wisdom of crowds, in which the open discussion spaces of the forums create a 9 Mechanical Turk, https://www.mturk.com/mturk/welcome. 10.For some brief coverage of these discussions see Williams, ‘The Reliability, Efficiency, and Affordability of Amazon’s Mechanical Turk’; Williams, ‘The Ethics of Amazon’s Mechanical Turk’. 11.For an account of the history of the Galaxy Zoo project, see ‘Chris Lintott on The Galaxy Zoo’, 2010, http://www.youtube.com/watch?v=j_zQIQRr1Bo. 12.Raddick et al., ‘Galaxy Zoo’. place for more open-ended development of free-form description, discussion and analysis of these images. Akey question emerges from the concept of human computation: how could we use human judgement to augment computer-processable information? It would be a waste of the public’s time to invite people to complete a task that a computer could do. The value that human computation offers is the potential of how the unique capabilities of people can be integrated into systems for the creation of public good. The Wisdom of Crowds, or Why Wasn’t I Consulted? The phrase comes from James Surowiecki’s book, The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. Surowiecki talks about a range of examples of how crowds of people can create important and valuable kinds of knowledge. Unlike human computation, the wisdom of crowds is not about highly structured activities. In Surowiecki’s argument, the wisdom of crowds is an emergent phenomenon resulting from how discussion and interaction platforms like Wikis enable individuals to add to and edit each other’s work. The ‘wisdom of crowds’ notion tends to come with a bit too much utopian baggage. It is hard to believe that a technology like the web would substantively change our rather imperfect human nature. In light of this, Paul Ford’s reformulation of the wisdom of crowds is particularly valuable. Ford, in his blog, suggests that the heart of this matter is that the web, unlike other media, is particularly well suited to answer the question ‘Why wasn’t I consulted?’:13 Why wasn’t I consulted, which I abbreviate as WWIC, is the fundamental question of the Web. It is the rule from which other rules are derived. Humans have a fundamental need to be consulted, engaged, to exercise their knowledge (and thus power), and no other medium that came before has been able to tap into that as effectively. He goes on to explain a series of projects that succeed because of their ability to energise this human desire to be consulted: If you tap into the human need to be consulted you can get some interesting reactions. Here are a few: Wikipedia, StackOverflow, Hunch, Reddit, MetaFilter, YouTube, Twitter, StumbleUpon, About, Quora, eBay, Yelp, Flickr, IMDB, Amazon.com, Craigslist, GitHub, SourceForge, every messageboard or site with comments, 4Chan, Encyclopedia Dramatica. Plus the entire Open Source movement. 13.See Ford, ‘The Web Is a Customer Service Medium’. Each of these cases taps into our desire to respond. The comments sections in online news articles, or our ability to sign up for an account and start providing our thoughts and ideas on Twitter or in a Tumblr, are fundamentally about this desire to be heard. Returning to the example of Galaxy Zoo, the carefully designed human computation classification exercise provides one kind of input, while the project’s active web forums capitalise on the opportunity to consult. For better and for worse, the interactive power of the web as a medium comes from the invitation sites offer to their visitors to comment, edit and otherwise enhance content through the open invitation of the free text box, inviting them to share their perspective. Importantly, some of the most valuable discoveries in the Galaxy Zoo project – including an entirely new kind of green-coloured galaxy – were the result of users sharing and discussing some of the images from the classification exercise in the open discussion forums. Akey question emerges from the concept of the ‘wisdom of crowds’: how could we empower and consult with a community of users? Unlike human computation, the goal here is not users’ ability to process information or make judgements, but rather their desire to provide their knowledge. Tools as Scaffolding Helping someone succeed is often about getting them the right tools. Consider the function of scaffolding. Whether in constructing a house or building a workforce, scaffolding puts workers in a position to do their job. By standing on the scaffolding, they are able to do their work without thinking about what supports them. Once they are immersed in the activity of the work, the tool ‘disappears’ and allows them to go about their tasks while taking for granted that they are suspended six or seven feet in the air. It is fruitful to think about a wide range of tools that serve as scaffolds.14 All tools can act as scaffolds to enable us to accomplish a particular task. This observation has a direct translation into the design of online tools as well. For example, before joining the Library of Congress, I worked on the Zotero project, a widely used free and open-source reference management tool that helps researchers collect, organise and cite bibliographic information. Zotero was translated into more than 30 languages by its users. The translation process was made significantly easier through BabelZilla, an online community for developers and translators of extensions for Firefox web browser. BabelZilla has a robust community of users who work to create translations. One of the most ingenious features of this platform is that texts that need to be translated from the source 14.Here I am drawing on the Vygotskyan tradition of ‘scaffolding’. See Wood et al., ‘The Role of Tutoring in Problem Solving’. On the idea of cultural mediation, see Vygotsky, Mind in Society. code are stripped out so that potential translators are presented with a simple web page where they just type in translations of lines of text. This not only makes the process much simpler and quicker, it also means that potential translators need absolutely no knowledge of the programming in order to contribute to a translated version. Without BabelZilla, a potential translator would need to know about how Firefox works in a technical sense, and be comfortable with editing files in technical formats like XML (Extensible Mark-up Language) in a text editor. However, BabelZilla scaffolds users over that required knowledge by delegating the complexity of creating the underlying XML to the BabelZilla translation tool, so users just fill out translations in a simple text box on a web page. If we again return to the Galaxy Zoo example, we can now think of the classification task as a scaffold that allows interested amateurs to participate at the cutting edge of scientific inquiry. In this scenario, the entire technical apparatus, all of the technical equipment used in the Sloan Digital Sky Survey, the design of the Galaxy Zoo site and all the work of the scientists and engineers that went into those systems are part of one big scaffold that puts users in a position to contribute to the frontiers of science through their actions on the website, without needing the skills and background of a professional scientist.15 Additional key questions emerge from the notion of scaffolding: how can our tools act as scaffolds to help make the most of user efforts? What expertise can we embed inside the design of our tools to magnify user efforts? How can our tools put a potential user in exactly the right position, with the right knowledge, just at the moment he or she needs it, to accomplish a given activity? Understanding Participant Motivation Ben Brumfield runs a range of crowdsourcing transcription projects.16 At one point he noticed that one of his most prolific contributors was slowing down, cutting back significantly on the time spent transcribing these manuscripts. The user explained that there were not many manuscripts left to transcribe. For this user, the two or three hours a day spent working on transcriptions were important. Participating in this project was so satisfying, and contributing to it involved so much self-image, that the user needed to ration out those remaining pages to make sure that the experience lasted as long as it could. When Brumfield found that out, he quickly put up some more pages. This particular story illustrates several points about what motivates us. After people’s basic needs are covered, they tend to be primarilymotivated by things that are not financial. People identify and support causes and projects that 15.This broader understanding of tools is best explained in Andy Clark’s notion of cognitive extension. Clark, Supersizing the Mind. 16.For background on Ben Brumfield’s crowdsourcing work and projects, see his blog at http://manuscripttranscription.blogspot.com/. provide them with a sense of purpose. They establish and sustain their identity and sense of self through their actions.17 People get meaning from doing things that matter to them. They find a sense of belonging by being a part of something bigger than themselves. Projects that can tap into these identities and purposes while providing meaning to people’s lives are projects that – far from exploiting people – can provide a way for them to connect with each other and make meaningful contributions to the public good. This is one of the places where libraries, archives and museums have the most to offer. As stewards of cultural memory, our institutions have a strong sense of purpose and their explicit mission is to serve the public good. This notion of motivation prompts further key questions for projects: to whose sense of purpose does this project connect? What identities are involved? To what kinds of people does this project matter? How can we connect with and invite the participation of those people? Why Are We Putting Cultural Heritage Collections Online Again? There are many reasons that we put digital collections online. The single most important is to make history and culture accessible so that we can invite students, researchers, teachers and the public to explore and connect with our past. Historians, librarians, archivists and curators who share digital collections and exhibits can measure their success in moving towards this goal by how people use, reuse, explore and understand these objects. These crowdsourcing projects are commonly described as a means by which we can get better data to help enable the kinds of use and reuse that we want people to make of our collections. In this respect, the general idea of crowdsourcing is described as an instrument for getting people to help us with data that can make collections more accessible. Crowdsourcing does this – and more. In the process of developing these crowdsourcing projects we have stumbled onto something far more exciting than speeding up or lowering the costs of document transcription. An example will help illustrate. Increased Use, Deeper Use: Crowdsourcing Civil War Diaries In 2011, the University of Iowa libraries crowdsourced the transcription of a set of Civil War diaries. According to Nicole Saylor, the head of Digital Library Services, the project was very successful.18 The library rapidly received extensive transcriptions and ended up attracting more donors to support its work. 17.For a popular account of current thinking on the psychology of motivation, see Pink, Drive. For more substantive but still accessible academic research on the subject, see the essays in Elliot and Dweck, Handbook of Competence and Motivation. 18.Owens, ‘Crowdsourcing the Civil War’. The project also succeeded in dramatically increasing visitors to the library’s website. As Saylor explained, ‘On June 9, 2011, we went from about 1,000 daily hits to our digital library on a really good day, to more than 70,000.’As great as all this is, as far as I am concerned, something even more valuable also happened. When people came to transcribe the diaries, they engaged with the objects more deeply than they would have if transcription were not an option. Consider this quote from Saylor explaining how one particular transcriber interacted with the collection: The transcriptionists actually follow the story told in these manuscripts and often become invested in the story or motivated by the thought of furthering research by making these written texts accessible. One of our most engaged transcribers, a man from the north of England, has written us to say that the people in the diaries have become almost an extended part of his family. He gets caught up in their lives, and even mourns their deaths. He has enlisted one of his friends, who has a Ph.D. in military history, to look for errors in the transcriptions already submitted. “You can do it when you want as long as you want, and you are, literally, making history”, he once wrote us. That kind of patron passion for a manuscript collection is a dream. Of the user feedback we’ve received, a few of my other favorites are: “This is one of the COOLEST and most historically interesting things I have seen since I first saw a dinosaur fossil and realized how big they actually were”. “I got hooked and did about 20. It’s getting easier the longer I transcribe for him because I’m understanding his handwriting and syntax better”. “Best thing ever. Will be my new guilty pleasure that I don’t even need to feel that guilty about”. The transcriptions are useful – they make the content more accessible – but as Saylor explains, ‘The connections we’ve made with users and their sustained interest in the collection is the most exciting and gratifying part’. This is exactly as it should be! The open invitation to contribute provided by crowdsourcing and the particular call to action to transcribe these documents on the anniversary of the Civil War are the most meaningful and precious user experiences that a cultural heritage institution can offer. It is essential that the project provide meaningful work. These projects invite the public to leave a mark and help enhance the collections. If the goal is to get people to engage with collections and with the past, then the transcripts are actually a wonderful by-product of offering meaningful activities for the public to engage in. Meaningful Activity Is the Apex of User Experience for Cultural Heritage Collections What crowdsourcing does (and most digital collection platforms fail to do) is to offer an opportunity for someone to do something more than consume information. When done well, crowdsourcing offers us an opportunity to provide meaningful ways for individuals to engage with and contribute to public memory. Not limited to being an instrument which enables us to deliver content better to end users, crowdsourcing is the best way actually to engage our users in the fundamental reason that these digital collections exist in the first place. When we adopt this mindset, the money spent on crowdsourcing projects – in terms of designing and building systems, staff time to manage and so on – cannot be compared to the costs of having someone transcribe documents on Mechanical Turk. The transcription of those documents is actually a precious bit of activity that could mean the world to someone. You cannot ask users to take on just any task or obstacle – for example, if you asked users to transcribe documents that could easily be transcribed by computers, the whole project would lose its meaning and purpose. It is not about Sisyphean tasks; it is about providing meaningful ways for the public to enhance collections while more deeply engaging with and exploring them. Just as Brumfield’s user rationed out the transcription of those documents, we might actually think about crowdsourcing as one of the most valuable experiences we can offer our users. Instead of simply giving them the ability to browse or poke around in digital collections, we can invite them to participate. We are in a position to let the users of these collections leave a mark on the collections. Instead of browsing through a collection they literally become authors of our historical record. Acknowledgement This essay is based on a talk given as part of a workshop at the International Internet Preservation Consortium General Assembly, May 4, 2012. A version of this essay was published in Trevor Owens, ‘Digital Cultural Heritage and the Crowd’, Curator: The Museum Journal 56, no. 1 (2013): 121–30. References Clark, Andy. Supersizing the Mind: Embodiment, Action, and Cognitive Extension. New York: Oxford University Press, 2008. Elliot, Andrew and Carol Dweck, eds. Handbook of Competence and Motivation. New York: Guilford Press, 2005. Ford, Paul. ‘The Web Is a Customer Service Medium’. Blog, January 6, 2011. http://www.ftrain.com/wwic.html. Holley, Rose. ‘Crowdsourcing: How and Why Should Libraries Do It?’. D-Lib Magazine 16, no. 3/4 (2010). http://www.dlib.org/dlib/march10/ holley/03holley.html. Owens, Trevor. ‘Crowdsourcing the Civil War: Insights Interview with Nicole Saylor’. The Signal: The Library of Congress Digital Preservation Blog, December 6, 2011. http://blogs.loc.gov/digitalpreservation/2011/12/crowdsourcing-the-civil.war-insights-interview-with-nicole-saylor/. Pink, Daniel. Drive: The Surprising Truth about What Motivates Us. New York: Riverhead, 2009. Raddick, Jordan, Georgia Bracey, Pamela Gay, Chris Lintott, Phil Murray, Kevin Schawinski, Alexander Szalay and Jan Vandenberg. ‘Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers’. Astronomy Education Review 9, no. 1 (2010). http://portico.org/stable?au=pgg3ztfdp8z. Smith-Yoshimura, Karen. Social Metadata for Libraries, Archives, and Museums: Executive Summary. Dublin, OH: OCLC Research, 2012. Surowiecki, James. The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. New York: Doubleday, 2004. Taranto, Barbara. ‘Crowdsourcing Metadata’. Paper presented at the Coalition for Networked Information Fall 2011 Membership Meeting, Arlington, VA, December 12–13, 2011. Trant, Jennifer. ‘Curating Collections Knowledge: Museums on the Cyberinfrastructure’. In Museum Informatics: People, Information and Technology in Museums, edited by Paul F. Marty and Katherine Burton Jones, 275–91. New York: Routledge, 2008. Vershbow, Ben. ‘Bringing in the Crowd: Effects, Affects and a Few (Minor) Defects’. Paper presented at Make It Work: Improvisations on the Stewardship of Digital Information, conference of the National Digital Stewardship Alliance, Washington, DC, July 19–21, 2011. Von Ahn, Luis. ‘Human Computation’. Google TechTalks, 2006. http://video. google.com/videoplay?docid=-8246463980976635143. Von Ahn, Luis, Ben Maurer, Colin McMillen, David Abraham and Manuel Blum. ‘ReCaptcha: Human-Based Character Recognition via Web Security Measures’. Science 321, no. 5895 (2008): 1465–8. doi:10.1126/science.1160379. Vygotsky, L.S. Mind in Society: The Development of Higher Psychological Processes. Edited by Michael Cole, Vera John-Steiner and Sylvia Scribner. Cambridge, MA: Harvard University Press, 1978. Williams, George. ‘The Reliability, Efficiency, and Affordability of Amazon’s Mechanical Turk’. The Chronicle of Higher Education. ProfHacker, February 22, 2010. http://chronicle.com/blogs/profhacker/the-reliability-efficiencyaffordability-of-amazons-mechanical-turk/22994. Williams, George. ‘The Ethics of Amazon’s Mechanical Turk’. The Chronicle of Higher Education. ProfHacker, March 1, 2010. http://chronicle.com/blogs/ profhacker/the-ethics-of-amazons-mechanical-turk/23010. Wood, David, Jerome Bruner and Gail Ross. ‘The Role of Tutoring in Problem Solving’. Journal of Child Psychology and Psychiatry 17 (1976): 89–100. Index 281 Index Ancestry 132, 223 APIs (Application programming interfaces) 106–7, 122, 126, 129, 135, 223 archives 91, 92–3, 96, 130, 140, 143, 149, 152, 211–31, 271; see also Waisda? authority 7, 213, 218, 221, 222, 247–63 BBC 186, 187, 188 British Library 1, 48, 67–8, 242 Brooklyn Museum 6, 17–43, 47, 190 Click! 19–25 GO: a community-curated open studio project 29–42 Split Second 25–9 citizen history 4, 53–4 citizen science 4, 6, 7, 45, 50–51, 54, 143, 180, 195, 225, 232, 234, 236, 271 constructivism 213, 235, 252, 259 crowdfunding 236 crowdsourced tasks 6, 145, 163, 237, 239–42 categorising, see tagging collecting 19–25, 144, 146, 148–51, 240; see also Click!; The Welsh Experience of the First World War contextualisation and identification 94, 156, 164, 226, 242, 256 curation 19–25, 46–8 georeferencing, georectification 114, 117, 145, 242 markup 63, 71, 76, 77, 80, 81 observation 50, 240 tagging 18, 48–9, 238, 241; see also steve.museum; Waisda?; Your Paintings Tagger text correction 8, 39–40, 72, 145, 156, 239–40; see also National Library of Australia Trove transcription 6, 8, 49, 52, 145, 151–3, 156, 223, 240, 276, 277–8; see also Papers of the War Department; Scripto, Transcribe Bentham; Welsh Wills Online; What’s on the Menu? translation 242, 275–6 crowdsourcing, alternative names for 4, 73, 96, 156, 213, 257, 259–60 crowdsourcing, definitions of 1, 2, 3–4, 45, 141, 163, 211, 231, 232, 233–4, 269–70 crowdsourcing, history of 5, 45, 141 crowdsourcing and volunteering 115, 143, 195, 214, 235, 271 design 7, 17, 20–21, 37, 63, 67, 68–71, 97–9, 105, 117, 118–22, 116, 126, 134–5, 173, 205, 226, 232 digital humanities 97, 140, 226, 243 humanities crowdsourcing 11, 65, 86, 96, 231–44 project evaluation 23, 28, 33, 37, 38, 40–42, 174 effectiveness of crowdsourced task 74–8, 80–81, 85–6, 176, 177–8, 179 Evans, Max 92, 93, 213 FamilySearch Indexing 8, 132, 142 Flickr 17, 47, 49, 225 Flickr Commons 94, 114, 165, 167, 203 Freebase 114, 181 FreeBMD 217 games 115, 130–31, 223, 235, 272; see also Waisda? Holley, Rose 5, 142, 199 Howe, Jeff 3, 7, 140–41, 231 human-computer task integration and machine learning 8, 72 legal and licensing issues 47, 91, 122, 157, 164, 224 libraries 91, 95, 108–9, 168; see also British Library; National Library of Australia; National Library of Wales; New York Public Library linked data 188, 219 McGonigal, Jane 115, 130, 247 Mechanical Turk 167, 190, 273 MediaWiki 61, 63, 95, 97, 98, 105, 106 media and publicity 65, 67, 100, 113, 123, 126, 148, 149, 170, 180, 220 microtasks 9, 163 moderation and quality control 5, 63, 65, 71, 75–84, 132, 152, 189, 191, 205, 221, 223, 240 museums 117, 186, 247–9, 251–2, 254–8, 260–63; see also Brooklyn Museum; Royal Museums Greenwich; steve.museum National Library of Australia Trove 1, 5, 49, 95, 117, 240 National Library of Wales Cymru1900Wales 144, 145–6 The Welsh Experience of the First World War 140, 144, 146–51 Welsh Wills Online 144, 151–6 New York Public Library Geotagger 129 What’s on the Menu? 113–37, 270 Old Weather 45–56, 94, 117, 134, 215, 223, 232, 238 open source software as model 3, 6, 93, 250, 262 Papers of the War Department 89–105, 152 participants 67, 103, 152, 187, 194–6 motivations 2, 67, 89, 130, 167, 174, 194–200, 218, 243, 276 recruitment 67, 71, 100, 115, 123, 148, 178, 180; see also media and publicity reward and recognition 2, 46, 65, 132, 134–5, 143, 206, 279 super-contributors 72–3, 76, 83, 101, 179, 203 public history 93 Royal Museums Greenwich (including the National Maritime Museum and the Royal Observatory) 45, 47, 50–51 Scripto 2, 97–9, 105–10 semantic gap 165, 177, 239; see also crowdsourced tasks Shirky, Clay 115, 161 social media 38, 99, 142, 161, 180, 220, 241; see also Flickr steve.museum 165, 190 Surowiecki, James The wisdom of the crowds 3, 8, 19–20, 143–4, 165–6, 233, 274–5 Text Encoding Initiative (TEI) 60, 63, 67–8, 81, 84; see also crowdsourced markup Text recognition Optical character recognition (OCR) 118, 151, 239 Handwritten Text Recognition (HTR) 71–2 Transcribe Bentham 57–88, 95, 100, 156, 195, 225, 232 usability, see design user experience design, see design user-generated content, see web 2.0 vocabulary use 165, 174–7, 181, 186, 188, 193–4, 238 folksonomies 238; see also tagging Waisda? 161–81, 203 web 2.0 4, 19, 94, 141, 144, 213, 218, 235, 257 Index 283 Wikipedia 63, 72, 93, 114, 131, 135, 142, Zooniverse 6, 50, 94–5, 145, 180; see also 181, 248, 250 Old Weather DBpedia 188, 193 Ancient Lives 94 edit-a-thons 48 Galaxy Zoo 1, 53, 142, 191, 195–9, 232, 271, 273, 275, 276 Your Paintings Tagger 185–208 